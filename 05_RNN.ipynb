{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/05_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiobin3g2wyW"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "A Recurrent Neural Network (RNN) is a specific type of architecture that is widely used to deal with sequential information. So far, the introduced CNN architectures from the previous tutorials treated inputs as independent objects, however, many applications need to deal with data that is interconnected. For instance, if you are translating a sentence from English to Taiwanese, and you are predicting the next word, it is useful to know which words came before the last one.\n",
        "\n",
        "RNNs are called recurrent since they apply the same operation to each of the input sequences, with the output of an individual element being dependent on the previous one. Theoretically, RNNs establish a connection between the actual input and ALL the previous ones. Although this is assumed, in the practice, RNNs have proven to only *remember* a limited number of inputs. In other words, RNNs have a *memory* that allows them to *remember* previous elements and use their information to deal with the current input.\n",
        "\n",
        "RNNs can be split into multiple types depending on their applications. For instance, if we want to predict one word given only the previous one, the topology of our network is a *One to One*. Another example is image captioning, where we can design a *One to Many* architecture to obtain a description from a single input image. The following diagram shows the different types of problems we can face ([Source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)):\n",
        "\n",
        "\n",
        "![alt text](http://karpathy.github.io/assets/rnn/diags.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t6BQY9B21nV"
      },
      "source": [
        "**A Closer Look to RNN**\n",
        "\n",
        "The figure below shows the simplest version of an RNN, which can be easily derived from a simple feedforward architecture by adding a single loop:\n",
        "\n",
        "![alt text](https://i.ibb.co/qnGH6RT/vanilla-rnn.png)\n",
        "\n",
        "During training, the hidden state $h$ is iteratively updated based on the input value $x$ and the learned weights $W_h$ and $W_x$. The final output $y$ is estimated from the current state $h_t$ and the matrix $W_y$.\n",
        "\n",
        "Although RNN can assure short-term dependencies within the network, simple RNNs become unable to learn to connect information as the gap between past and present information grows. To overcome this limitation, in practical applications LSTM unit is adopted, that is a special RNNs architecture composed of multiple interacting layers ([Source](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)):\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOUUKOPH2_SG"
      },
      "source": [
        "In this tutorial, we will use LSTMs straight away as a black box."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "Cv18ukXvgPwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjNEJLHjafwK"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import requests\n",
        "import sys\n",
        "import string\n",
        "import time\n",
        "import torch\n",
        "import datasets\n",
        "import copy\n",
        "\n",
        "from IPython.display import SVG\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "import wget\n",
        "import torch.nn.functional as F\n",
        "import torchinfo\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    num_epochs: int = 10,\n",
        "    val_loader: DataLoader = None,\n",
        "    device: torch.device = DEVICE,\n",
        "    verbose: bool = True,\n",
        "):\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * x.size(0)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for x, y in val_loader:\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    y_pred = model(x)\n",
        "                    loss = criterion(y_pred, y)\n",
        "                    val_loss += loss.item() * x.size(0)\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "            if verbose:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMJ28M_227fj"
      },
      "source": [
        "# ***Many to One* RNNs - Regression**\n",
        "\n",
        "In this section, we are going to implement an easy example of an RNN in PyTorch. Given the records of a local newspaper, which indicates the number of new subscriptions per month, we want to predict the number of members that will join next month.\n",
        "\n",
        "First, we load the data with the utility method in Pandas `read_csv`, which allows us to load directly the dataset from an URL address. Let's print some rows of the imported record to see the format of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PZRejNk29jN"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"https://drive.google.com/uc?id=1Hv2nuwVXO_aZN89llGva0hSH6k3kKAVm\",usecols=[1],engine = \"python\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd3kk4Zh3CA-"
      },
      "source": [
        "Moreover, let's plot the number of subscriptions per month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEmJqK1z3Dei"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15, 5))\n",
        "plt.plot(data, label = \"City Newspaper Subscriptions\")\n",
        "plt.xlabel(\"Months\")\n",
        "plt.ylabel(\"1000 Subscriptions\")\n",
        "plt.title(\"Monthly Total Subscriptions to City Newspaper 1949 - 1960\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x55PWxD3IFE"
      },
      "source": [
        "First of all, we need to prepare the data for the training and test stage. Once the data is loaded, we normalize the input values and split them between training (70%) and testing (30%).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIrOh9Pi3JZg"
      },
      "outputs": [],
      "source": [
        "# convert pandas data frame into a numpy array of float32\n",
        "data_np = data.values.astype(\"float32\")\n",
        "\n",
        "# Split data into training and test using 70/30 split\n",
        "TRAINING_PERC = 0.70\n",
        "train_np = data_np[:int(len(data_np) * TRAINING_PERC), :]\n",
        "test_np = data_np[int(len(data_np) * TRAINING_PERC):, :]\n",
        "\n",
        "print(\"Number of samples training set:\", len(train_np))\n",
        "print(\"Number of samples test set:\", len(test_np))\n",
        "\n",
        "# Fit a MinMaxScaler only on the training data\n",
        "normalizer = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
        "train_np_norm = normalizer.fit_transform(train_np)\n",
        "\n",
        "# Use the same scaler to transform the test data\n",
        "test_np_norm = normalizer.transform(test_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI5UamBO3KrU"
      },
      "source": [
        "We also arrange the dataset (input and labels) in the appropriate PyTorch format by using the helper function `create_dataset()`.\n",
        "\n",
        "`create_dataset()` takes as argument the variable `window_size`. This variable is highly important when dealing with sequences since it is going to determine the length of our input data to the network. For instance, by setting `window_size` to 5, we will be using the last 5 monthly subscriptions values to predict the next one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvGh-1db3MOy"
      },
      "outputs": [],
      "source": [
        "# helper function to read data\n",
        "def create_dataset(dataset, window_size=1):\n",
        "    X = [dataset[i:i+window_size, 0] for i in range(len(dataset) - window_size)]\n",
        "    y = [dataset[i+window_size, 0] for i in range(len(dataset) - window_size)]\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create test and training sets for regression with window size 5\n",
        "window_size = 5\n",
        "train_X, train_Y = create_dataset(train_np_norm, window_size)\n",
        "test_X, test_Y = create_dataset(test_np_norm, window_size)\n",
        "\n",
        "# Add channel dimension (N, seq_len, 1)\n",
        "train_X = np.expand_dims(train_X, -1)\n",
        "test_X = np.expand_dims(test_X, -1)\n",
        "\n",
        "# Convert to tensors\n",
        "train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
        "train_Y_tensor = torch.tensor(train_Y, dtype=torch.float32).unsqueeze(1)  # (N, 1)\n",
        "\n",
        "# Shapes\n",
        "print(f\"Shape of training inputs: {train_X_tensor.shape}\")\n",
        "print(f\"Shape of training labels: {train_Y_tensor.shape}\")\n",
        "\n",
        "# Build a train data loader\n",
        "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlB3Edd03NwG"
      },
      "source": [
        "Once all the data is ready, we create the model as a `nn.Module` object including 16 LSTM units and a dense layer outputting a single scalar.\n",
        "As mentioned, we specify a window size equal to 5, so that the prediction of the current element depends only on the previous five ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuT4lcHt3Qqp"
      },
      "outputs": [],
      "source": [
        "class LSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=16, output_size=1):\n",
        "        super(LSTMRegressor, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = LSTMRegressor()\n",
        "print(torchinfo.summary(model, input_size=(1, 5, 1)))\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    nn.MSELoss(),\n",
        "    optim.Adam(model.parameters(), lr=0.001),\n",
        "    num_epochs=500,\n",
        "    verbose=False,\n",
        ")"
      ],
      "metadata": {
        "id": "oxBp_Jl4CIjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrMqbfrK3Nz8"
      },
      "source": [
        "Now we can compute the RMSE on the training and test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkhF1mrr3Tor"
      },
      "outputs": [],
      "source": [
        "def get_predict_and_score(model, X, Y, normalizer):\n",
        "    model = model.to(DEVICE)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
        "        labels = torch.tensor(Y, dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Convert predictions and labels to NumPy\n",
        "        pred = outputs.cpu().numpy()\n",
        "        actual = labels.cpu().numpy()\n",
        "\n",
        "        # Inverse transform both\n",
        "        pred_inv = normalizer.inverse_transform(pred)\n",
        "        actual_inv = normalizer.inverse_transform(actual)\n",
        "\n",
        "        # Compute MSE\n",
        "        rmse = math.sqrt(mean_squared_error(actual_inv, pred_inv))\n",
        "        return rmse, pred_inv\n",
        "\n",
        "rmse_train, train_predict = get_predict_and_score(model, train_X, train_Y, normalizer)\n",
        "rmse_test, test_predict = get_predict_and_score(model, test_X, test_Y, normalizer)\n",
        "\n",
        "print(\"Training RMSE: %.2f\" % rmse_train)\n",
        "print(\"Test RMSE: %.2f\" % rmse_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVOaCHMc3W6I"
      },
      "source": [
        "Moreover, we can plot the predictions and actual values in a graph to check visually the performance of our predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUjXRdOg3Vjj"
      },
      "outputs": [],
      "source": [
        "# Training predictions placement\n",
        "train_predictions = np.empty_like(data_np)\n",
        "train_predictions[:] = np.nan\n",
        "train_predictions[window_size:window_size + len(train_predict), :] = train_predict\n",
        "\n",
        "# Test predictions placement\n",
        "test_predictions = np.empty_like(data_np)\n",
        "test_predictions[:] = np.nan\n",
        "test_start = window_size + len(train_predict)\n",
        "test_predictions[test_start:test_start + len(test_predict), :] = test_predict\n",
        "\n",
        "# Create the plot.\n",
        "plt.figure(figsize = (15, 5))\n",
        "plt.plot(data_np, label = \"True value\") #transform back the full dataset\n",
        "plt.plot(train_predictions, label = \"Training predictions\")\n",
        "plt.plot(test_predictions, label = \"Test predictions\")\n",
        "plt.xlabel(\"Months\")\n",
        "plt.ylabel(\"1000 member subscriptions\")\n",
        "plt.title(\"Comparison true vs. predicted in the training and testing set\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMzQDr20HPbw"
      },
      "source": [
        "#Sequence Modelling: Text\n",
        "\n",
        "Now we will use RNNs to tackle text examples. Text is one of the modalities that have been widely tackled with deep learning improving upon past methods. We will present two problems: classification and generation.\n",
        "\n",
        "A small summary of both problems is given below.\n",
        "\n",
        "**Classification**\n",
        "\n",
        "Classification is a standard problem, where we have some input data $x$ and try to classify it as one of the available classes $y$. In the case of sequential data, though, $x$ will be a sequence of elements that will be processed by the RNN to return the label $y$. The image depicts a sequence classification problem, where the red blocks are inputs, the green blocks the RNN model, and the blue block is the output. An example of a text classification problem that can be tackled with RNNs is \"Hate speech detection\", where the architecture must identify if an input text contains racist or sexist language, among others.\n",
        "\n",
        "![alt text](https://i.ibb.co/TtZPpZr/Capture.jpg)\n",
        "\n",
        "\n",
        "**Generation**\n",
        "\n",
        "In a generation problem, we aim to generate a sequence $y$ following the same distribution as the real data $x$. We will input the sequence into the model, and we will output another sequence $y$. Text translation is a typical example of many to many RNNs.\n",
        "\n",
        "![](https://i.ibb.co/7gSwnT2/Capture.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4kJNRmcq3DL"
      },
      "source": [
        "# Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLVCgocM6Kk"
      },
      "source": [
        "For the text classification example we will use the IMDB movie reviews dataset available in Pytorch through the `torchtext` library. The dataset includes tens of thousands of movie reviews taken from the IMDB website with a corresponding label for each review. The label is binary, and indicates if the review has a positive (label=1) or negative (label=0) sentiment. This problem is a quite standard Natural Language Processing (NLP) problem, and it is called sentiment classification or sentiment analysis.\n",
        "\n",
        "Before loading the data and building our model, we will explain a common part of NLP models, the embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAzSKba3J5NA"
      },
      "source": [
        "### Embeddings\n",
        "**References:**\n",
        "\n",
        "[1] https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "[2] https://colab.research.google.com/drive/1oXjNYSJ3VsRvAsXN4ClmtsVEgPW_CX_c?hl=en#scrollTo=p9q7qfXrvq-J\n",
        "\n",
        "\n",
        "Usually, the first step of text modelling is transforming the words into a numerical vector that represents the meaning or some properties of the word. This vector can be then processed by the network. To do so, we first will encode the word sequences in integer numbers, and we will also have a dictionary that contains the relationship `(actual word, integer)`. For example, the sentence \"the cat is on the table and the dog is on the mat\" can be encoded in the form $(7, 1, 3, 5, 7, 6, 0, 7, 2, 3, 5, 7, 4)$, with the corresponding dictionary $(and, 0), (cat, 1), (dog, 2)\\dots (the, 7)$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhRu4KoqSVvH"
      },
      "outputs": [],
      "source": [
        "sentence = 'the cat is on the table and the dog is on the mat'\n",
        "## We form a list of unique words by using a set\n",
        "## which returns only unique elements\n",
        "## we also sort them, which is not necessary\n",
        "sentence_set = sorted(set(sentence.split(' ')))\n",
        "words = list(sentence_set)\n",
        "## We now form a dictionary in the form of\n",
        "## e.g. dict_words[and] = 1\n",
        "dict_words = dict((word, i) for i, word in enumerate(words))\n",
        "## We now encode the sentence in a list of integers\n",
        "encoded_sentence = [dict_words[w] for w in sentence.split()]\n",
        "print(\"Sentence Set: \", sentence_set)\n",
        "print(\"Vocabulary: \", encoded_sentence)\n",
        "print(\"Encoded: \", dict_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoHyEZdVzHFD"
      },
      "source": [
        "PyTorch does not include a built-in tokenizer as part of its core API. Instead, tokenization and text preprocessing are typically handled manually or using external libraries such as `nltk`, `spacy`, or `torchtext`.\n",
        "\n",
        "In this tutorial, we will use `nltk` to handle tokenization and build a vocabulary manually using Python collections. This allows us to convert raw text into numerical sequences that can be passed into neural network models. The process includes:\n",
        "\n",
        "- Splitting raw text into tokens using `nltk.word_tokenize`\n",
        "- Mapping each token to a unique integer ID using a `Counter`\n",
        "- Padding sequences to uniform length for batching\n",
        "\n",
        "You can read more about `nltk` and its tokenizers in the [NLTK documentation](https://www.nltk.org/api/nltk.tokenize.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5EL3-o5wsQl"
      },
      "outputs": [],
      "source": [
        "# First, we tokenize the input text using NLTK's word_tokenize.\n",
        "# This splits the sentence into individual words (tokens).\n",
        "# We also lowercase the text to ensure consistent vocabulary mapping.\n",
        "tokens = nltk.word_tokenize(sentence.lower())\n",
        "print(\"Tokens:\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKshU7Lj0Sgl"
      },
      "outputs": [],
      "source": [
        "## You can also check the word count\n",
        "word_counts = Counter(tokens)\n",
        "print(\"Word counts:\", word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RBQrbI10CuR"
      },
      "outputs": [],
      "source": [
        "## Now, the class has formed a dictionary with all the words\n",
        "word_index = {word: idx + 1 for idx, (word, _) in enumerate(word_counts.items())}\n",
        "print(\"Word index:\", word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNI-js25161w"
      },
      "outputs": [],
      "source": [
        "## You can transform the data to sequences of integers\n",
        "## We print the first 10 elements\n",
        "encoded_sequence = [word_index[token] for token in tokens]\n",
        "print(\"Encoded sequence:\", encoded_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lITR0l3SUAU"
      },
      "source": [
        "Now that we have the sequence in the form of a list of integers,  we could input that directly to the RNN. However, as those numbers are arbitraryly chosen, it would be really hard for the RNN to understand the relationships between the words. To give the model more representation power, we first want to transform the integer to a vector of dimension $d$ which represents the semantic meaning of the words. Each of these vectors is called an Embedding. The actual values of the vectors are not important, only the relationships between them. For example, `dog` and `cat` vectors are probably going to be closer than `dog` and `the`. Or if you are doing sentiment classification, probably two words encoding similar positive sentiments will be closer than a word expressing positive sentiments and a completely neutral word.\n",
        "\n",
        " The embeddings can be initialized randomly and the model will learn suitable values to reduce the loss during the training process. However, embeddings which have already been trained in a large corpus of text such as Wikipedia can also be used. Examples of these pretrained embeddings are [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) or  [GloVe](https://nlp.stanford.edu/projects/glove/). These methods are trained using the context of the words, for example predicting the surrounding words when a specific word is given. As a result, words that appear in similar contexts have closer embeddings.\n",
        "\n",
        "![alt text](https://i.ibb.co/s5sg6dZ/Screenshot-from-2019-02-08-15-13-41.png)\n",
        "\n",
        "The image shows relationships between words in the embedding space.\n",
        "\n",
        "\n",
        "\n",
        "We now download the GloVe pretrained vectors trained in Wikipedia and Gigaword. [Here](https://nlp.stanford.edu/projects/glove/) you have an overview of what GloVe is. We will see how the embeddings contain a semantic meaning that allows us to model semantic relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie3ToDUgKWN4"
      },
      "outputs": [],
      "source": [
        "## Download and unzip glove pretrained embeddings\n",
        "!wget https://imperialcollegelondon.box.com/shared/static/c9trfhhwl9ohje5g3sapu3xk2zoywp3c.txt -O glove_vectors.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5uobxs34Bzo"
      },
      "source": [
        "The code below prints the first 5 lines of the imported `glove_vectors.txt` file.\n",
        "The first line is a header indicating that the dataset contains 400,000 tokens, each represented by a 300-dimensional embedding vector.\n",
        "The lines that follow contain the actual embeddings, where each line corresponds to a word or punctuation mark (e.g., `the`, `,`, `.`, `of`, etc.), followed by its associated vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoNfOGDQ3aQa"
      },
      "outputs": [],
      "source": [
        "with open(\"glove_vectors.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for _ in range(5):\n",
        "        print(f.readline())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDs0EebuUJ5x"
      },
      "source": [
        "We will now load the GloVe embeddings into a `torch.nn.Embedding` layer to manipulate the embeddings. This piece of code takes some time to load, as the embeddings file is quite large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK0qwB--K0Bp"
      },
      "outputs": [],
      "source": [
        "# Load GloVe vectors and build vocab and weight matrix\n",
        "def load_glove_as_embedding_layer(glove_path, embedding_dim):\n",
        "    vocab = {\"<unk>\": 0}\n",
        "    vectors = [torch.zeros(embedding_dim)]  # <unk> embedding\n",
        "    skipped = 0\n",
        "\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        first_line = f.readline()\n",
        "        if len(first_line.strip().split()) == 2:\n",
        "            print(f\"Skipping header: {first_line.strip()}\")\n",
        "        else:\n",
        "            f.seek(0)\n",
        "\n",
        "        for line in f:\n",
        "            tokens = line.strip().split()\n",
        "            word = tokens[0]\n",
        "            vector = tokens[1:]\n",
        "\n",
        "            if len(vector) != embedding_dim:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                vector_tensor = torch.tensor([float(val) for val in vector], dtype=torch.float32)\n",
        "            except ValueError:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            vocab[word] = len(vectors)\n",
        "            vectors.append(vector_tensor)\n",
        "\n",
        "    if len(vectors) <= 1:\n",
        "        raise RuntimeError(\"No valid embeddings loaded. Check the file formatting.\")\n",
        "\n",
        "    print(f\"Loaded {len(vectors)-1} word vectors. Skipped {skipped} lines.\")\n",
        "\n",
        "    weight_matrix = torch.stack(vectors)\n",
        "    embedding_layer = nn.Embedding.from_pretrained(weight_matrix, freeze=False)\n",
        "    return vocab, embedding_layer\n",
        "\n",
        "# Example usage\n",
        "glove_path = \"glove_vectors.txt\"\n",
        "vocab, embedding = load_glove_as_embedding_layer(glove_path, embedding_dim=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiZ1Nbru5Mlk"
      },
      "outputs": [],
      "source": [
        "## Helper function - get embedding by word\n",
        "def get_vector(word):\n",
        "    idx = vocab.get(word)\n",
        "    if idx is None:\n",
        "        raise ValueError(f\"Word '{word}' not in vocabulary.\")\n",
        "    return embedding(torch.tensor(idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrj-dPMbSsrc"
      },
      "source": [
        "Now we will do some operations using the embeddings of the words. First, we will do some words arithmetics based on the embeddings, which it will show us that the words encode some semantic meaning. For example, the distance between words with similar semantic meaning but different genders is approximately fixed. Meaning that the vector resulting from doing $man - woman$ should be similar to $king - queen$, hence:\n",
        "\n",
        "$king - queen \\approx man - woman \\rightarrow woman + king - man \\approx queen$\n",
        "\n",
        "We can check this using the method `most_similar` and the `positive` and `negative` arguments as following:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PYEzmorMFD_"
      },
      "outputs": [],
      "source": [
        "## We can do word arithmetics.\n",
        "## We look for the nearest neighbour of the vector resulting on doing\n",
        "## the operation 'king' + 'woman' - 'man'\n",
        "\n",
        "def perform_arithmetic(positive, negative, vocab, embedding, top_k=5):\n",
        "    \"\"\"\n",
        "    Performs word vector arithmetic and finds the top_k most similar words,\n",
        "    excluding any input words.\n",
        "\n",
        "    Args:\n",
        "        positive (list): Words to add.\n",
        "        negative (list): Words to subtract.\n",
        "        vocab (dict): Word-to-index dictionary.\n",
        "        embedding (nn.Embedding): Embedding layer with preloaded GloVe vectors.\n",
        "        top_k (int): Number of nearest neighbors to return.\n",
        "\n",
        "    Returns:\n",
        "        list: Top-k similar words excluding inputs.\n",
        "    \"\"\"\n",
        "    device = embedding.weight.device  # Ensures computation matches embedding's device\n",
        "\n",
        "    # Compute the target vector: sum(positive) - sum(negative)\n",
        "    vector = sum(get_vector(w) for w in positive) - sum(get_vector(w) for w in negative)\n",
        "\n",
        "    # Compute cosine similarities to all vocab vectors\n",
        "    all_vectors = embedding.weight\n",
        "    similarities = F.cosine_similarity(vector.unsqueeze(0), all_vectors)\n",
        "\n",
        "    # Get most similar words\n",
        "    top_k_ids = similarities.topk(top_k + len(positive) + len(negative)).indices\n",
        "    inv_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "    # Remove input words\n",
        "    input_words = set(positive + negative)\n",
        "    result = []\n",
        "    for idx in top_k_ids:\n",
        "        word = inv_vocab.get(idx.item())\n",
        "        if word not in input_words:\n",
        "            result.append(word)\n",
        "        if len(result) == top_k:\n",
        "            break\n",
        "\n",
        "    return result\n",
        "\n",
        "# Run the query\n",
        "similar_words = perform_arithmetic(positive=[\"king\", \"woman\"], negative=[\"man\"], vocab=vocab, embedding=embedding)\n",
        "print(\"Most similar words to king + woman - man:\", similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1bgFWT3aGUi"
      },
      "outputs": [],
      "source": [
        "## Similar examples\n",
        "## google + ios - apple\n",
        "similar_words = perform_arithmetic(positive=[\"google\", \"ios\"], negative=[\"apple\"], vocab=vocab, embedding=embedding)\n",
        "print(\"Most similar words to google + ios - apple:\", similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J09zgTciaHU4"
      },
      "outputs": [],
      "source": [
        "## england + paris - france\n",
        "similar_words = perform_arithmetic(positive=[\"england\", \"paris\"], negative=[\"france\"], vocab=vocab, embedding=embedding)\n",
        "print(\"Most similar words to england + paris - france:\", similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7LMqhpATsAL"
      },
      "outputs": [],
      "source": [
        "## We can also check the word that does not match the rest\n",
        "## This is done by finding the word with the embedding vector furthest away from the mean\n",
        "def doesnt_match(words):\n",
        "    vectors = torch.stack([get_vector(w) for w in words])\n",
        "    mean_vector = vectors.mean(dim=0)\n",
        "    distances = F.cosine_similarity(vectors, mean_vector.unsqueeze(0))\n",
        "    odd_one_index = distances.argmin().item()\n",
        "    return words[odd_one_index]\n",
        "\n",
        "print(\"Word that doesnt match from ['breakfast', 'cereal', 'dinner', 'lunch']: \",\n",
        "      doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjgjAlaBQvaH"
      },
      "source": [
        "In http://projector.tensorflow.org/ you can visualize the words projected in the $\\mathbb{R}^3$ space using either PCA or tSNE, which are both techniques for dimensionality reduction. There, you can see how words are clustered by meaning or topic.\n",
        "\n",
        "\n",
        "![](https://brenndoerfer.github.io/deep-sentiment-analysis-distill/img/tensorboard_projector.gif)\n",
        "\n",
        "Image taken from [here](https://brenndoerfer.github.io/deep-sentiment-analysis-distill/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30w3Y4oHJy79"
      },
      "source": [
        "### IMDB\n",
        "Now let's start tackling a text classification problem. We first load the IMDB dataset for sentiment classification. As we said, it contains movie reviews with a corresponding binary sentiment label (label=1 corresponds to positive sentiment, and label=0 to negative sentiment). The words are already encoded as integers in order from most common words to less common (e.g. `the` is a common word so it should be encoded as a small integer). That makes it easy to filter the non-common words by using the argument `num_words` when loading the data. The filtered words will be all encoded as a special token `<UNK>`, which means unknown. For example, if we want to load the dataset with only the top $5000$ most common words we can do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWU0zG7AHd3u"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Parameters\n",
        "nb_words = 5000\n",
        "maxlen = 80\n",
        "\n",
        "# Load IMDb\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Tokenizer\n",
        "def tokenize(text):\n",
        "    return nltk.word_tokenize(text.lower())\n",
        "\n",
        "# 1. Build vocabulary from training set\n",
        "counter = Counter()\n",
        "for text in dataset['train']['text']:\n",
        "    counter.update(tokenize(text))\n",
        "\n",
        "# Reserve 0:<PAD>, 1:<START>, 2:<UNK>\n",
        "most_common = counter.most_common(nb_words - 3)\n",
        "word2idx = {'<PAD>': 0, '<START>': 1, '<UNK>': 2}\n",
        "for idx, (word, _) in enumerate(most_common, start=3):\n",
        "    word2idx[word] = idx\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "# 2. Encode function\n",
        "def encode(text):\n",
        "    tokens = tokenize(text)\n",
        "    indices = [word2idx.get(t, word2idx['<UNK>']) for t in tokens]\n",
        "    return [word2idx['<START>']] + indices[:maxlen-1]\n",
        "\n",
        "# 3. Preprocess dataset into fixed-size tensors\n",
        "def preprocess(texts):\n",
        "    encoded = [torch.tensor(encode(t), dtype=torch.long) for t in texts]\n",
        "    padded = pad_sequence(encoded, batch_first=True, padding_value=word2idx['<PAD>'])\n",
        "    # ensure exact maxlen\n",
        "    if padded.size(1) < maxlen:\n",
        "        pad_len = maxlen - padded.size(1)\n",
        "        padded = torch.nn.functional.pad(padded, (0, pad_len), value=word2idx['<PAD>'])\n",
        "    return padded[:, :maxlen]\n",
        "\n",
        "# 4. Apply to IMDb\n",
        "x_train = preprocess(dataset['train']['text'])\n",
        "y_train = torch.tensor(dataset['train']['label'], dtype=torch.long)\n",
        "\n",
        "x_test = preprocess(dataset['test']['text'])\n",
        "y_test = torch.tensor(dataset['test']['label'], dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "test_ds = TensorDataset(x_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiYpN-jwcGd-"
      },
      "source": [
        "We see that the dataset contains 25000 examples for both train and testing. Let's print an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF2twk31cNiN"
      },
      "outputs": [],
      "source": [
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqxzLNgAcUmL"
      },
      "source": [
        "The printed example is a sequence of numbers. It is important to note that there are three special integers in this IMDB dataset: $0$, $1$ and $2$. $0$ will be used to pad the sequences, which we will explain now. $1$ is used as the `<START>` token (you can see how the printed sequence starts with a `1`). $2$ is the token `<UNK>` which is used for all the filtered non-common words. If we want to retrieve the actual sentence, we can use the method `word2idx()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y26JIPAadBtX"
      },
      "outputs": [],
      "source": [
        "# Retrieve the first encoded sentence\n",
        "encoded_sentence = x_train[0]\n",
        "\n",
        "# Decode using idx2word\n",
        "decoded_sentence = ' '.join([idx2word[idx.item()] for idx in encoded_sentence if idx.item() in idx2word])\n",
        "\n",
        "print(decoded_sentence)\n",
        "print(y_train[0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyhd3lQefNH4"
      },
      "source": [
        "We can see how we just recovered the sentence that we had printed before as a sequence of numbers. The review is negative, so the corresponding label is \"0\".\n",
        "\n",
        "One of the problems with the given text data is that the sequences all have a different length. We want to give PyTorch a batch of inputs with fixed dimensions. To do so, we define a maximum length `maxlen`, and truncate the sentences longer than that, and also pad with $0$'s at the beginning the sentences shorter than that length.\n",
        "\n",
        "We use PyTorchâ€™s `pad_sequence` utility from `torch.nn.utils.rnn`, along with `torch.nn.functional.pad`, to accomplish this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2ljchrWPMz3"
      },
      "source": [
        "We will now build the model we will use for sentiment classification. The model is formed by an Embedding layer, where the model will learn a vector of dimensionality `embedding_dim` for each of the words; an LSTM layer and a linear layer that maps the output of the LSTM to 1 value. We train this with the `binary_crossentropy` loss and `sigmoid` activation as we only have two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh2cHXD5HjQG"
      },
      "outputs": [],
      "source": [
        "# Model Parameters\n",
        "embedding_dim = 300\n",
        "lstm_units = 128\n",
        "vocab_size = nb_words\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2idx['<PAD>'])\n",
        "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_units, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)                           # (batch, seq_len) -> (batch, seq_len, embed_dim)\n",
        "        output, (h_n, c_n) = self.lstm(x)               # output: (batch, seq_len, hidden), h_n: (1, batch, hidden)\n",
        "        last_hidden = h_n[-1]                           # (batch, hidden)\n",
        "        out = self.fc(last_hidden)                      # (batch, 1)\n",
        "        return torch.sigmoid(out).squeeze(1)            # (batch,)\n",
        "\n",
        "# Instantiate the model\n",
        "model = SentimentLSTM(vocab_size, embedding_dim, lstm_units).to(DEVICE)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTDlUcEF0mqn"
      },
      "source": [
        "Let's train it for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJfDwWRlHoEm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "history = {\n",
        "    'loss': [],\n",
        "    'val_loss': [],\n",
        "    'accuracy': [],\n",
        "    'val_accuracy': [],\n",
        "    'epoch': []\n",
        "}\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE).float()\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(batch_x)\n",
        "        loss = criterion(preds, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        all_preds.extend((preds > 0.5).cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    train_loss = sum(train_losses) / len(train_losses)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE).float()\n",
        "            preds = model(batch_x)\n",
        "            loss = criterion(preds, batch_y)\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "            val_preds.extend((preds > 0.5).cpu().numpy())\n",
        "            val_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    val_loss = sum(val_losses) / len(val_losses)\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "    # Save to history\n",
        "    history['epoch'].append(epoch)\n",
        "    history['loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['accuracy'].append(train_acc)\n",
        "    history['val_accuracy'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}  \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}  \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tVyZLyv0rbO"
      },
      "source": [
        "This is a classification example, so let's print the loss and the classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3D78e9yHo8l"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history['epoch'], history['loss'], label='training')\n",
        "plt.plot(history['epoch'], history['val_loss'], label='validation')\n",
        "plt.title('loss')\n",
        "plt.grid(visible=True)\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history['epoch'], history['accuracy'], label='training')\n",
        "plt.plot(history['epoch'], history['val_accuracy'], label='validation')\n",
        "plt.title('accuracy')\n",
        "plt.grid(visible=True)\n",
        "plt.legend(loc='best')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjoId37R0yVI"
      },
      "source": [
        "It looks like we could have stopped the training earlier as the validation loss keeps increasing after the first epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOHn1utQHqHu"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE).float()\n",
        "        preds = model(batch_x)\n",
        "        preds_binary = (preds > 0.5).cpu().numpy()\n",
        "        all_preds.extend(preds_binary)\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3HtK17c1S_F"
      },
      "source": [
        "### Saving best performing model\n",
        "\n",
        "Now we save the best performing models using the validation loss as a metric. As you already know, lower training error does not mean in some cases better performance in the validation or test split.\n",
        "\n",
        "In the last example we ran the model for 5 epochs, however after the first epoch the validation loss increased. We want to use the model performing the best in the validation set. To do so, we can use a ModelCheckpoint callback as was explained in past tutorials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzVNbh5a1xsq"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Re-instantiate model\n",
        "embedding_dim = 300\n",
        "lstm_units = 128\n",
        "vocab_size = nb_words\n",
        "\n",
        "model = SentimentLSTM(vocab_size, embedding_dim, lstm_units).to(DEVICE)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "initial_weight = model.fc.weight.clone()\n",
        "\n",
        "# For tracking best validation loss\n",
        "best_val_acc = 0\n",
        "best_model_state = None\n",
        "best_epoch = 0\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE).float()\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(batch_x)\n",
        "        loss = criterion(preds, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        all_preds.extend((preds > 0.5).cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    train_loss = sum(train_losses) / len(train_losses)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE).float()\n",
        "            preds = model(batch_x)\n",
        "            loss = criterion(preds, batch_y)\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "            val_preds.extend((preds > 0.5).cpu().numpy())\n",
        "            val_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    val_loss = sum(val_losses) / len(val_losses)\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = copy.deepcopy(model.state_dict())\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save(best_model_state, 'best_model.pth')\n",
        "        print(\"Saved best model\")\n",
        "\n",
        "print(f\"Best model was from epoch {best_epoch} with val accuracy {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrtRFGUF22tf"
      },
      "source": [
        "We now saved the model after every epoch if the validation accuracy increased. In this case, the best epoch is the fourth one, so that version of the model is saved. Let's load the model now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwHZ54_22vSa"
      },
      "outputs": [],
      "source": [
        "# Load the best saved model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KLudp1u3CHY"
      },
      "source": [
        "And now we check if the accuracy is better compared to when we used the model trained for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsuLgHQu2zqu"
      },
      "outputs": [],
      "source": [
        "# Let's see the accuracy in the test split\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE).float()\n",
        "        preds = model(batch_x)\n",
        "        preds_binary = (preds > 0.5).cpu().numpy()\n",
        "        all_preds.extend(preds_binary)\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ufb6B68hUhl"
      },
      "source": [
        "### Importance of embeddings\n",
        "Now, let's check quickly if using Embeddings provide any benefit. For this experiment, we remove the Embedding layer, meaning we will be inputting the word index, i.e. an integer, to the LSTM. Additionally, we need to vary the shape of the input data as the LSTM needs a third dimension with the number of channels per input (in this case 1, as each word is represented by an integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1NY5IhRhppg"
      },
      "outputs": [],
      "source": [
        "# model parameters\n",
        "# Same model as before but without embeddings\n",
        "lstm_units = 128\n",
        "\n",
        "# Reshape to (batch_size, seq_len, 1)\n",
        "x_train_r = x_train.unsqueeze(-1).float()\n",
        "x_test_r = x_test.unsqueeze(-1).float()\n",
        "\n",
        "class LSTMNoEmbedding(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        out = self.fc(hidden.squeeze(0))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_NNnSGDwwrG"
      },
      "outputs": [],
      "source": [
        "# Use same labels\n",
        "train_dataset = TensorDataset(x_train_r, y_train)\n",
        "test_dataset = TensorDataset(x_test_r, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZaz4eYyiBPc"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = LSTMNoEmbedding(hidden_dim=128).to(DEVICE)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    train_loss, train_correct, total = 0, 0, 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(xb).squeeze()\n",
        "        loss = criterion(output, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * yb.size(0)\n",
        "        preds = (torch.sigmoid(output) > 0.5).long()\n",
        "        train_correct += (preds == yb.long()).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / total\n",
        "    avg_train_acc = train_correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE).float()\n",
        "            output = model(xb).squeeze()\n",
        "            loss = criterion(output, yb)\n",
        "\n",
        "            val_loss += loss.item() * yb.size(0)\n",
        "            preds = (torch.sigmoid(output) > 0.5).long()\n",
        "            val_correct += (preds == yb.long()).sum().item()\n",
        "            val_total += yb.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / val_total\n",
        "    avg_val_acc = val_correct / val_total\n",
        "\n",
        "    history['loss'].append(avg_train_loss)\n",
        "    history['accuracy'].append(avg_train_acc)\n",
        "    history['val_loss'].append(avg_val_loss)\n",
        "    history['val_accuracy'].append(avg_val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Train Acc={avg_train_acc:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={avg_val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZcRp4nXiN84"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history['loss'], label='training')\n",
        "plt.plot(history['val_loss'], label='validation')\n",
        "plt.title('loss')\n",
        "plt.grid(visible=True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history['accuracy'], label='training')\n",
        "plt.plot(history['val_accuracy'], label='validation')\n",
        "plt.title('accuracy')\n",
        "plt.grid(visible=True)\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyOqn7yginQg"
      },
      "source": [
        "We see how the accuracy is just slightly better compared to doing a random guess as there are only 2 classes. That is because the integer that encodes the word is chosen arbitrarily and does not encode any property of the words nor any relationship between words. Hence, using learned embeddings results in models with better capacity/accuracy for this kind of tasks.\n",
        "\n",
        "Instead of inputting directly integers to the RNN, we could argue that another way of representing the input words is to input the one-hot representation of the word directly to the RNN instead of using embeddings. This method has two problems. First, the resulting dimensionality would be too large. In this case we are loading the $5000$ most common words in IMDB, so each word would be encoded in a vector of dimensionality $5000$. Secondly, the encoding does not give any notion of similarity between the different words. For these two reasons, embedding the words to a common lower dimensionality space is also better than using one-hot encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EwrgAmcM9na"
      },
      "source": [
        "# Text Generation\n",
        "\n",
        "We now will present an example of text generation using an RNN. We will use the Tiny Shakespeare dataset, which contains samples from Shakespeare works.\n",
        "\n",
        "We pose the problem as a classification problem as in the Sentiment classification example. In a text generation setting, given a sequence, we aim to predict the next one. During the training step, the process would be quite similar to the classification task, i.e. given a sequence you predict a label/word/character. However, in the prediction setting, we aim to output a whole sentence, not only a word or character. For that reason we will follow the procedure explained in the image.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*XvUt5wDQA8D3C0wAuxAvbA.png)\n",
        "The image is taken from [here](https://medium.com/@david.campion/text-generation-using-bidirectional-lstm-and-doc2vec-models-1-3-8979eb65cb3a)\n",
        "\n",
        "As in the text classification section, we encode the raw sentences in the form of a sequence of integers. In this case, however, we have a text with a really wide vocabulary, and we aim to predict the following element of the sequence. In this dataset the number of words is really large, so if we used a word-level model the number of available classes would be quite large if we did not filter the number of possible words. For this example, instead of encoding on a word level, we will do it on a character level, which results in a more limited vocabulary. We aim to predict the next character when inputting a sentence of length $d$. In evaluation mode, i.e. when we are predicting the next character, we then input a seed of $d$ characters as the first input, and afterwards we use the last $d$ characters as input to predict the next character.\n",
        "\n",
        "The advantages of predicting characters instead of words is that character-level models have a limited vocabulary/classes compared to the number of possible words in a text. They are also more flexible (for example, they can generate \"fake\" url links if trained in e.g. wikipedia data). However, word-level models usually present higher performance as it easier for them to keep track of the long-term meaning of the sentence, and also can avoid any spelling mistakes that may happen in character-level models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_-da7NsDrJf"
      },
      "source": [
        "We first download the data we use for the example, and then read the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcR0v2GGNFCd"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt\n",
        "## We read all the raw data in the variable data\n",
        "data = open('./tiny-shakespeare.txt', 'r').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxFUMjE9kP4C"
      },
      "source": [
        "Let's print a subset of the data for visualization purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c8ZSkLKkX6Y"
      },
      "outputs": [],
      "source": [
        "print(data[:364])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhZmAeuzD0Wt"
      },
      "source": [
        "We see in the raw text that the name of the characters is printed, that there is a blank line between the different character lines, and also that lines are usually not longer than ~80 characters. We hope the network is capable of learning all of this.\n",
        "\n",
        "Now we will structure the data so we can input it into our RNN model. We aim to encode the text using a sequence of integers as we explained in the classification section.  Hence, we form a dictionary with all the different characters appearing in the text (including whitespaces and punctuation) and its corresponding integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs3tRv_1whDc"
      },
      "outputs": [],
      "source": [
        "characters = sorted(list(set(data)))\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEfzwgCnwjwo"
      },
      "source": [
        "Now we want to split the text in examples of length `seq_length` where the label is the next character. As we mentioned, we pose the problem as a set of successive classification problems, where we try to predict the next word given an input sentence. To split the text in pairs of (sequence, next word), we use the dictionary `char_to_n` to encode the different elements as integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gulHCFMJ_tEE"
      },
      "outputs": [],
      "source": [
        "x = []\n",
        "y = []\n",
        "length = len(data)\n",
        "seq_length = 100\n",
        "for i in range(0, length-seq_length, 1):\n",
        "  sequence = data[i:i + seq_length]\n",
        "  label = data[i + seq_length]\n",
        "  x.append([char_to_n[char] for char in sequence])\n",
        "  y.append(char_to_n[label])\n",
        "n_samples = len(x)\n",
        "print(\"Total Samples: {:d}\".format(n_samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehank2xQFGOo"
      },
      "source": [
        "We now form a test split by using 5% of the available data. To do so, we just take the last 5% of the data. Usually, you need to randomize the data before splitting in test/train to avoid having a different distribution of the data in both splits. However, as this is a sequential data where we want to predict the next character, if we shuffle the splits we would have this in training:\n",
        "\n",
        "``\n",
        "x = 'the boy is tal_' y = 'l'\n",
        "``\n",
        "\n",
        "and this in testing:\n",
        "\n",
        "``\n",
        "x = 'the boy is ta_' y = 'l'\n",
        "``\n",
        "\n",
        "which would contaminate the splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbBqXbtpfa5T"
      },
      "outputs": [],
      "source": [
        "# train/test split\n",
        "x_train = torch.tensor(x[:int(n_samples*0.95)], dtype=torch.long)\n",
        "x_test  = torch.tensor(x[int(n_samples*0.95):], dtype=torch.long)\n",
        "y_train = torch.tensor(y[:int(n_samples*0.95)], dtype=torch.long)\n",
        "y_test  = torch.tensor(y[int(n_samples*0.95):], dtype=torch.long)\n",
        "\n",
        "# reshape to (num_samples, seq_length)\n",
        "x_train = x_train.view(len(x_train), seq_length)\n",
        "x_test  = x_test.view(len(x_test), seq_length)\n",
        "\n",
        "# one-hot encode labels\n",
        "num_classes = len(characters)  # number of distinct characters\n",
        "y_train = F.one_hot(y_train, num_classes=num_classes).float()\n",
        "y_test  = F.one_hot(y_test,  num_classes=num_classes).float()\n",
        "\n",
        "y_train_idx = y_train.argmax(dim=1)  # shape [N]\n",
        "y_test_idx  = y_test.argmax(dim=1)   # shape [N]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U63V-XoLkzD2"
      },
      "source": [
        "Let's print a sequence and the shape of the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_KvSAVQzOvq"
      },
      "outputs": [],
      "source": [
        "print(x_train[8000])\n",
        "print(y_train_idx.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uudj2qT_FNJG"
      },
      "source": [
        "Here, we define the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13TcalgkqHsM"
      },
      "outputs": [],
      "source": [
        "class LSTMTextGen(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, lstm_units):\n",
        "        super(LSTMTextGen, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, lstm_units, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_units, vocab_size)  # output = vocab size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)            # (B, T, E)\n",
        "        out, _ = self.lstm(x)            # (B, T, H)\n",
        "        last = out[:, -1, :]             # (B, H)\n",
        "        logits = self.fc(last)           # (B, V)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC2BaDcumZnm"
      },
      "source": [
        "The model is a little bit complex, so training takes some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmvgKtWfCWAV"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(x_train, y_train_idx), batch_size=128, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(x_test,  y_test_idx),  batch_size=128, shuffle=False)\n",
        "\n",
        "text_gen_model = LSTMTextGen(vocab_size=y_train.shape[1], embedding_size=100, lstm_units=128).to(DEVICE)\n",
        "\n",
        "train(\n",
        "    text_gen_model,\n",
        "    train_loader,\n",
        "    nn.CrossEntropyLoss(),\n",
        "    torch.optim.Adam(text_gen_model.parameters(), lr=1e-3),\n",
        "    num_epochs=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5XsfQRNr9An"
      },
      "source": [
        "### Prediction\n",
        "\n",
        "We now take any of the sequences from the test split and use it as an initial pattern for our model. Then we enter a loop where given an input sequence, we predict the next character and then form a new input sequence by appending the predicted character and dropping the first character.\n",
        "\n",
        "Whenever we input a sequence, we obtain as output a probability distribution of the possible characters. For example, given the sequence `the cat and the do` the model will output a probability distribution of the next character where probably the character `g` will have a high probability (forming then `the cat and the dog`). However, the character `c` can also be a possibility, as the sentence formed may be `the cat and the doctor`. When deciding what character to predict, a strategy is to just take the character with the maximum probability at all times, but then the variability of the formed text is then smaller. Another option would be to sample following the same probability of distribution as the model outputs. However, this might result in a excessive variability with some sentences not making any sense.\n",
        "\n",
        "This trade-off of variability against more probable sequences is controlled by what it is called the temperature of the sampling process. The temperature controls the smoothing of the probability vector. A temperature close to 0 will result in taking always the safest (i.e. that with the highest probability) as the next element. A temperature close to 1 will decide on the next element following the same distribution of probability as the original output of the model.\n",
        "\n",
        "In a more formal way, being $p_i$ the probability of the element $i$ output by the RNN model and $T$ the temperature, the probability after applying the mentioned smoothing $\\hat{p_i}$ is:\n",
        "\n",
        "$$\n",
        "\\hat{p_i} = \\frac{e^{\\log(p_i)/T}}{\\sum_j e^{\\log(p_j)/T}}\n",
        "$$\n",
        "\n",
        "You can check how a small $T$ is going to make the element with the largest probability be close to 1 after the process.\n",
        "\n",
        "Before starting the text prediction, let's show an example of how the sampling temperature affects the model choices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loPFAdDvIsWO"
      },
      "outputs": [],
      "source": [
        "def temperature_smoothing(prediction, temperature=1.0, eps=1.0e-9):\n",
        "    prediction = np.asarray(prediction, dtype=np.float64)\n",
        "\n",
        "    if temperature <= 0:\n",
        "        # At T=0, return a one-hot vector at the argmax\n",
        "        one_hot = np.zeros_like(prediction)\n",
        "        one_hot[np.argmax(prediction)] = 1.0\n",
        "        return one_hot\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    logits = np.log(prediction + eps)  # avoid log(0)\n",
        "    logits /= temperature\n",
        "    exp_preds = np.exp(logits - np.max(logits))  # for numerical stability\n",
        "    return exp_preds / np.sum(exp_preds)\n",
        "\n",
        "\n",
        "prediction = np.asarray([0.2, 0.3, 0.1, 0.4])\n",
        "for T in [1.0, 0.5, 0.25, 0.0]:\n",
        "    print(f\"Output probabilities with temp: {T}\")\n",
        "    print(temperature_smoothing(prediction, T))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST0l8UWDJy-t"
      },
      "source": [
        "You can see how a lower temperature makes the small probabilities become smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnpdKtGwIjue"
      },
      "source": [
        "Here you can set the temperature and check how the output varies. You will see that for $T\\approx0$ the generated text has low variability."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(\n",
        "    model,\n",
        "    seed,\n",
        "    n_to_char,\n",
        "    length=300,\n",
        "    temperature=1.0,\n",
        "):\n",
        "    pattern = seed[:]\n",
        "    output = []\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            x_in = torch.tensor(pattern, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "            logits = model(x_in)\n",
        "\n",
        "            if temperature == 0:\n",
        "                # greedy argmax\n",
        "                next_idx = torch.argmax(logits, dim=-1).item()\n",
        "            else:\n",
        "                # temperature scaling\n",
        "                scaled_logits = logits / temperature\n",
        "                probs = torch.softmax(scaled_logits, dim=-1)\n",
        "                next_idx = torch.multinomial(probs.squeeze(0), 1).item()\n",
        "\n",
        "            output.append(n_to_char[next_idx])\n",
        "            pattern = pattern[1:] + [next_idx]\n",
        "\n",
        "    return \"\".join(output)"
      ],
      "metadata": {
        "id": "gVZDJtRly80e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REoPwrqieFLK"
      },
      "outputs": [],
      "source": [
        "# Vary the temperature here\n",
        "temperature = 0.7\n",
        "\n",
        "# We select a random element from the test set as seed\n",
        "start = np.random.randint(0, len(x_test)-1)\n",
        "seed = x_test[start].tolist()\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([n_to_char[i] for i in seed]), \"\\\"\")\n",
        "\n",
        "print(f\"\\nGenerated (T={temperature}):\")\n",
        "print(generate_text(text_gen_model, seed, n_to_char, length=300, temperature=temperature))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSQs-l7OmNoI"
      },
      "source": [
        "Notice how the network has learnt the structure of the text, to start a new line every few words and to put the name of the characters too. The text itself seems grammatically correct in most cases but it fails to make much sense in most examples. Generating sentences with actual semantic meaning is harder for these type of models compared to generating grammatically correct sentences. Test different temperature settings to see how it affects the generation\n",
        "\n",
        "If you want to know more about text generation, along with some extra generated examples, we refer you to [Andrej Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hY4m63VgiM9"
      },
      "source": [
        "### Quantitative evaluation of the generated text\n",
        "\n",
        "For a quantitative evaluation of the generated text we will use a metric commonly used in image captioning and translation tasks, BLEU. BLEU looks for matches on a word level between the generated text and the reference text. Specifically, BLEU looks for matches in n-grams up to $n=4$, where an n-gram is defined as a contiguous sentence of $n$ items. For example, in the sentence `the sky is blue`, an n-gram of $n=2$ would be `is blue`. BLEU scores range from [0,1].\n",
        "\n",
        "As we mentioned, BLEU is usual in more constrained text generation tasks, such as image captioning. In our text generation task there is a large number of both grammatically and semantically correct possibilities when generating new sentences, and one of the issues for this task is to develop a metric that can properly evaluate the semantic and syntactic quality of the generated text. Human evaluation is quite common when evaluating the quality of generated sentences in these less constrained tasks due to the difficulty of finding proper automatic metrics. However, we hope to see a correlation between the BLEU score and the quality of the generated text. Here we take an input sentence from the test data and generate a sentence. Then, we compare the generated sentence to the real one from the corpus. We do so 20 times and provide the average BLEU score.\n",
        "\n",
        "The used function is integrated in the package `nltk` and it is called using the following syntax `sentence_bleu(reference, candidate).`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL6GfMfvXiYj"
      },
      "outputs": [],
      "source": [
        "characters = sorted(list(set(data)))\n",
        "n_to_char = {n: char for n, char in enumerate(characters)}\n",
        "char_to_n = {char: n for n, char in enumerate(characters)}\n",
        "\n",
        "# Vary the temperature here\n",
        "temperature = 0.5\n",
        "n_eval = 20\n",
        "seq_char_length = 100\n",
        "smoother = SmoothingFunction().method1\n",
        "\n",
        "text_gen_model = text_gen_model.to(DEVICE)\n",
        "text_gen_model.eval()\n",
        "\n",
        "bleu_score = 0.0\n",
        "with torch.no_grad():\n",
        "    for _ in range(n_eval):\n",
        "        start = np.random.randint(0, len(x_test) - seq_char_length - 1)\n",
        "        pattern = x_test[start].tolist()\n",
        "\n",
        "        reference_ids = x_test[start+1 : start+1+seq_char_length].flatten().tolist()\n",
        "        reference_text = ''.join([n_to_char[v] for v in reference_ids])\n",
        "\n",
        "        candidate_text = generate_text(\n",
        "            text_gen_model,\n",
        "            pattern,\n",
        "            n_to_char,\n",
        "            length=seq_char_length,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "\n",
        "        reference_tokens = list(reference_text)\n",
        "        candidate_tokens = list(candidate_text)\n",
        "\n",
        "        bleu_score += sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoother)\n",
        "\n",
        "bleu_score /= n_eval\n",
        "print(\"BLEU Score:\", bleu_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEPP1gUthOI0"
      },
      "source": [
        "# Transformers\n",
        "RNNs are still quite used in several architectures and Natural Language Processing (NLP) tasks. However, during the last 3 years (since the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) was published) a new architecture, the Transformer, has started to outperform RNNs in several text/NLP benchmarks. A Transformer does not use any recurrence, it instead uses attention to focus on specific parts of a sentence. If you want an in-depth explanation of the architecture, you can take a look to the [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) blog post.\n",
        "\n",
        "Both text classification and text generation tasks have been succesfully tackled by transformers. Transfomers seem to scale better than recurrent neural networks, they can have great results when using large architectures and large datasets, whereas RNNs seem not to benefit as much from using more parameters and training examples. State-of-the-art transformer models can have even billions of parameters and can benefit from training in TBs of text data.\n",
        "\n",
        "We will show examples for both text classification and text generation of models pretrained on large datasets. The aim of the examples is to show you how these large models trained in diverse/large datasets can transfer their knowledge to several tasks (similarly to the transfer learning/finetuning section in the CNN Architectures notebook). However, we will not implement any transformer model from scratch. If you are interested in doing so, besides the theoretical explanation of the [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) blog post, you can also check the handsâ€‘on tutorial [â€œTransformer Model Tutorial in PyTorch: From Theory to Codeâ€](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch) by DataCamp.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPNbRN4lCkkL"
      },
      "source": [
        "##Text Classification\n",
        "\n",
        "For the text classification task, the model called BERT from Google, and all the subsequent variants have achieved state-of-the-art results. BERT is a method to train language representations, meaning that you then can use those represenations/embeddings to predict the label of the sentence in a text classification task.\n",
        "\n",
        "We will now show a quick example of finetuning BERT in IMDB to show you the capabilities of this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7p4CcVq7VvU"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Load dataset and tokenizer\n",
        "dataset = load_dataset(\"imdb\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True)\n",
        "dataset = dataset.rename_column(\"label\", \"labels\")\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "train_loader = DataLoader(dataset[\"train\"], batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(dataset[\"test\"], batch_size=16)\n",
        "\n",
        "# Model + optimizer + scheduler\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(DEVICE)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "num_training_steps = len(train_loader)\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "for batch in progress_bar:\n",
        "    optimizer.zero_grad()\n",
        "    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "    outputs = model(**batch)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    progress_bar.set_postfix(loss=loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "preds, labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1).cpu().tolist()\n",
        "        preds.extend(predictions)\n",
        "        labels.extend(batch[\"labels\"].cpu().tolist())\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(labels, preds)\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(labels, preds, target_names=[\"Negative\", \"Positive\"]))"
      ],
      "metadata": {
        "id": "059-H2gC8U6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CfPJQK7j3JO"
      },
      "source": [
        "## Text Generation\n",
        "\n",
        "In text generation, the GPT variants from OpenAI have shown impressive results. We will now show an example from a pretrained [GPT2](https://openai.com/blog/better-language-models/) using the [Hugging Face](https://huggingface.co/transformers/index.html#) library, which it contains several state-of-the-art Natural Language Processing (NLP) models. The code to generate text is extracted from this [Colab Notebook](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb#scrollTo=HBtDOdD0wx3l).\n",
        "\n",
        "First, we install the Hugging Face library and load a GPT2 pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj4IgSwrZ58G"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# Use EOS as PAD to avoid warnings\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")  # PyTorch model\n",
        "model.resize_token_embeddings(len(tokenizer))         # adjust for pad token if needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbs0t7nmkKgi"
      },
      "source": [
        "Now, we generate text using as input a part of the Tiny Shakespeare dataset. Note that this model has not been finetuned using the Tiny Shakespeare dataset, instead it has been trained in a larger more diverse dataset. You can run the following code to get different outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzzXQEW0Z9LO"
      },
      "outputs": [],
      "source": [
        "prompt = (\n",
        "    \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\"\n",
        "    \"All:\\nSpeak, speak.\\n\"\n",
        "    \"First Citizen:\\nYou are all resolved rather to die than to famish?\"\n",
        ")\n",
        "\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "sample_output = model.generate(\n",
        "    **inputs,\n",
        "    do_sample=True,\n",
        "    max_length=200,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.8,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + \"-\" * 100)\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBWzhH76cZTr"
      },
      "source": [
        "What is quite impressive about the output of the GPT model is the capability of matching the style of the input text. The model is not finetuned in our dataset, but it still generates characters names and similary grammar/vocabulary to the given input. This is the result of the large capacity of the model and the diverse training data it used.\n",
        "\n",
        "Let's try to change the input to a completely different one, using in this case an input from a rugby game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RN1CBKGdN8W"
      },
      "outputs": [],
      "source": [
        "prompt = (\n",
        "    \"40 mins. Tom Curry receives Stuart Hoggâ€™s kick-off and Youngs kicks poorly to put Scotland back on the attack.\\n\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "sample_output = model.generate(\n",
        "    **inputs,\n",
        "    do_sample=True,\n",
        "    max_length=200,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.8,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + \"-\" * 100)\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z956_MZLetBY"
      },
      "source": [
        "We see how the model understands that the input is related to sports and changes the output style compared to the Tiny Shakespeare example.\n",
        "\n",
        "We just showed examples with GPT2. However, the newer versions of GPT, GPT3, GPT4 and GPT5, which are trained using larger models and datasets, show largely improved results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwawQJMZ3aQM"
      },
      "source": [
        "# Coursework\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKfTmNC7Bj5E"
      },
      "source": [
        "### **Task 1: RNN Regression**\n",
        "\n",
        "In this task, you are asked to estimate the next value of a time series. Specifically, we have selected the popular airline passenger dataset. This dataset contains the number of passengers that travels with a certain airline company. The data contains 144 entries, each entry corresponds to the number of the passengers that travel in a given month. The dataset starts in 1949, and it lasts until 1960.\n",
        "\n",
        "Similarly to the previous example, we import the data and plot it to see the structure."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv"
      ],
      "metadata": {
        "id": "YA2fLF24syKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRf3UGQ33bnN"
      },
      "outputs": [],
      "source": [
        "# Load dataset (only 2nd column)\n",
        "data = pd.read_csv(\"airline-passengers.csv\", usecols=[1], engine=\"python\")\n",
        "\n",
        "# Plot passengers over time\n",
        "data.plot(title=\"Airline Passengers (1949â€“1960)\")\n",
        "plt.xlabel(\"Months\")\n",
        "plt.ylabel(\"Passengers\")\n",
        "plt.show()\n",
        "\n",
        "# Convert to float32 numpy\n",
        "data_np = data.to_numpy(dtype=\"float32\")\n",
        "\n",
        "# Train/test split (70/30)\n",
        "split_idx = int(len(data_np) * 0.7)\n",
        "train_np, test_np = data_np[:split_idx], data_np[split_idx:]\n",
        "\n",
        "print(f\"Training samples: {len(train_np)} | Test samples: {len(test_np)}\")\n",
        "\n",
        "# Scale using MinMaxScaler (fit on train, transform both)\n",
        "scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
        "train_np_norm, test_np_norm = scaler.fit_transform(train_np), scaler.transform(test_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOi9udyY3dHS"
      },
      "source": [
        "First of all, you need to train an RNN on the airline passenger dataset. This exercise expects you to study the impact of the `window_size` variable when defining the `train` and `test` dataset splits. Remember that the `window_size` variable indicates the number of past observations used for predicting the current value. Here, we treat the `test` split as a validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIB08JxD3ezu"
      },
      "outputs": [],
      "source": [
        "def create_dataset(dataset, window_size = 1):\n",
        "    data_x, data_y = [], []\n",
        "    for i in range(len(dataset) - window_size):\n",
        "        sample = dataset[i:(i + window_size), 0]\n",
        "        data_x.append(sample)\n",
        "        data_y.append(dataset[i + window_size, 0])\n",
        "    return np.array(data_x), np.array(data_y)\n",
        "\n",
        "\n",
        "window_size = 1 # Use this variable to build the dataset with different number of inputs\n",
        "\n",
        "# Create test and training sets for regression with different window sizes.\n",
        "train_X, train_Y = create_dataset(train_np_norm, window_size)\n",
        "test_X, test_Y = create_dataset(test_np_norm, window_size)\n",
        "\n",
        "train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))\n",
        "test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 1))\n",
        "\n",
        "# Convert to tensors\n",
        "train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
        "train_Y_tensor = torch.tensor(train_Y, dtype=torch.float32).unsqueeze(1)\n",
        "test_X_tensor = torch.tensor(test_X, dtype=torch.float32)\n",
        "test_Y_tensor = torch.tensor(test_Y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "print(\"Shape of training inputs: \" + str((train_X_tensor.shape)))\n",
        "print(\"Shape of training labels: \" + str((train_Y_tensor.shape)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH6W6t5q3gfN"
      },
      "source": [
        "\n",
        "**Report**:\n",
        "\n",
        "*   Create a plot showing the test curves of models trained with different `window_size` values. Report the plot and discuss the main differences you observe between the predicted curves. You can use the style proposed on the Many to One RNNs - Regression section to plot your curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwb9IfiCLKuT"
      },
      "source": [
        "### **Task 2: Text Embeddings**\n",
        "For this task, we tackle a classification problem using the IMDB sentiment dataset as done in the example in the notebook. Labels in IMDB are 0 for negative reviews and 1 for positive reviews. The definitions of the models you will use for this task are given the code below. This task is similar to the transfer learning/finetuning task in the CNN Architectures notebook, however we now test the effect of transfer learning in the embeddings. In this task we use train, validation and test splits with Early Stopping. That means that we will take the best performing model in the validation set and use it in the test set to get a final performance.\n",
        "\n",
        "**Report**\n",
        "* Using embeddings of dimensionality 1, train a model without using any LSTM, only using an average pooling of the input embeddings (called `embeddings_model` in the code given below). Then train another model with an LSTM and trainable embeddings initialized at random (called `lstm_model`). Finally train a model with an LSTM with non-trainable embeddings initialized with GloVe embeddings (called `lstm_glove_model`). The code to train the three models is given below. Report in a table the test accuracy obtained after training with the given code for the three models. Also attach in the Appendix the training and validation accuracy curves for the different models trained. You can report the curves after using EarlyStopping with patience 10 (already given in the code), so you don't have to train for the full 50 epochs the three models. Discuss the results.\n",
        "\n",
        "* Predict the sentiment of the two given example reviews in the code below for the model trained without a LSTM (`embeddings_model`) and for the model trained with a LSTM and GloVe embeddings (`lstm_glove_model`). Report the predictions (you can use the same table as when reporting test accuracies). Discuss the results. Also discuss the differences you can observe between the GloVe embeddings and the embeddings learnt in `embeddings_model` (e.g. what kind of properties the embeddings encode, or differences in the closest words).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gq0lcyLnaSN"
      },
      "source": [
        "We provide the training code you need to use for this exercise below. First we load the dataset as we did in the tutorial. In this exercise, we will use of train, validation and test splits, which are defined in the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0ibhzVdnZ0X"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "nb_words = 5000\n",
        "maxlen = 100\n",
        "\n",
        "# Load IMDb dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Tokenizer\n",
        "def tokenize(text):\n",
        "    return nltk.word_tokenize(text.lower())\n",
        "\n",
        "# Build vocabulary from training set\n",
        "counter = Counter()\n",
        "for text in dataset['train']['text']:\n",
        "    counter.update(tokenize(text))\n",
        "\n",
        "# Create word2idx with special tokens\n",
        "most_common = counter.most_common(nb_words - 3)\n",
        "word2idx = {'<PAD>': 0, '<START>': 1, '<UNK>': 2}\n",
        "for idx, (word, _) in enumerate(most_common, start=3):\n",
        "    word2idx[word] = idx\n",
        "\n",
        "# Reverse index\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "# Encode and pad a single text\n",
        "def encode(text, maxlen=maxlen):\n",
        "    tokens = tokenize(text)\n",
        "    indices = [word2idx['<START>']] + [word2idx.get(t, word2idx['<UNK>']) for t in tokens]\n",
        "    indices = indices[:maxlen]  # truncate if too long\n",
        "    indices += [word2idx['<PAD>']] * (maxlen - len(indices))  # pad if too short\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Preprocess list of texts\n",
        "def preprocess(text_list):\n",
        "    return torch.stack([encode(text) for text in text_list])\n",
        "\n",
        "# Process datasets\n",
        "x_train_full = preprocess(dataset['train']['text'])\n",
        "y_train_full = torch.tensor(dataset['train']['label'], dtype=torch.long)\n",
        "x_test = preprocess(dataset['test']['text'])\n",
        "y_test = torch.tensor(dataset['test']['label'], dtype=torch.long)\n",
        "\n",
        "# Train/Val split\n",
        "x_val = x_train_full[20000:]\n",
        "y_val = y_train_full[20000:]\n",
        "x_train = x_train_full[:20000]\n",
        "y_train = y_train_full[:20000]\n",
        "\n",
        "# Print shapes\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_val shape:', x_val.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "# Wrap in DataLoaders\n",
        "train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=32)\n",
        "test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqBseveW25fw"
      },
      "outputs": [],
      "source": [
        "# Function implementing early stopping logic\n",
        "\n",
        "def train_with_early_stopping(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    device,\n",
        "    max_epochs=50,\n",
        "    patience=10,\n",
        "    reshape=False\n",
        "):\n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "    epochs_no_improve = 0\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # --- Training ---\n",
        "        model.train()\n",
        "        train_loss, train_correct, total = 0.0, 0, 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "            if reshape:\n",
        "              loss = criterion(outputs, y_batch.unsqueeze(1).float())\n",
        "            else:\n",
        "              loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "            preds = (torch.sigmoid(outputs) >= 0.5).long()\n",
        "            train_correct += (preds.view(-1) == y_batch.long()).sum().item()\n",
        "            total += x_batch.size(0)\n",
        "\n",
        "        avg_train_loss = train_loss / total\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in val_loader:\n",
        "                x_val, y_val = x_val.to(device), y_val.to(device).float()\n",
        "                outputs = model(x_val)\n",
        "                if reshape:\n",
        "                  loss = criterion(outputs, y_val.unsqueeze(1).float())\n",
        "                else:\n",
        "                  loss = criterion(outputs, y_val)\n",
        "\n",
        "                val_loss += loss.item() * x_val.size(0)\n",
        "                preds = (torch.sigmoid(outputs) >= 0.5).long()\n",
        "                val_correct += (preds.view(-1) == y_val.long()).sum().item()\n",
        "                val_total += x_val.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "        # --- Early stopping logic ---\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "            print(\"  â†³ New best model saved\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    # --- Final test evaluation ---\n",
        "    model.eval()\n",
        "    test_loss, test_correct, test_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(device), y_test.to(device).float()\n",
        "            outputs = model(x_test)\n",
        "            if reshape:\n",
        "              loss = criterion(outputs, y_test.unsqueeze(1).float())\n",
        "            else:\n",
        "              loss = criterion(outputs, y_test)\n",
        "\n",
        "            test_loss += loss.item() * x_test.size(0)\n",
        "            preds = (torch.sigmoid(outputs) >= 0.5).long()\n",
        "            test_correct += (preds.view(-1) == y_test.long()).sum().item()\n",
        "            test_total += x_test.size(0)\n",
        "\n",
        "    final_test_loss = test_loss / test_total\n",
        "    final_test_acc = test_correct / test_total\n",
        "\n",
        "    print(f\"Final Test Loss: {final_test_loss:.4f}\")\n",
        "    print(f\"Final Test Accuracy: {final_test_acc:.4f}\")\n",
        "\n",
        "    return history, final_test_loss, final_test_acc, best_model_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EDi21HPGU3h"
      },
      "source": [
        "The following code includes the model that uses embeddings of size 1 (so each word is only represented by a single digit) and averages them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7421cnIYoPQ-"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class EmbeddingsModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=1):\n",
        "        super(EmbeddingsModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.global_avg_pool = lambda x: x.mean(dim=1)\n",
        "        self.fc = nn.Linear(embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)                     # [batch_size, maxlen, 1]\n",
        "        x = self.global_avg_pool(x)               # [batch_size, 1]\n",
        "        x = self.fc(x)                            # [batch_size, 1]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK0HXie6FN-t"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Instantiate model\n",
        "embeddings_model = EmbeddingsModel(\n",
        "    vocab_size=len(word2idx)\n",
        ").to(DEVICE)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(embeddings_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Call the training function\n",
        "history, test_loss, test_acc, best_embeddings_model_state = train_with_early_stopping(\n",
        "    model=embeddings_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    device=DEVICE,\n",
        "    reshape=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRm3ql3UE9w7"
      },
      "source": [
        "We use Early Stopping, so the best validation model is then used to compute the result in the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_DTYhcl68GF"
      },
      "source": [
        "Now we have `embedding_model` trained. The code below will print the embedding of any `query_word`, which in this case is a single number. We also give you the code to compute the `top_k` closest embeddings to `query_word`. The metric used is the L2 distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9uJoVAt7EWk"
      },
      "outputs": [],
      "source": [
        "def get_most_similar_words(best_model_state, model_class, parameters,\n",
        "                           query, word2idx, idx2word, top_k=10, is_index=False):\n",
        "    \"\"\"\n",
        "    query: can be a word (like 'cat') or an index (like 1 if is_index=True)\n",
        "    \"\"\"\n",
        "    # Load model and set to eval mode\n",
        "    model = model_class(**parameters)\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.eval()\n",
        "\n",
        "    # Get embeddings\n",
        "    embeddings = model.embedding.weight.data.cpu().numpy()\n",
        "\n",
        "    # Handle query input\n",
        "    if is_index:\n",
        "        query_idx = int(query)\n",
        "        query_word = idx2word.get(query_idx, \"<UNK>\")\n",
        "    else:\n",
        "        query_word = query\n",
        "        if query_word not in word2idx:\n",
        "            print(f\"Word '{query_word}' not found in vocab.\")\n",
        "            return\n",
        "        query_idx = word2idx[query_word]\n",
        "\n",
        "    query_vector = embeddings[query_idx]\n",
        "\n",
        "    # Compute L2 distances\n",
        "    distances = ((embeddings - query_vector) ** 2).sum(axis=1)\n",
        "    nearest_indices = distances.argsort()[1:top_k+1]\n",
        "\n",
        "    print(f\"Query index: {query_idx}\")\n",
        "    print(f\"Query word: '{query_word}'\")\n",
        "    print(f\"Embedding value of '{query_word}' is {query_vector[0]:.6f}\")\n",
        "    print(f\"Most {top_k} similar words to '{query_word}':\")\n",
        "    for rank, idx in enumerate(nearest_indices, start=1):\n",
        "        print(f\"{rank}: {idx2word.get(idx, '<UNK>')}\")\n",
        "\n",
        "# Example usage\n",
        "parameters = {\n",
        "    \"vocab_size\": len(word2idx)\n",
        "}\n",
        "\n",
        "get_most_similar_words(\n",
        "    best_model_state=best_embeddings_model_state,\n",
        "    model_class=EmbeddingsModel,\n",
        "    parameters=parameters,\n",
        "    query='8',            # Query parameter\n",
        "    word2idx=word2idx,\n",
        "    idx2word=idx2word,\n",
        "    top_k=10,\n",
        "    is_index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhD9hosS7cKQ"
      },
      "source": [
        "The code below gives the prediction for two example reviews we input. Remember that predictions close to 0 refer to a negative review, and predictions close to 1 refer to a positive review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvc9M9jB589r"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text, model_class, best_model_state, word2idx, parameters, maxlen, device='cpu'):\n",
        "    # Re-instantiate and load model\n",
        "    model = model_class(**parameters)\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and encode\n",
        "    tokens = text.lower().split()\n",
        "    encoded = [word2idx.get('<START>', 1)] + [word2idx.get(w, word2idx['<UNK>']) for w in tokens]\n",
        "    tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0)  # batch of 1\n",
        "\n",
        "    # Pad or truncate\n",
        "    if tensor.size(1) < maxlen:\n",
        "        pad_len = maxlen - tensor.size(1)\n",
        "        tensor = F.pad(tensor, (0, pad_len), value=word2idx['<PAD>'])\n",
        "    else:\n",
        "        tensor = tensor[:, :maxlen]\n",
        "\n",
        "    tensor = tensor.to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        output = model(tensor)\n",
        "        prob = torch.sigmoid(output).item()\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFAWpl-X5_y9"
      },
      "outputs": [],
      "source": [
        "neg_review = \"the movie is boring and not good\"\n",
        "pos_review = \"the movie is good and not boring\"\n",
        "\n",
        "neg_score = predict_sentiment(\n",
        "    text=neg_review,\n",
        "    model_class=EmbeddingsModel,\n",
        "    best_model_state=best_embeddings_model_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "pos_score = predict_sentiment(\n",
        "    text=pos_review,\n",
        "    model_class=EmbeddingsModel,\n",
        "    best_model_state=best_embeddings_model_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "print(f\"The score for the negative review is: {neg_score}\")\n",
        "print(f\"The score for the positive review is: {pos_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FzItW89aLVj"
      },
      "source": [
        "With the above code, we trained a model that classifies the sentiment of the sentence using the average of all the embeddings, which were only of size 1. Now we will increase the capacity of the embeddings to 300 and will also add a LSTM to process the embeddings. Hence, the model has a much higher capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-z4K9iAaKR1"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=300, lstm_units=50, dropout=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        ### Do not modify the layers below\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, num_layers=2, dropout=dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(lstm_units, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)                     # [batch_size, seq_len, embedding_dim]\n",
        "        x = self.dropout1(x)\n",
        "        output, (hidden, _) = self.lstm(x)        # hidden: [1, batch_size, lstm_units]\n",
        "        x = self.dropout2(hidden[-1])             # Take the final hidden state\n",
        "        x = self.fc(x)                            # [batch_size, 1]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msBDHkgfFhtq"
      },
      "source": [
        "Similarly, we use EarlyStopping for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCAyRtjo-s0Z"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Instantiate model\n",
        "lstm_model = LSTMModel(\n",
        "    vocab_size=len(word2idx),\n",
        "    embedding_dim=300,\n",
        "    lstm_units=50\n",
        ").to(DEVICE)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Call the training function\n",
        "history, test_loss, test_acc, best_lstm_state = train_with_early_stopping(\n",
        "    model=lstm_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    device=DEVICE,\n",
        "    reshape=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR31KTPAAqzZ"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    \"vocab_size\": len(word2idx)\n",
        "}\n",
        "\n",
        "neg_review = \"the movie is boring and not good\"\n",
        "pos_review = \"the movie is good and not boring\"\n",
        "\n",
        "neg_score = predict_sentiment(\n",
        "    text=neg_review,\n",
        "    model_class=LSTMModel,\n",
        "    best_model_state=best_lstm_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "pos_score = predict_sentiment(\n",
        "    text=pos_review,\n",
        "    model_class=LSTMModel,\n",
        "    best_model_state=best_lstm_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "print(f\"The score for the negative review is: {neg_score}\")\n",
        "print(f\"The score for the positive review is: {pos_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3F0IRz2FqVX"
      },
      "source": [
        "We just trained a model with a large number of parameters in the IMDB, which is a small dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7dnHR3za1xB"
      },
      "source": [
        "The last model we train is the same model as the `lstm_model` above, but in this case we use the embeddings from the GloVe method (which were introduced in this notebook) without any finetuning. First, we download them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7ZjGnuWb4H9"
      },
      "outputs": [],
      "source": [
        "!wget https://imperialcollegelondon.box.com/shared/static/c9trfhhwl9ohje5g3sapu3xk2zoywp3c.txt -O glove_vectors.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vhK-7g7GIZX"
      },
      "source": [
        "Then we load the GloVe embeddings with dimensionality 300 we just downloaded. This takes some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XzERyfEC-sl"
      },
      "outputs": [],
      "source": [
        "# Load GloVe vectors and build vocab and weight matrix\n",
        "def load_glove(glove_path, embedding_dim):\n",
        "    vocab = {}\n",
        "    vectors = []\n",
        "    skipped = 0\n",
        "\n",
        "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        first_line = f.readline()\n",
        "        if len(first_line.strip().split()) == 2:\n",
        "            print(f\"Skipping header: {first_line.strip()}\")\n",
        "        else:\n",
        "            f.seek(0)\n",
        "\n",
        "        for line in f:\n",
        "            tokens = line.rstrip().split()\n",
        "            word, values = tokens[0], tokens[1:]\n",
        "\n",
        "            if len(values) != embedding_dim:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                vector = np.array(values, dtype=np.float32)\n",
        "            except ValueError:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            vocab[word] = len(vectors)\n",
        "            vectors.append(torch.from_numpy(vector))\n",
        "\n",
        "    if not vectors:\n",
        "        raise RuntimeError(\"No valid embeddings loaded. Check file format.\")\n",
        "\n",
        "    weight_matrix = torch.stack(vectors)\n",
        "    print(f\"Loaded {len(vectors)} word vectors. Skipped {skipped} lines.\")\n",
        "    return vocab, weight_matrix\n",
        "\n",
        "# Example\n",
        "glove_path = \"glove_vectors.txt\"\n",
        "vocab, weight_matrix = load_glove(glove_path, embedding_dim=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0kqDg7ocd5t"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 300\n",
        "embedding_matrix = torch.zeros((nb_words, embedding_dim))\n",
        "\n",
        "# Align GloVe vectors to our vocabulary\n",
        "for word, idx in word2idx.items():\n",
        "    if idx >= nb_words:\n",
        "        continue  # Skip words beyond vocab limit\n",
        "    if word in vocab:\n",
        "        embedding_matrix[idx] = weight_matrix[vocab[word]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXEgmD5CdbEo"
      },
      "source": [
        "To initialize the PyTorch Embedding layer with the embeddings we loaded, we can use the function `nn.Embedding.from_pretrained`. Also, to freeze the embeddings during training, we use `freeze=freeze`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeKMwpxIDd5A"
      },
      "outputs": [],
      "source": [
        "class LSTMWithGloVe(nn.Module):\n",
        "    def __init__(self, embedding_tensor, lstm_units=50, dropout=0.2, freeze=True):\n",
        "        super(LSTMWithGloVe, self).__init__()\n",
        "\n",
        "        # Define embedding layer from pretrained tensor\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=freeze)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_tensor.shape[1],  # Embedding dim\n",
        "            hidden_size=lstm_units,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            num_layers=2\n",
        "        )\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(lstm_units, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)                     # [batch_size, seq_len, embedding_dim]\n",
        "        x = self.dropout1(x)\n",
        "        output, (hidden, _) = self.lstm(x)        # hidden: [1, batch_size, lstm_units]\n",
        "        x = self.dropout2(hidden[-1])             # Final hidden state\n",
        "        x = self.fc(x)                            # [batch_size, 1]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqNwoQE9D69E"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Instantiate model\n",
        "lstm_glove_model = LSTMWithGloVe(\n",
        "    embedding_tensor=embedding_matrix,\n",
        "    lstm_units=50\n",
        ").to(DEVICE)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(lstm_glove_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Call the training function\n",
        "history, test_loss, test_acc, best_lstm_glove_state = train_with_early_stopping(\n",
        "    model=lstm_glove_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    device=DEVICE,\n",
        "    reshape=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0v3mUULGkfV"
      },
      "source": [
        "We can also compute the closest words in the GloVe embeddings to any `query_word` using the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIJqzFFBgdJM"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    \"embedding_tensor\": embedding_matrix\n",
        "}\n",
        "\n",
        "get_most_similar_words(\n",
        "    best_model_state=best_lstm_glove_state,\n",
        "    model_class=LSTMWithGloVe,\n",
        "    parameters=parameters,\n",
        "    query='8',              # Query parameter\n",
        "    word2idx=word2idx,\n",
        "    idx2word=idx2word,\n",
        "    top_k=10,\n",
        "    is_index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXiBKBgtG2xf"
      },
      "source": [
        "We use the same example reviews as for the `EmbeddingsModel` case and we compute the predictions using the `LSTMWithGloVe`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvGmp625hr39"
      },
      "outputs": [],
      "source": [
        "neg_review = \"the movie is boring and not good\"\n",
        "pos_review = \"the movie is good and not boring\"\n",
        "\n",
        "neg_score = predict_sentiment(\n",
        "    text=neg_review,\n",
        "    model_class=LSTMWithGloVe,\n",
        "    best_model_state=best_lstm_glove_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "pos_score = predict_sentiment(\n",
        "    text=pos_review,\n",
        "    model_class=LSTMWithGloVe,\n",
        "    best_model_state=best_lstm_glove_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "print(f\"The score for the negative review is: {neg_score}\")\n",
        "print(f\"The score for the positive review is: {pos_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CywiPmy1eZoF"
      },
      "source": [
        "### **Task 3: Text Generation**\n",
        "In this task we focus on the text generation problem. For this purpose, we will download the scripts of the TV show Game of Thrones and try to generate some text resembling the style of the scripts.\n",
        "\n",
        "\n",
        "**Report**\n",
        "* Plot the retrieved BLEU for different temperature values (from 0 to 2 in the x-axis) for both the character-level model and the word-level model. To compute the BLEU score, use a minimum of 20 generated samples per temperature used to reduce variability (you can increase it at the cost of higher computational time for lower variability). Each sample should contain 100 characters for the char-level model or 30 words for the word-level model (the code given uses these parameters by default). Do you see any relationship between the obtained BLEU score and temperature used? If you generate sentences at different temperatures what differences can you observe? Are the generated sentences grammatically correct? Do the generated sentences make sense?\n",
        "\n",
        "We give below the code needed to download the dataset and to compute the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD9gEFKXqukY"
      },
      "source": [
        "We first download and read the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_I1pRickXKl"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/shekharkoirala/Game_of_Thrones\n",
        "\n",
        "data = open('./Game_of_Thrones/Data/final_data.txt', 'r').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHqzKZQ8aYkp"
      },
      "source": [
        "**Character-level model**\n",
        "\n",
        "We first include the code to build the character-level dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnwDgj_unBhW"
      },
      "outputs": [],
      "source": [
        "# Vocabulary\n",
        "characters = sorted(set(data))\n",
        "n_to_char = {i: ch for i, ch in enumerate(characters)}\n",
        "char_to_n = {ch: i for i, ch in enumerate(characters)}\n",
        "\n",
        "# Sliding windows\n",
        "seq_char_length = 100\n",
        "x_char = np.array([\n",
        "    [char_to_n[ch] for ch in data[i:i+seq_char_length]]\n",
        "    for i in range(len(data) - seq_char_length)\n",
        "], dtype=np.int64)\n",
        "\n",
        "y_char = np.array([\n",
        "    char_to_n[data[i+seq_char_length]]\n",
        "    for i in range(len(data) - seq_char_length)\n",
        "], dtype=np.int64)\n",
        "\n",
        "print(\"Total Samples:\", len(x_char))\n",
        "print(\"x_char shape:\", x_char.shape, \"y_char shape:\", y_char.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcmI8iV7afK5"
      },
      "source": [
        "The splits used for training are given below, although we already give the model trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VkGsFkMnCuB"
      },
      "outputs": [],
      "source": [
        "# Sizes\n",
        "n_samples = len(x_char)\n",
        "n_samples_train = int(n_samples * 0.7)\n",
        "n_samples_test  = int(n_samples * 0.2)\n",
        "n_samples_val   = n_samples - n_samples_train - n_samples_test\n",
        "\n",
        "# Train/val/test splits\n",
        "x_train_char = x_char[:n_samples_train]\n",
        "y_train_char = y_char[:n_samples_train]\n",
        "\n",
        "x_val_char   = x_char[n_samples_train:n_samples_train + n_samples_val]\n",
        "y_val_char   = y_char[n_samples_train:n_samples_train + n_samples_val]\n",
        "\n",
        "x_test_char  = x_char[n_samples_train + n_samples_val:]\n",
        "y_test_char  = y_char[n_samples_train + n_samples_val:]\n",
        "\n",
        "# Convert all to torch tensors in one go\n",
        "to_tensor = lambda arr: torch.tensor(arr, dtype=torch.long)\n",
        "\n",
        "x_train_char, y_train_char = map(to_tensor, (x_train_char, y_train_char))\n",
        "x_val_char,   y_val_char   = map(to_tensor, (x_val_char, y_val_char))\n",
        "x_test_char,  y_test_char  = map(to_tensor, (x_test_char, y_test_char))\n",
        "\n",
        "print(f\"x_train_char: {x_train_char.shape}, y_train_char: {y_train_char.shape}\")\n",
        "print(f\"x_val_char:   {x_val_char.shape},   y_val_char:   {y_val_char.shape}\")\n",
        "print(f\"x_test_char:  {x_test_char.shape},  y_test_char:  {y_test_char.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcyzP955aukz"
      },
      "source": [
        "The definition of the model is the one given below. You will not train the model, so this piece of code is only for you to know what kind of model we trained for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO3FoRM1nEOe"
      },
      "outputs": [],
      "source": [
        "# define the LSTM model\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size=300, lstm_units=256):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, lstm_units, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_units, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)                 # (B, T, E)\n",
        "        out, _ = self.lstm(emb)                 # (B, T, H)\n",
        "        last = out[:, -1, :]                    # (B, H)  last time step\n",
        "        logits = self.fc(last)                  # (B, V)\n",
        "        return logits                           # raw logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSgGhwym_WLx"
      },
      "source": [
        "As the training takes a while, we include a saved model that you can load to skip the training step. Use this model to compute your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J6m8nQmt1eO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "CODE USED FOR TRAINING (DO NOT RUN IT!)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# ---- config ----\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "patience = 10\n",
        "lr = 1e-3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "vocab_size = len(characters)\n",
        "model = CharLSTM(vocab_size=vocab_size, embedding_size=300, lstm_units=256).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# ---- data loaders (labels are integer class ids) ----\n",
        "train_ds = TensorDataset(x_train_char, y_train_char)\n",
        "val_ds   = TensorDataset(x_val_char,   y_val_char)\n",
        "test_ds  = TensorDataset(x_test_char,  y_test_char)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
        "\n",
        "# ---- early stopping ----\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, mode='max', min_delta=0.0, restore_best=True):\n",
        "        self.patience = patience\n",
        "        self.mode = mode  # 'max' for accuracy\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best = restore_best\n",
        "        self.best_score = None\n",
        "        self.counter = 0\n",
        "        self.best_state = None\n",
        "        self.best_epoch = 0\n",
        "\n",
        "    def step(self, score, model, epoch):\n",
        "        improve = False\n",
        "        if self.best_score is None:\n",
        "            improve = True\n",
        "        else:\n",
        "            if self.mode == 'max':\n",
        "                improve = score > (self.best_score + self.min_delta)\n",
        "            else:\n",
        "                improve = score < (self.best_score - self.min_delta)\n",
        "\n",
        "        if improve:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            if self.restore_best:\n",
        "                self.best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
        "            self.best_epoch = epoch\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter > self.patience  # stop if patience exceeded\n",
        "\n",
        "    def restore(self, model):\n",
        "        if self.restore_best and self.best_state is not None:\n",
        "            model.load_state_dict(self.best_state)\n",
        "\n",
        "early_stop = EarlyStopping(patience=patience, mode='max', min_delta=0.0, restore_best=True)\n",
        "\n",
        "# ---- history ----\n",
        "history = {\n",
        "    'loss': [],\n",
        "    'val_loss': [],\n",
        "    'accuracy': [],\n",
        "    'val_accuracy': []\n",
        "}\n",
        "\n",
        "def accuracy_from_logits(logits, targets):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "# ---- training loop ----\n",
        "best_path = 'char_gen_model.pth'\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # train\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)            # [B, V]\n",
        "        loss = criterion(logits, yb)  # yb: [B] integer targets\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc  += accuracy_from_logits(logits, yb)\n",
        "        n_batches  += 1\n",
        "\n",
        "    train_loss = epoch_loss / n_batches\n",
        "    train_acc  = epoch_acc  / n_batches\n",
        "\n",
        "    # validate\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_acc  = 0.0\n",
        "    n_val    = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            val_loss += loss.item()\n",
        "            val_acc  += accuracy_from_logits(logits, yb)\n",
        "            n_val    += 1\n",
        "\n",
        "    val_loss /= max(1, n_val)\n",
        "    val_acc  /= max(1, n_val)\n",
        "\n",
        "    # record history\n",
        "    history['loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['accuracy'].append(train_acc)\n",
        "    history['val_accuracy'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch:3d}/{epochs}  \"\n",
        "          f\"loss={train_loss:.4f}  acc={train_acc:.4f}  \"\n",
        "          f\"val_loss={val_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # early stopping on val_accuracy\n",
        "    stop = early_stop.step(val_acc, model, epoch)\n",
        "\n",
        "    # save best\n",
        "    if early_stop.best_state is not None and early_stop.best_epoch == epoch:\n",
        "        torch.save(early_stop.best_state, best_path)\n",
        "\n",
        "    if stop:\n",
        "        print(f\"Early stopping at epoch {epoch}. Restoring best epoch {early_stop.best_epoch}...\")\n",
        "        break\n",
        "\n",
        "# restore best weights\n",
        "early_stop.restore(model)\n",
        "torch.save(model.state_dict(), best_path)\n",
        "\n",
        "# ---- test evaluation ----\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_acc = 0.0\n",
        "n_test = 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        test_loss += loss.item()\n",
        "        test_acc  += accuracy_from_logits(logits, yb)\n",
        "        n_test    += 1\n",
        "\n",
        "test_loss /= max(1, n_test)\n",
        "test_acc  /= max(1, n_test)\n",
        "\n",
        "print(f\"\\nFinal test loss is: {test_loss:.4f}\")\n",
        "print(f\"Final test accuracy is: {test_acc:.4f}\")\n",
        "print(f\"Best model saved to: {best_path}\")\n",
        "files.download(best_path)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr7SMrHtfim_"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "!wget 'https://raw.githubusercontent.com/MatchLab-Imperial/deep-learning-course/master/asset/05_RNN/char_gen_model.pth' -O char_gen_model.pth\n",
        "\n",
        "vocab_size = len(characters)\n",
        "model = CharLSTM(vocab_size=vocab_size, embedding_size=300, lstm_units=256).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"char_gen_model.pth\", map_location=DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsuUE8vua77e"
      },
      "source": [
        "The code you need to evaluate the BLEU score is given below. Vary the temperature to the different needed values. It takes around 1 minute in average per temperature if `n_eval` is set to 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5ufMSdgp0A6"
      },
      "outputs": [],
      "source": [
        "characters = sorted(list(set(data)))\n",
        "n_to_char = {n: char for n, char in enumerate(characters)}\n",
        "char_to_n = {char: n for n, char in enumerate(characters)}\n",
        "\n",
        "# Vary the temperature here\n",
        "temperature = 0.5\n",
        "n_eval = 20\n",
        "seq_char_length = 100\n",
        "smoother = SmoothingFunction().method1\n",
        "bleu_score = 0.0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(n_eval):\n",
        "      # Randomly select a starting point in the test data\n",
        "      start = np.random.randint(0, len(x_test_char) - seq_char_length - 1)\n",
        "      pattern = x_test_char[start].tolist()\n",
        "      reference = x_test_char[start + seq_char_length].tolist()\n",
        "\n",
        "      # Convert reference from numbers to characters\n",
        "      reference = ''.join([n_to_char[value] for value in reference])\n",
        "\n",
        "      # Generate characters using the model\n",
        "      output_sent = ''\n",
        "      for _n in range(seq_char_length):\n",
        "          x = torch.tensor(pattern, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "          logits = model(x)\n",
        "          temp = float(temperature) + 0.01\n",
        "          probs = F.softmax((logits + 1e-7) / temp, dim=-1)\n",
        "          idx = torch.multinomial(probs[0], num_samples=1).item()\n",
        "          output_sent += n_to_char[idx]\n",
        "          pattern.append(idx)\n",
        "          pattern = pattern[1:]\n",
        "\n",
        "      # Preprocess reference and candidate text\n",
        "      reference = word_tokenize(reference.lower())\n",
        "      candidate = word_tokenize(output_sent.lower())\n",
        "      reference = list(filter(lambda x: x != '', reference))\n",
        "      candidate = list(filter(lambda x: x != '', candidate))\n",
        "\n",
        "      # Remove incomplete words at the beginning and end of both lists\n",
        "      if len(reference) > 2:\n",
        "          reference = reference[1:-1]\n",
        "      if len(candidate) > 2:\n",
        "          candidate = candidate[1:-1]\n",
        "\n",
        "      # Compute BLEU score\n",
        "      bleu_score += sentence_bleu([reference], candidate, smoothing_function=smoother)\n",
        "\n",
        "bleu_score /= n_eval\n",
        "print(\"BLEU Score:\", bleu_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVEF_WPycOnD"
      },
      "source": [
        "The code below allows you to generate sentences for different input patterns and different temperature values. You can test how the temperature values affect the quality of the output sentences for the character-level model by generating a few examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yshz7OZGnIWX"
      },
      "outputs": [],
      "source": [
        "# Change the temperature here\n",
        "temperature = 0.7\n",
        "seed_text = \"TYRION pours himself some wine and drinks it down. He pours another glass, and walks back to CERSEI \"\n",
        "pattern = [char_to_n[ch] for ch in seed_text[:seq_char_length]]\n",
        "\n",
        "print(\"\\nPredicted:\")\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(300):\n",
        "        x = torch.tensor(pattern, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "        logits = model(x)\n",
        "\n",
        "        if temperature == 0:\n",
        "            idx = torch.argmax(logits, dim=-1).item()\n",
        "        else:\n",
        "            probs = F.softmax(logits / temperature, dim=-1)\n",
        "            idx = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "        ch = n_to_char[idx]\n",
        "        sys.stdout.write(ch)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        pattern = pattern[1:] + [idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeNwJWRN0Mue"
      },
      "source": [
        "**Word-level model**\n",
        "\n",
        "We now give the code to run the word-level model. The code is similar to the char-level model. The main difference is that we only try to predict the 2000 words most commonly used in the dataset. The reason for this limitation is to limit the size of the output layer and number of input embeddings for memory constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-XfzutEv9Ta"
      },
      "outputs": [],
      "source": [
        "n_words = 2000\n",
        "seq_length = 30\n",
        "\n",
        "# Preprocess text: lowercase + space around punctuation\n",
        "data_p = (\n",
        "    data.replace('.', ' . ').replace(',', ' , ').replace(':', ' : ')\n",
        "        .replace('?', ' ? ').replace('!', ' ! ')\n",
        "        .replace('\\n', ' \\n ').replace('[', ' [ ').replace(']', ' ] ')\n",
        "        .replace(')', ' ) ').replace('(', ' ( ').lower().split()\n",
        ")\n",
        "data_p = [tok for tok in data_p if tok.strip()]\n",
        "\n",
        "# Build vocab (most common words)\n",
        "common = [w for w, _ in Counter(data_p).most_common(n_words)]\n",
        "word_to_n = {w: i for i, w in enumerate(common)}\n",
        "n_to_word = {i: w for i, w in enumerate(common)}\n",
        "OOV_IDX = len(word_to_n)\n",
        "\n",
        "# Build dataset\n",
        "x_word, y_word = [], []\n",
        "for i in range(len(data_p) - seq_length):\n",
        "    seq = [word_to_n.get(w, OOV_IDX) for w in data_p[i:i+seq_length]]\n",
        "    label = data_p[i + seq_length]\n",
        "    if label in word_to_n:  # only predict in-vocab words\n",
        "        x_word.append(seq)\n",
        "        y_word.append(word_to_n[label])\n",
        "\n",
        "n_samples = len(x_word)\n",
        "print(\"Total Samples:\", n_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teAOmchqxvQd"
      },
      "outputs": [],
      "source": [
        "n_samples = len(x_word)\n",
        "n_samples_train = int(n_samples * 0.7)\n",
        "n_samples_test  = int(n_samples * 0.2)\n",
        "n_samples_val   = n_samples - n_samples_train - n_samples_test\n",
        "\n",
        "# Train / val / test splits\n",
        "x_train_word = x_word[:n_samples_train]\n",
        "y_train_word = y_word[:n_samples_train]\n",
        "\n",
        "x_val_word   = x_word[n_samples_train:n_samples_train+n_samples_val]\n",
        "y_val_word   = y_word[n_samples_train:n_samples_train+n_samples_val]\n",
        "\n",
        "x_test_word  = x_word[n_samples_train+n_samples_val:]\n",
        "y_test_word  = y_word[n_samples_train+n_samples_val:]\n",
        "\n",
        "# Convert labels to numpy first\n",
        "y_train_word = np.array(y_train_word)\n",
        "y_val_word   = np.array(y_val_word)\n",
        "y_test_word  = np.array(y_test_word)\n",
        "\n",
        "# Convert all to tensors\n",
        "x_train_word = torch.tensor(x_train_word, dtype=torch.long)\n",
        "x_val_word   = torch.tensor(x_val_word,   dtype=torch.long)\n",
        "x_test_word  = torch.tensor(x_test_word,  dtype=torch.long)\n",
        "\n",
        "y_train_word = torch.tensor(y_train_word, dtype=torch.long)\n",
        "y_val_word   = torch.tensor(y_val_word,   dtype=torch.long)\n",
        "y_test_word  = torch.tensor(y_test_word,  dtype=torch.long)\n",
        "\n",
        "# Shapes\n",
        "print(f\"x_train_word: {x_train_word.shape}, y_train_word: {y_train_word.shape}\")\n",
        "print(f\"x_val_word:   {x_val_word.shape},   y_val_word:   {y_val_word.shape}\")\n",
        "print(f\"x_test_word:  {x_test_word.shape},  y_test_word:  {y_test_word.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJvV2NaP0yam"
      },
      "source": [
        "The definition of the word-level model we train is given below. The model is the same as in the char-level case, the only difference is the size of the output vector and the number of input embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myJRka3BxyUf"
      },
      "outputs": [],
      "source": [
        "# define the LSTM model\n",
        "class WordLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size=300, lstm_units=256):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, lstm_units, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_units, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)           # (B, T, E)\n",
        "        out, _ = self.lstm(emb)           # (B, T, H)\n",
        "        last = out[:, -1, :]              # (B, H) last timestep\n",
        "        logits = self.fc(last)            # (B, V)\n",
        "        return logits                     # raw logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGaHG_ZfoaxC"
      },
      "source": [
        "As with the character-level model, training the word-level model takes a while. Use the saved model we included to compute your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcw8vzdQyROI"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "CODE USED FOR TRAINING (DO NOT RUN IT!)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# ---- config ----\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "patience = 10\n",
        "lr = 1e-3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "vocab_size = n_words + 1\n",
        "model = WordLSTM(vocab_size=vocab_size, embedding_size=300, lstm_units=256).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# ---- data loaders (labels are integer class ids) ----\n",
        "train_ds = TensorDataset(x_train_word, y_train_word)\n",
        "val_ds   = TensorDataset(x_val_word,   y_val_word)\n",
        "test_ds  = TensorDataset(x_test_word,  y_test_word)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
        "\n",
        "# ---- early stopping ----\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, mode='max', min_delta=0.0, restore_best=True):\n",
        "        self.patience = patience\n",
        "        self.mode = mode  # 'max' for accuracy\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best = restore_best\n",
        "        self.best_score = None\n",
        "        self.counter = 0\n",
        "        self.best_state = None\n",
        "        self.best_epoch = 0\n",
        "\n",
        "    def step(self, score, model, epoch):\n",
        "        improve = False\n",
        "        if self.best_score is None:\n",
        "            improve = True\n",
        "        else:\n",
        "            if self.mode == 'max':\n",
        "                improve = score > (self.best_score + self.min_delta)\n",
        "            else:\n",
        "                improve = score < (self.best_score - self.min_delta)\n",
        "\n",
        "        if improve:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            if self.restore_best:\n",
        "                self.best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
        "            self.best_epoch = epoch\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter > self.patience  # stop if patience exceeded\n",
        "\n",
        "    def restore(self, model):\n",
        "        if self.restore_best and self.best_state is not None:\n",
        "            model.load_state_dict(self.best_state)\n",
        "\n",
        "early_stop = EarlyStopping(patience=patience, mode='max', min_delta=0.0, restore_best=True)\n",
        "\n",
        "# ---- history ----\n",
        "history = {\n",
        "    'loss': [],\n",
        "    'val_loss': [],\n",
        "    'accuracy': [],\n",
        "    'val_accuracy': []\n",
        "}\n",
        "\n",
        "def accuracy_from_logits(logits, targets):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "# ---- training loop ----\n",
        "best_path = 'word_gen_model.pth'\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # train\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)            # [B, V]\n",
        "        loss = criterion(logits, yb)  # yb: [B] integer targets\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc  += accuracy_from_logits(logits, yb)\n",
        "        n_batches  += 1\n",
        "\n",
        "    train_loss = epoch_loss / n_batches\n",
        "    train_acc  = epoch_acc  / n_batches\n",
        "\n",
        "    # validate\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_acc  = 0.0\n",
        "    n_val    = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            val_loss += loss.item()\n",
        "            val_acc  += accuracy_from_logits(logits, yb)\n",
        "            n_val    += 1\n",
        "\n",
        "    val_loss /= max(1, n_val)\n",
        "    val_acc  /= max(1, n_val)\n",
        "\n",
        "    # record history\n",
        "    history['loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['accuracy'].append(train_acc)\n",
        "    history['val_accuracy'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch:3d}/{epochs}  \"\n",
        "          f\"loss={train_loss:.4f}  acc={train_acc:.4f}  \"\n",
        "          f\"val_loss={val_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # early stopping on val_accuracy\n",
        "    stop = early_stop.step(val_acc, model, epoch)\n",
        "    if early_stop.best_state is not None and early_stop.best_epoch == epoch:\n",
        "        torch.save(early_stop.best_state, best_path)\n",
        "\n",
        "    if stop:\n",
        "        print(f\"Early stopping at epoch {epoch}. Restoring best epoch {early_stop.best_epoch}...\")\n",
        "        break\n",
        "\n",
        "# restore best weights\n",
        "early_stop.restore(model)\n",
        "torch.save(model.state_dict(), best_path)\n",
        "\n",
        "# ---- test evaluation ----\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_acc = 0.0\n",
        "n_test = 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        test_loss += loss.item()\n",
        "        test_acc  += accuracy_from_logits(logits, yb)\n",
        "        n_test    += 1\n",
        "\n",
        "test_loss /= max(1, n_test)\n",
        "test_acc  /= max(1, n_test)\n",
        "\n",
        "print(f\"\\nFinal test loss is: {test_loss:.4f}\")\n",
        "print(f\"Final test accuracy is: {test_acc:.4f}\")\n",
        "print(f\"Best model saved to: {best_path}\")\n",
        "files.download(best_path)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql_o1vFO5rFp"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "!wget 'https://raw.githubusercontent.com/MatchLab-Imperial/deep-learning-course/master/asset/05_RNN/word_gen_model.pth' -O word_gen_model.pth\n",
        "\n",
        "vocab_size = n_words + 1\n",
        "model = WordLSTM(vocab_size=vocab_size, embedding_size=300, lstm_units=256).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"word_gen_model.pth\", map_location=DEVICE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgvJiBOs5CEF"
      },
      "outputs": [],
      "source": [
        "# Vary the temperature here\n",
        "temperature = 0.7\n",
        "n_eval = 20\n",
        "seq_char_length = 100\n",
        "smoother = SmoothingFunction().method1\n",
        "bleu_score = 0.0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(n_eval):\n",
        "      # We look for references that do not contain any non-common words as we only\n",
        "      # learnt to predict the 2000 most common words\n",
        "      while True:\n",
        "          start = np.random.randint(0, len(x_test_word)-seq_length-1)\n",
        "          pattern = x_test_word[start].tolist()\n",
        "          reference = x_test_word[start+seq_length].tolist()\n",
        "          if n_words not in reference:\n",
        "              break\n",
        "      reference = ' '.join([n_to_word[value] for value in reference])\n",
        "\n",
        "      # generate words\n",
        "      output_sent = ''\n",
        "      for i in range(seq_length):\n",
        "          x = torch.tensor(pattern, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "          logits = model(x)\n",
        "          probs = F.softmax((logits + 1e-7) / (float(temperature) + 0.01), dim=-1)\n",
        "          idx = torch.multinomial(probs[0], num_samples=1).item()\n",
        "\n",
        "          word = n_to_word.get(idx, '')\n",
        "          output_sent += word + ' '\n",
        "\n",
        "          pattern.append(idx)\n",
        "          pattern = pattern[1:]\n",
        "\n",
        "      # Preprocess reference and candidate text\n",
        "      reference = word_tokenize(reference.lower())\n",
        "      candidate = word_tokenize(output_sent.lower())\n",
        "\n",
        "      # Remove empty strings (if any) after tokenization\n",
        "      reference = list(filter(lambda x: x != '', reference))\n",
        "      candidate = list(filter(lambda x: x != '', candidate))\n",
        "\n",
        "      # Remove incomplete words at the beginning and end of both lists\n",
        "      if len(reference) > 2:\n",
        "          reference = reference[1:-1]\n",
        "      if len(candidate) > 2:\n",
        "          candidate = candidate[1:-1]\n",
        "\n",
        "      # Compute BLEU score for the candidate and reference\n",
        "      bleu_score += sentence_bleu([reference], candidate, smoothing_function=smoother)\n",
        "\n",
        "bleu_score /= n_eval\n",
        "print(\"BLEU Score:\", bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf1cYUeC27W3"
      },
      "outputs": [],
      "source": [
        "# Vary the temperature here\n",
        "temperature = 0.7\n",
        "seed_text = (\n",
        "    \"TYRION pours himself some wine and drinks it down. He pours another glass, \"\n",
        "    \"and walks back to CERSEI placing his cup on her desk. He takes another glass.\\n\"\n",
        "    \"TYRION: \"\n",
        ")\n",
        "\n",
        "# Preprocess seed\n",
        "pattern = (\n",
        "    seed_text.replace('.', ' . ').replace(',', ' , ').replace(':', ' : ')\n",
        "    .replace('?', ' ? ').replace('!', ' ! ')\n",
        "    .replace('\\n', ' \\n ').replace('[', ' [ ').replace(']', ' ] ')\n",
        "    .replace(')', ' ) ').replace('(', ' ( ')\n",
        "    .lower()\n",
        "    .split()\n",
        ")\n",
        "pattern = [w for w in pattern if w.strip()][:seq_length]\n",
        "print(\"\\nInput Pattern:\\n\", \" \".join(pattern))\n",
        "\n",
        "# Map to indices (OOV â†’ n_words)\n",
        "pattern = [word_to_n.get(w, n_words) for w in pattern]\n",
        "\n",
        "print(\"\\nPredicted:\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for _ in range(100):\n",
        "        x = torch.tensor(pattern, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "        logits = model(x)\n",
        "\n",
        "        if temperature == 0:\n",
        "            idx = torch.argmax(logits, dim=-1).item()\n",
        "        else:\n",
        "            probs = F.softmax(logits / temperature, dim=-1)\n",
        "            idx = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "        word = n_to_word.get(idx, \"<UNK>\")\n",
        "        sys.stdout.write(word + \" \")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Update pattern\n",
        "        pattern = pattern[1:] + [idx]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}