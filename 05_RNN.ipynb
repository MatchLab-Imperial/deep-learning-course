{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/05_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiobin3g2wyW"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "A Recurrent Neural Network (RNN) is a specific type of architecture that is widely used to deal with sequential information. So far, the introduced CNN architectures from the previous tutorials treated inputs as independent objects, however, many applications need to deal with data that is interconnected. For instance, if you are translating a sentence from English to Taiwanese, and you are predicting the next word, it is useful to know which words came before the last one.\n",
        "\n",
        "RNNs are called recurrent since they apply the same operation to each of the input sequences, with the output of an individual element being dependent on the previous one. Theoretically, RNNs establish a connection between the actual input and ALL the previous ones. Although this is assumed, in the practice, RNNs have proven to only *remember* a limited number of inputs. In other words, RNNs have a *memory* that allows them to *remember* previous elements and use their information to deal with the current input.\n",
        "\n",
        "RNNs can be split into multiple types depending on their applications. For instance, if we want to predict one word given only the previous one, the topology of our network is a *One to One*. Another example is image captioning, where we can design a *One to Many* architecture to obtain a description from a single input image. The following diagram shows the different types of problems we can face ([Source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)):\n",
        "\n",
        "\n",
        "![alt text](http://karpathy.github.io/assets/rnn/diags.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t6BQY9B21nV"
      },
      "source": [
        "**A Closer Look to RNN**\n",
        "\n",
        "The figure below shows the simplest version of an RNN, which can be easily derived from a simple feedforward architecture by adding a single loop:\n",
        "\n",
        "![alt text](https://i.ibb.co/qnGH6RT/vanilla-rnn.png)\n",
        "\n",
        "During training, the hidden state $h$ is iteratively updated based on the input value $x$ and the learned weights $W_h$ and $W_x$. The final output $y$ is estimated from the current state $h_t$ and the matrix $W_y$.\n",
        "\n",
        "Although RNN can assure short-term dependencies within the network, simple RNNs become unable to learn to connect information as the gap between past and present information grows. To overcome this limitation, in practical applications LSTM unit is adopted, that is a special RNNs architecture composed of multiple interacting layers ([Source](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)):\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOUUKOPH2_SG"
      },
      "source": [
        "In this tutorial, we will use LSTMs straight away as a black box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMJ28M_227fj"
      },
      "source": [
        "# ***Many to One* RNNs - Regression**\n",
        "\n",
        "In this section, we are going to implement an easy example of an RNN in Keras. Given the records of a local newspaper, which indicates the number of new subscriptions per month, we want to predict the number of members that will join next month.\n",
        "\n",
        "First, we load the data with the utility method in Pandas `read_csv`, which allows us to load directly the dataset from an URL address. Let's print some rows of the imported record to see the format of the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "\n",
        "from collections import Counter\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import requests\n",
        "import sys\n",
        "import string\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from IPython.display import SVG\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import tarfile\n",
        "from tensorflow.keras import callbacks, datasets, layers, models, preprocessing, utils\n",
        "from tqdm import tqdm\n",
        "import transformers\n",
        "import wget\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "qjNEJLHjafwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PZRejNk29jN"
      },
      "source": [
        "data = pd.read_csv(\"https://drive.google.com/uc?id=1Hv2nuwVXO_aZN89llGva0hSH6k3kKAVm\",usecols=[1],engine = \"python\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd3kk4Zh3CA-"
      },
      "source": [
        "Moreover, let's plot the number of subscriptions per month."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEmJqK1z3Dei"
      },
      "source": [
        "plt.figure(figsize = (15, 5))\n",
        "plt.plot(data, label = \"City Newspaper Subscriptions\")\n",
        "plt.xlabel(\"Months\")\n",
        "plt.ylabel(\"1000 Subscriptions\")\n",
        "plt.title(\"Monthly Total Subscriptions to City Newspaper 1949 - 1960\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x55PWxD3IFE"
      },
      "source": [
        "First of all, we need to prepare the data for the training and test stage. Once the data is loaded, we normalize the input values and split them between training (70%) and testing (30%).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIrOh9Pi3JZg"
      },
      "source": [
        "# convert pandas data frame in numpy array of float.\n",
        "data_np = data.values.astype(\"float32\")\n",
        "\n",
        "# normalize data with min max normalization\n",
        "normalizer = sklearn.preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
        "dataset = normalizer.fit_transform(data_np)\n",
        "\n",
        "# Using 70% of data for training, 30% for test.\n",
        "TRAINING_PERC = 0.70\n",
        "\n",
        "train_size = int(len(dataset) * TRAINING_PERC)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
        "print(\"Number of samples training set: \" + str((len(train))))\n",
        "print(\"Number of samples test set: \" + str((len(test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI5UamBO3KrU"
      },
      "source": [
        "We also arrange the dataset (input and labels) in the appropriate Keras' format by using the helper function `create_dataset()`.\n",
        "\n",
        "`create_dataset()` takes as argument the variable `window_size`. This variable is highly important when dealing with sequences since it is going to determine the length of our input data to the network. For instance, by setting `window_size` to 5, we will be using the last 5 monthly subscriptions values to predict the next one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvGh-1db3MOy"
      },
      "source": [
        "# helper function to read data.\n",
        "def create_dataset(dataset, window_size = 1):\n",
        "    data_x, data_y = [], []\n",
        "    for i in range(len(dataset) - window_size - 1):\n",
        "        sample = dataset[i:(i + window_size), 0]\n",
        "        data_x.append(sample)\n",
        "        data_y.append(dataset[i + window_size, 0])\n",
        "    return(np.array(data_x), np.array(data_y))\n",
        "\n",
        "# Create test and training sets for regression with window size 5.\n",
        "window_size = 5\n",
        "train_X, train_Y = create_dataset(train, window_size)\n",
        "test_X, test_Y = create_dataset(test, window_size)\n",
        "train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))\n",
        "test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 1))\n",
        "\n",
        "print(\"Shape of training inputs: \" + str((train_X.shape)))\n",
        "print(\"Shape of training labels: \" + str((train_Y.shape)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlB3Edd03NwG"
      },
      "source": [
        "Once all the data is ready, we create the model as a `Sequential` object including 16 LSTM units and a dense layer outputting a single scalar.\n",
        "As mentioned, we specify a window size equal to 5, so that the prediction of the current element depends only on the previous five ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuT4lcHt3Qqp"
      },
      "source": [
        "batch_size = 32\n",
        "rnn = models.Sequential([\n",
        "    layers.Input(shape=(window_size, 1)),\n",
        "    layers.LSTM(16),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "rnn.compile(loss = \"mean_squared_error\",  optimizer = \"adam\", metrics = ['mse'])\n",
        "rnn.fit(train_X, train_Y, epochs=500, batch_size=batch_size, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrMqbfrK3Nz8"
      },
      "source": [
        "Now we can compute the MSE on the training and test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkhF1mrr3Tor"
      },
      "source": [
        "def get_predict_and_score(model, X, Y):\n",
        "    # transform the prediction to the original scale.\n",
        "    pred = normalizer.inverse_transform(model.predict(X))\n",
        "    # transform also the label to the original scale for interpretability.\n",
        "    orig_data = normalizer.inverse_transform([Y])\n",
        "    # calculate RMSE.\n",
        "    score = math.sqrt(sklearn.metrics.mean_squared_error(orig_data[0], pred[:, 0]))\n",
        "    return score, pred\n",
        "\n",
        "mse_train, train_predict = get_predict_and_score(rnn, train_X, train_Y)\n",
        "mse_test, test_predict = get_predict_and_score(rnn, test_X, test_Y)\n",
        "\n",
        "print(\"Training data error: %.2f MSE\" % mse_train)\n",
        "print(\"Test data error: %.2f MSE\" % mse_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVOaCHMc3W6I"
      },
      "source": [
        "Moreover, we can plot the predictions and actual values in a graph to check visually the performance of our predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUjXRdOg3Vjj"
      },
      "source": [
        "# Training predictions.\n",
        "train_predictions = np.empty_like(dataset)\n",
        "train_predictions[:, :] = np.nan\n",
        "train_predictions[window_size:len(train_predict) + window_size, :] = train_predict\n",
        "\n",
        "# Test predictions.\n",
        "test_predictions = np.empty_like(dataset)\n",
        "test_predictions[:, :] = np.nan\n",
        "test_predictions[len(train_predict) + (window_size * 2) + 1:len(dataset) - 1, :] = test_predict\n",
        "\n",
        "# Create the plot.\n",
        "plt.figure(figsize = (15, 5))\n",
        "plt.plot(normalizer.inverse_transform(dataset), label = \"True value\")\n",
        "plt.plot(train_predictions, label = \"Training predictions\")\n",
        "plt.plot(test_predictions, label = \"Test predictions\")\n",
        "plt.xlabel(\"Months\")\n",
        "plt.ylabel(\"1000 member subscriptions\")\n",
        "plt.title(\"Comparison true vs. predicted in the training and testing set\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMzQDr20HPbw"
      },
      "source": [
        "#Sequence Modelling: Text\n",
        "\n",
        "Now we will use RNNs to tackle text examples. Text is one of the modalities that have been widely tackled with deep learning improving upon past methods. We will present two problems: classification and generation.\n",
        "\n",
        "A small summary of both problems is given below.\n",
        "\n",
        "**Classification**\n",
        "\n",
        "Classification is a standard problem, where we have some input data $x$ and try to classify it as one of the available classes $y$. In the case of sequential data, though, $x$ will be a sequence of elements that will be processed by the RNN to return the label $y$. The image depicts a sequence classification problem, where the red blocks are inputs, the green blocks the RNN model, and the blue block is the output. An example of a text classification problem that can be tackled with RNNs is \"Hate speech detection\", where the architecture must identify if an input text contains racist or sexist language, among others.\n",
        "\n",
        "![alt text](https://i.ibb.co/TtZPpZr/Capture.jpg)\n",
        "\n",
        "\n",
        "**Generation**\n",
        "\n",
        "In a generation problem, we aim to generate a sequence $y$ following the same distribution as the real data $x$. We will input the sequence into the model, and we will output another sequence $y$. Text translation is a typical example of many to many RNNs.\n",
        "\n",
        "![](https://i.ibb.co/7gSwnT2/Capture.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4kJNRmcq3DL"
      },
      "source": [
        "# Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLVCgocM6Kk"
      },
      "source": [
        "**References:**\n",
        "\n",
        "https://colab.research.google.com/github/csc-training/intro-to-dl/blob/master/day1/keras-imdb-rnn.ipynb#scrollTo=BYu8ql1nG7xc\n",
        "\n",
        "For the text classification example we will use the IMDB dataset available in Keras. The dataset includes tens of thousands of movie reviews taken from the IMDB website with a corresponding label for each review. The label is binary, and indicates if the review has a positive (label=1) or negative (label=0) sentiment. This problem is a quite standard Natural Language Processing (NLP) problem, and it is called sentiment classification or sentiment analysis.\n",
        "\n",
        "Before loading the data and building our model, we will explain a common part of NLP models, the embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAzSKba3J5NA"
      },
      "source": [
        "### Embeddings\n",
        "**References:**\n",
        "\n",
        "[1] https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "[2] https://colab.research.google.com/drive/1oXjNYSJ3VsRvAsXN4ClmtsVEgPW_CX_c?hl=en#scrollTo=p9q7qfXrvq-J\n",
        "\n",
        "\n",
        "Usually, the first step of text modelling is transforming the words into a numerical vector that represents the meaning or some properties of the word. This vector can be then processed by the network. To do so, we first will encode the word sequences in integer numbers, and we will also have a dictionary that contains the relationship `(actual word, integer)`. For example, the sentence \"the cat is on the table and the dog is on the mat\" can be encoded in the form $(7, 1, 3, 5, 7, 6, 0, 7, 2, 3, 5, 7, 4)$, with the corresponding dictionary $(and, 0), (cat, 1), (dog, 2)\\dots (the, 7)$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhRu4KoqSVvH"
      },
      "source": [
        "sentence = 'the cat is on the table and the dog is on the mat'\n",
        "## We form a list of unique words by using a set\n",
        "## which returns only unique elements\n",
        "## we also sort them, which is not necessary\n",
        "sentence_set = sorted(set(sentence.split(' ')))\n",
        "words = list(sentence_set)\n",
        "## We now form a dictionary in the form of\n",
        "## e.g. dict_words[and] = 1\n",
        "dict_words = dict((word, i) for i, word in enumerate(words))\n",
        "## We now encode the sentence in a list of integers\n",
        "encoded_sentence = [dict_words[w] for w in sentence.split()]\n",
        "print(sentence_set)\n",
        "print(encoded_sentence)\n",
        "print(dict_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoHyEZdVzHFD"
      },
      "source": [
        "Keras contains a class called `Tokenizer` that helps in performing this process. We will not use it for the examples in this tutorial, as we want to show all the steps involved in the tokenization process, but the `Tokenizer` class provides a high-level way to handle this preprocessing step. As it can be quite useful, we provide a small overview of it. [Here](https://keras.io/preprocessing/text/) you have a link to the Keras documentation explaining the different arguments of the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5EL3-o5wsQl"
      },
      "source": [
        "## First you define a tokenizer object\n",
        "## If you want a character level tokenization we set char_level=True\n",
        "## If set to false, it will tokenize in a word level\n",
        "## You can check in the documentation the different arguments\n",
        "tokenizer = preprocessing.text.Tokenizer(char_level=False)\n",
        "## Then you fit it on the data you have\n",
        "## The method expects a list of sentences\n",
        "tokenizer.fit_on_texts([sentence])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RBQrbI10CuR"
      },
      "source": [
        "## Now, the class has formed a dictionary with all the words\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKshU7Lj0Sgl"
      },
      "source": [
        "## You can also check the word count\n",
        "print(tokenizer.word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNI-js25161w"
      },
      "source": [
        "## You can transform the data to sequences of integers\n",
        "## We print the first 10 elements\n",
        "tokenizer.texts_to_sequences([sentence])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lITR0l3SUAU"
      },
      "source": [
        "Now that we have the sequence in the form of a list of integers,  we could input that directly to the RNN. However, as those numbers are arbitraryly chosen, it would be really hard for the RNN to understand the relationships between the words. To give the model more representation power, we first want to transform the integer to a vector of dimension $d$ which represents the semantic meaning of the words. Each of these vectors is called an Embedding. The actual values of the vectors are not important, only the relationships between them. For example, `dog` and `cat` vectors are probably going to be closer than `dog` and `the`. Or if you are doing sentiment classification, probably two words encoding similar positive sentiments will be closer than a word expressing positive sentiments and a completely neutral word.\n",
        "\n",
        " The embeddings can be initialized randomly and the model will learn suitable values to reduce the loss during the training process. However, embeddings which have already been trained in a large corpus of text such as Wikipedia can also be used. Examples of these pretrained embeddings are [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) or  [GloVe](https://nlp.stanford.edu/projects/glove/). These methods are trained using the context of the words, for example predicting the surrounding words when a specific word is given. As a result, words that appear in similar contexts have closer embeddings.\n",
        "\n",
        "![alt text](https://i.ibb.co/s5sg6dZ/Screenshot-from-2019-02-08-15-13-41.png)\n",
        "\n",
        "The image shows relationships between words in the embedding space.\n",
        "\n",
        "\n",
        "\n",
        "We now download the GloVe pretrained vectors trained in Wikipedia and Gigaword. [Here](https://nlp.stanford.edu/projects/glove/) you have an overview of what GloVe is. We will see how the embeddings contain a semantic meaning that allows us to model semantic relationships."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie3ToDUgKWN4"
      },
      "source": [
        "## Download and unzip glove pretrained embeddings\n",
        "!wget https://imperialcollegelondon.box.com/shared/static/c9trfhhwl9ohje5g3sapu3xk2zoywp3c.txt -O gensim_glove_vectors.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDs0EebuUJ5x"
      },
      "source": [
        "We will now load the GloVe embeddings. We use `gensim` to manipulate the embeddings, which is a nice tool that can be used to play with GloVe or Word2Vec embeddings. This piece of code takes some time to load, as the embeddings file is quite large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK0qwB--K0Bp"
      },
      "source": [
        "## We load the embeddings\n",
        "## Gensim is a really useful module that provides high-level function\n",
        "## to use with the embeddings\n",
        "!pip install gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrj-dPMbSsrc"
      },
      "source": [
        "Now we will do some operations using the embeddings of the words. First, we will do some words arithmetics based on the embeddings, which it will show us that the words encode some semantic meaning. For example, the distance between words with similar semantic meaning but different genders is approximately fixed. Meaning that the vector resulting from doing $man - woman$ should be similar to $king - queen$, hence:\n",
        "\n",
        "$king - queen \\approx man - woman \\rightarrow woman + king - man \\approx queen$\n",
        "\n",
        "We can check this using the method `most_similar` and the `positive` and `negative` arguments as following:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PYEzmorMFD_"
      },
      "source": [
        "## We can do word arithmetics.\n",
        "## We look for the nearest neighbour of the vector resulting on doing\n",
        "## the operation 'king' + 'woman' - 'man'\n",
        "glove_model.most_similar(positive=['woman', 'king'], negative=['man'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1bgFWT3aGUi"
      },
      "source": [
        "## Similar examples\n",
        "glove_model.most_similar(positive=['google', 'ios'], negative=['apple'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J09zgTciaHU4"
      },
      "source": [
        "glove_model.most_similar(positive=['england', 'paris'], negative=['france'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7LMqhpATsAL"
      },
      "source": [
        "## We can also check the word that does not match the rest\n",
        "glove_model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g85rTdpOY-HZ"
      },
      "source": [
        "To check out other functions you can call use [this](https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjgjAlaBQvaH"
      },
      "source": [
        "In http://projector.tensorflow.org/ you can visualize the words projected in the $\\mathbb{R}^3$ space using either PCA or tSNE, which are both techniques for dimensionality reduction. There, you can see how words are clustered by meaning or topic.\n",
        "\n",
        "\n",
        "![](https://brenndoerfer.github.io/deep-sentiment-analysis-distill/img/tensorboard_projector.gif)\n",
        "\n",
        "Image taken from [here](https://brenndoerfer.github.io/deep-sentiment-analysis-distill/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30w3Y4oHJy79"
      },
      "source": [
        "### IMDB\n",
        "Now let's start tackling a text classification problem. We first load the IMDB dataset for sentiment classification. As we said, it contains movie reviews with a corresponding binary sentiment label (label=1 corresponds to positive sentiment, and label=0 to negative sentiment). The words are already encoded as integers in order from most common words to less common (e.g. `the` is a common word so it should be encoded as a small integer). That makes it easy to filter the non-common words by using the argument `num_words` when loading the data. The filtered words will be all encoded as a special token `<UNK>`, which means unknown. For example, if we want to load the dataset with only the top $5000$ most common words we can do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWU0zG7AHd3u"
      },
      "source": [
        "# number of most-frequent words to use\n",
        "nb_words = 5000\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = datasets.imdb.load_data(num_words=nb_words)\n",
        "print('x_train:', x_train.shape)\n",
        "print('x_test:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiYpN-jwcGd-"
      },
      "source": [
        "We see that the dataset contains 25000 examples for both train and testing. Let's print an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF2twk31cNiN"
      },
      "source": [
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqxzLNgAcUmL"
      },
      "source": [
        "The printed example is a sequence of numbers. It is important to note that there are three special integers in this IMDB dataset: $0$, $1$ and $2$. $0$ will be used to pad the sequences, which we will explain now. $1$ is used as the `<START>` token (you can see how the printed sequence starts with a `1`). $2$ is the token `<UNK>` which is used for all the filtered non-common words. If we want to retrieve the actual sentence, we can use the dictionary given when using the method `get_word_index()`. However, the given dictionary when calling `get_word_index()` does not take into account the three mentioned tokens, so we need to modify it a little bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y26JIPAadBtX"
      },
      "source": [
        "# get_word_index retrieves a mapping word -> index\n",
        "word_index = datasets.imdb.get_word_index()\n",
        "# We make space for the three special tokens\n",
        "word_index_c = dict((w, i+3) for (w, i) in word_index.items())\n",
        "word_index_c['<PAD>'] = 0\n",
        "word_index_c['<START>'] = 1\n",
        "word_index_c['<UNK>'] = 2\n",
        "# Instead of having dictionary word -> index we form\n",
        "# the dictionary index -> word\n",
        "index_word = dict((i, w) for (w, i) in word_index_c.items())\n",
        "# We now retrieve the sentence\n",
        "sentence = ''\n",
        "for index in x_train[0]:\n",
        "  sentence += index_word[index] + ' '\n",
        "print(sentence)\n",
        "print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyhd3lQefNH4"
      },
      "source": [
        "We can see how we just recovered the sentence that we had printed before as a sequence of numbers. We can also see how there is no punctuation in this dataset. The review is clearly positive, so the corresponding label is \"1\".\n",
        "\n",
        "One of the problems with the given text data is that the sequences all have a different length. We want to give Keras a batch of inputs with fixed dimensions. To do so, we define a maximum length `maxlen`, and truncate the sentences longer than that, and also pad with $0$'s at the beginning the sentences shorter than that length. We use the method `sequence.pad_sequence` from `keras.preprocessing`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGcgjhlVbyOv"
      },
      "source": [
        "# Truncate sentences after this number of words\n",
        "maxlen = 80\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXCucplf0M5O"
      },
      "source": [
        "We can check how some sentences are padded at the beginning to be of length `maxlen`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeBf1oOJzr4d"
      },
      "source": [
        "x_train[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2ljchrWPMz3"
      },
      "source": [
        "We will now build the model we will use for sentiment classification. The model is formed by an Embedding layer, where the model will learn a vector of dimensionality `embedding_dim` for each of the words; an LSTM layer and a linear layer that maps the output of the LSTM to 1 value. We train this with the `binary_crossentropy` loss and `sigmoid` activation as we only have two classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh2cHXD5HjQG"
      },
      "source": [
        "## Model parameters:\n",
        "# Dimensions of the embeddings\n",
        "embedding_dim = 300\n",
        "## LSTM dimensionality\n",
        "lstm_units = 128\n",
        "\n",
        "print('Build model...')\n",
        "text_class_model = models.Sequential([\n",
        "    layers.Input(shape=(maxlen,)),\n",
        "    layers.Embedding(nb_words, embedding_dim),\n",
        "    layers.LSTM(lstm_units),\n",
        "    layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "# To stack multiple RNN layers, all RNN layers except the last one need\n",
        "# to have \"return_sequences=True\".  An example of using two RNN layers:\n",
        "# model.add(LSTM(lstm_units, return_sequences=True))\n",
        "# model.add(LSTM(lstm_units))\n",
        "# You can add some dropout if you want\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "text_class_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "text_class_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTDlUcEF0mqn"
      },
      "source": [
        "Let's train it for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJfDwWRlHoEm"
      },
      "source": [
        "## We train the model for 5 epochs and check the accuracy in a validation split\n",
        "epochs = 5\n",
        "validation_split = 0.2\n",
        "\n",
        "history = text_class_model.fit(x_train, y_train, batch_size=128,\n",
        "          epochs=epochs, validation_split=validation_split)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tVyZLyv0rbO"
      },
      "source": [
        "This is a classification example, so let's print the loss and the classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3D78e9yHo8l"
      },
      "source": [
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(history.epoch,history.history['loss'], label='training')\n",
        "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
        "plt.title('loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(history.epoch,history.history['accuracy'], label='training')\n",
        "plt.plot(history.epoch,history.history['val_accuracy'], label='validation')\n",
        "plt.title('accuracy')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjoId37R0yVI"
      },
      "source": [
        "It looks like we could have stopped the training earlier as the validation loss did not decrease afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOHn1utQHqHu"
      },
      "source": [
        "# Let's see the accuracy in the test split\n",
        "scores = text_class_model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"%s: %.2f%%\" % (text_class_model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3HtK17c1S_F"
      },
      "source": [
        "### Saving best performing model\n",
        "\n",
        "Now we save the best performing models using the validation loss as a metric. As you already know, lower training error does not mean in some cases better performance in the validation or test split.\n",
        "\n",
        "In the last example we ran the model for 5 epochs, however after the first epoch the validation loss increased. We want to use the model performing the best in the validation set. To do so, we can use a ModelCheckpoint callback as was explained in past tutorials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzVNbh5a1xsq"
      },
      "source": [
        "###### We reset the model #######\n",
        "text_class_model = models.Sequential([\n",
        "    layers.Input(shape=(maxlen,)),\n",
        "    layers.Embedding(nb_words, embedding_dim),\n",
        "    layers.LSTM(lstm_units),\n",
        "    layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "text_class_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "#################################\n",
        "\n",
        "\n",
        "## Here we define the checkpoint callback. We first give the name it will use\n",
        "## to save the model. We also specify what metric should monitor, in this case\n",
        "## the validation loss (it could be validation accuracy too for example)\n",
        "## save_best_only means that the models will only be save when the monitored\n",
        "## metric improves with respect to the past best one\n",
        "checkpoint = callbacks.ModelCheckpoint('model-{epoch:03d}.keras', verbose=1,\n",
        "                             monitor='val_loss', save_best_only=True)\n",
        "## Now we pass the callback to the fit method\n",
        "history = text_class_model.fit(x_train, y_train, batch_size=128,\n",
        "                               epochs=epochs, callbacks=[checkpoint],\n",
        "                               validation_split=validation_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrtRFGUF22tf"
      },
      "source": [
        "We now saved the model after every epoch if the validation loss decreased. In this case, as the best epoch is the first one, it only saves the model after the first epoch. Let's load the model now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwHZ54_22vSa"
      },
      "source": [
        "text_class_model = models.load_model('model-001.keras')  # load model weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KLudp1u3CHY"
      },
      "source": [
        "And now we check if the accuracy is better compared to when we used the model trained for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsuLgHQu2zqu"
      },
      "source": [
        "# Let's see the accuracy in the test split\n",
        "scores = text_class_model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"%s: %.2f%%\" % (text_class_model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ufb6B68hUhl"
      },
      "source": [
        "### Importance of embeddings\n",
        "Now, let's check quickly if using Embeddings provide any benefit. For this experiment, we remove the Embedding layer, meaning we will be inputting the word index, i.e. an integer, to the LSTM. Additionally, we need to vary the shape of the input data as the LSTM needs a third dimension with the number of channels per input (in this case 1, as each word is represented by an integer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1NY5IhRhppg"
      },
      "source": [
        "# model parameters\n",
        "# Same model as before but without embeddings\n",
        "lstm_units = 128\n",
        "x_train_r = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test_r = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "text_class_model = models.Sequential([\n",
        "    layers.Input(shape=(maxlen, 1)),\n",
        "    layers.LSTM(lstm_units),\n",
        "    layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "text_class_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "text_class_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZaz4eYyiBPc"
      },
      "source": [
        "epochs = 5\n",
        "validation_split = 0.2\n",
        "\n",
        "history = text_class_model.fit(x_train_r, y_train, batch_size=128,\n",
        "          epochs=epochs,\n",
        "          validation_split=validation_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZcRp4nXiN84"
      },
      "source": [
        "scores = text_class_model.evaluate(x_test_r, y_test, verbose=2)\n",
        "print(\"%s: %.2f%%\" % (text_class_model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyOqn7yginQg"
      },
      "source": [
        "We see how the accuracy is just slightly better compared to doing a random guess as there are only 2 classes. That is because the integer that encodes the word is chosen arbitrarily and does not encode any property of the words nor any relationship between words. Hence, using learned embeddings results in models with better capacity/accuracy for this kind of tasks.\n",
        "\n",
        "Instead of inputting directly integers to the RNN, we could argue that another way of representing the input words is to input the one-hot representation of the word directly to the RNN instead of using embeddings. This method has two problems. First, the resulting dimensionality would be too large. In this case we are loading the $20000$ most common words in IMDB, so each word would be encoded in a vector of dimensionality $20000$. Secondly, the encoding does not give any notion of similarity between the different words. For these two reasons, embedding the words to a common lower dimensionality space is also better than using one-hot encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EwrgAmcM9na"
      },
      "source": [
        "# Text Generation\n",
        "\n",
        "We now will present an example of text generation using an RNN. We will use the Tiny Shakespeare dataset, which contains samples from Shakespeare works.\n",
        "\n",
        "We pose the problem as a classification problem as in the Sentiment classification example. In a text generation setting, given a sequence, we aim to predict the next one. During the training step, the process would be quite similar to the classification task, i.e. given a sequence you predict a label/word/character. However, in the prediction setting, we aim to output a whole sentence, not only a word or character. For that reason we will follow the procedure explained in the image.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*XvUt5wDQA8D3C0wAuxAvbA.png)\n",
        "The image is taken from [here](https://medium.com/@david.campion/text-generation-using-bidirectional-lstm-and-doc2vec-models-1-3-8979eb65cb3a)\n",
        "\n",
        "As in the text classification section, we encode the raw sentences in the form of a sequence of integers. In this case, however, we have a text with a really wide vocabulary, and we aim to predict the following element of the sequence. In this dataset the number of words is really large, so if we used a word-level model the number of available classes would be quite large if we did not filter the number of possible words. For this example, instead of encoding on a word level, we will do it on a character level, which results in a more limited vocabulary. We aim to predict the next character when inputting a sentence of length $d$. In evaluation mode, i.e. when we are predicting the next character, we then input a seed of $d$ characters as the first input, and afterwards we use the last $d$ characters as input to predict the next character.\n",
        "\n",
        "The advantages of predicting characters instead of words is that character-level models have a limited vocabulary/classes compared to the number of possible words in a text. They are also more flexible (for example, they can generate \"fake\" url links if trained in e.g. wikipedia data). However, word-level models usually present higher performance as it easier for them to keep track of the long-term meaning of the sentence, and also can avoid any spelling mistakes that may happen in character-level models.\n",
        "\n",
        "**References:** Some code snippets taken from [here](https://www.analyticsvidhya.com/blog/2018/03/text-generation-using-python-nlp/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_-da7NsDrJf"
      },
      "source": [
        "We first download the data we use for the example, and then read the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcR0v2GGNFCd"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt\n",
        "## We read all the raw data in the variable data\n",
        "data = open('./tiny-shakespeare.txt', 'r').read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxFUMjE9kP4C"
      },
      "source": [
        "Let's print a subset of the data for visualization purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c8ZSkLKkX6Y"
      },
      "source": [
        "print(data[:364])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhZmAeuzD0Wt"
      },
      "source": [
        "We see in the raw text that the name of the characters is printed, that there is a blank line between the different character lines, and also that lines are usually not longer than ~80 characters. We hope the network is capable of learning all of this.\n",
        "\n",
        "Now we will structure the data so we can input it into our RNN model. We aim to encode the text using a sequence of integers as we explained in the classification section.  Hence, we form a dictionary with all the different characters appearing in the text (including whitespaces and punctuation) and its corresponding integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs3tRv_1whDc"
      },
      "source": [
        "characters = sorted(list(set(data)))\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEfzwgCnwjwo"
      },
      "source": [
        "Now we want to split the text in examples of length `seq_length` where the label is the next character. As we mentioned, we pose the problem as a set of successive classification problems, where we try to predict the next word given an input sentence. To split the text in pairs of (sequence, next word), we use the dictionary `char_to_n` to encode the different elements as integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gulHCFMJ_tEE"
      },
      "source": [
        "x = []\n",
        "y = []\n",
        "length = len(data)\n",
        "seq_length = 100\n",
        "for i in range(0, length-seq_length, 1):\n",
        "  sequence = data[i:i + seq_length]\n",
        "  label = data[i + seq_length]\n",
        "  x.append([char_to_n[char] for char in sequence])\n",
        "  y.append(char_to_n[label])\n",
        "n_samples = len(x)\n",
        "print(\"Total Samples: {:d}\".format(n_samples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehank2xQFGOo"
      },
      "source": [
        "We now form a test split by using 5% of the available data. To do so, we just take the last 5% of the data. Usually, you need to randomize the data before splitting in test/train to avoid having a different distribution of the data in both splits. However, as this is a sequential data where we want to predict the next character, if we shuffle the splits we would have this in training:\n",
        "\n",
        "``\n",
        "x = 'the boy is tal_' y = 'l'\n",
        "``\n",
        "\n",
        "and this in testing:\n",
        "\n",
        "``\n",
        "x = 'the boy is ta_' y = 'l'\n",
        "``\n",
        "\n",
        "which would contaminate the splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbBqXbtpfa5T"
      },
      "source": [
        "x_train = x[:int(n_samples*0.95)]\n",
        "x_test = x[int(n_samples*0.95):]\n",
        "y_train = y[:int(n_samples*0.95)]\n",
        "y_test = y[int(n_samples*0.95):]\n",
        "\n",
        "## Transform the list to a numpy array\n",
        "x_train = np.reshape(x_train, (len(x_train), seq_length))\n",
        "## Onehot encoding of labels\n",
        "y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "x_test = np.reshape(x_test, (len(x_test), seq_length))\n",
        "y_test = utils.to_categorical(np.asarray(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U63V-XoLkzD2"
      },
      "source": [
        "Let's print a sequence and the shape of the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_KvSAVQzOvq"
      },
      "source": [
        "print(x_train[8000])\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uudj2qT_FNJG"
      },
      "source": [
        "Here, we define the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13TcalgkqHsM"
      },
      "source": [
        "# define the LSTM model\n",
        "embedding_size = 100\n",
        "lstm_units = 128\n",
        "\n",
        "text_gen_model = models.Sequential([\n",
        "    layers.Input(shape=(seq_length,)),\n",
        "    layers.Embedding(y_train.shape[1], embedding_size),\n",
        "    layers.LSTM(lstm_units),\n",
        "    layers.Dense(y_train.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "text_gen_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "text_gen_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC2BaDcumZnm"
      },
      "source": [
        "The model is a little bit complex, so training takes some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2EPi_Fee4dj"
      },
      "source": [
        "text_gen_model.fit(x_train, y_train, epochs=10 , batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5XsfQRNr9An"
      },
      "source": [
        "### Prediction\n",
        "\n",
        "We now take any of the sequences from the test split and use it as an initial pattern for our model. Then we enter a loop where given an input sequence, we predict the next character and then form a new input sequence by appending the predicted character and dropping the first character.\n",
        "\n",
        "Whenever we input a sequence, we obtain as output a probability distribution of the possible characters. For example, given the sequence `the cat and the do` the model will output a probability distribution of the next character where probably the character `g` will have a high probability (forming then `the cat and the dog`). However, the character `c` can also be a possibility, as the sentence formed may be `the cat and the doctor`. When deciding what character to predict, a strategy is to just take the character with the maximum probability at all times, but then the variability of the formed text is then smaller. Another option would be to sample following the same probability of distribution as the model outputs. However, this might result in a excessive variability with some sentences not making any sense.\n",
        "\n",
        "This trade-off of variability against more probable sequences is controlled by what it is called the temperature of the sampling process. The temperature controls the smoothing of the probability vector. A temperature close to 0 will result in taking always the safest (i.e. that with the highest probability) as the next element. A temperature close to 1 will decide on the next element following the same distribution of probability as the original output of the model.\n",
        "\n",
        "In a more formal way, being $p_i$ the probability of the element $i$ output by the RNN model and $T$ the temperature, the probability after applying the mentioned smoothing $\\hat{p_i}$ is:\n",
        "\n",
        "$$\n",
        "\\hat{p_i} = \\frac{e^{\\log(p_i)/T}}{\\sum_j e^{\\log(p_j)/T}}\n",
        "$$\n",
        "\n",
        "You can check how a small $T$ is going to make the element with the largest probability be close to 1 after the process.\n",
        "\n",
        "Before starting the text prediction, let's show an example of how the sampling temperature affects the model choices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loPFAdDvIsWO"
      },
      "source": [
        "def temperature_smoothing(prediction, temperature=1.0):\n",
        "  # This function computes the temperature smoothing function\n",
        "  # we explained\n",
        "  prediction = np.log(prediction + 1e-7) / (temperature + 0.01)\n",
        "  exp_preds = np.exp(prediction)\n",
        "  prediction = exp_preds / np.sum(exp_preds)\n",
        "  return prediction\n",
        "\n",
        "# We have this vector of probabilities\n",
        "prediction = np.asarray([0.2, 0.3, 0.1, 0.4])\n",
        "## Temperature = 1.0\n",
        "print('Output probabilities with temp = 1')\n",
        "print(temperature_smoothing(prediction, temperature=1))\n",
        "## Temperature = 0.5\n",
        "print('Output probabilities with temp = 0.5')\n",
        "print(temperature_smoothing(prediction, temperature=0.5))\n",
        "## Temperature = 0.25\n",
        "print('Output probabilities with temp = 0.25')\n",
        "print(temperature_smoothing(prediction, temperature=0.25))\n",
        "## Temperature = 0.0\n",
        "print('Output probabilities with temp = 0')\n",
        "print(temperature_smoothing(prediction, temperature=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST0l8UWDJy-t"
      },
      "source": [
        "You can see how a lower temperature makes the small probabilities become smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnpdKtGwIjue"
      },
      "source": [
        "Here you can set the temperature and check how the output varies. You will see that for $T\\approx0$ the generated text has low variability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REoPwrqieFLK"
      },
      "source": [
        "# Vary the temperature here\n",
        "temperature = 0.5\n",
        "\n",
        "# We select a random element from the test set as seed\n",
        "start = np.random.randint(0, len(x_test)-1)\n",
        "pattern = x_test[start].tolist()\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([n_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "print(\"\\nPredicted:\")\n",
        "# generate 300 characters\n",
        "for i in range(300):\n",
        "  x = np.reshape(pattern, (1, len(pattern)))\n",
        "  prediction = text_gen_model.predict(x, verbose=0).astype(np.float64)\n",
        "  ## We put the constant 0.02 to avoid dividing by zero\n",
        "  ## We sum by 1e-7 to avoid log(0)\n",
        "  prediction = np.log(prediction + 1e-7) / (temperature + 0.01)\n",
        "  exp_preds = np.exp(prediction)\n",
        "  prediction = exp_preds / np.sum(exp_preds)\n",
        "  ## We applied the smoothing with the temperature\n",
        "  ## Now we predict following the probabilities in the variable prediction\n",
        "  prediction = np.random.multinomial(1, prediction[0,:], 1)\n",
        "  index = np.argmax(prediction)\n",
        "  result = n_to_char[index]\n",
        "  seq_in = [n_to_char[value] for value in pattern]\n",
        "  ## Print the result\n",
        "  sys.stdout.write(result)\n",
        "\n",
        "  ## Create the input sequence for the next character by appending the predicted\n",
        "  ## character and dropping the first one to always have constant seq. length\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSQs-l7OmNoI"
      },
      "source": [
        "Notice how the network has learnt the structure of the text, to start a new line every few words and to put the name of the characters too. The text itself seems grammatically correct in most cases but it fails to make much sense in most examples. Generating sentences with actual semantic meaning is harder for these type of models compared to generating grammatically correct sentences. Test different temperature settings to see how it affects the generation\n",
        "\n",
        "If you want to know more about text generation, along with some extra generated examples, we refer you to [Andrej Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hY4m63VgiM9"
      },
      "source": [
        "### Quantitative evaluation of the generated text\n",
        "\n",
        "For a quantitative evaluation of the generated text we will use a metric commonly used in image captioning and translation tasks, BLEU. BLEU looks for matches on a word level between the generated text and the reference text. Specifically, BLEU looks for matches in n-grams up to $n=4$, where an n-gram is defined as a contiguous sentence of $n$ items. For example, in the sentence `the sky is blue`, an n-gram of $n=2$ would be `is blue`. BLEU scores range from [0,1].\n",
        "\n",
        "As we mentioned, BLEU is usual in more constrained text generation tasks, such as image captioning. In our text generation task there is a large number of both grammatically and semantically correct possibilities when generating new sentences, and one of the issues for this task is to develop a metric that can properly evaluate the semantic and syntactic quality of the generated text. Human evaluation is quite common when evaluating the quality of generated sentences in these less constrained tasks due to the difficulty of finding proper automatic metrics. However, we hope to see a correlation between the BLEU score and the quality of the generated text. Here we take an input sentence from the test data and generate a sentence. Then, we compare the generated sentence to the real one from the corpus. We do so 20 times and provide the average BLEU score.\n",
        "\n",
        "The used function is integrated in the package `nltk` and it is called using the following syntax `sentence_bleu(reference, candidate).`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(list(set(data)))\n",
        "n_to_char = {n: char for n, char in enumerate(characters)}\n",
        "char_to_n = {char: n for n, char in enumerate(characters)}\n",
        "\n",
        "# Vary the temperature here\n",
        "temperature = 0.5\n",
        "n_eval = 20\n",
        "seq_char_length = 100\n",
        "smoother = SmoothingFunction().method1\n",
        "bleu_score = 0.0\n",
        "\n",
        "for _ in range(n_eval):\n",
        "    # Randomly select a starting point in the test data\n",
        "    start = np.random.randint(0, len(x_test) - seq_char_length - 1)\n",
        "    pattern = x_test[start].tolist()\n",
        "    reference = x_test[start + seq_char_length].tolist()\n",
        "\n",
        "    # Convert reference from numbers to characters\n",
        "    reference = ''.join([n_to_char[value] for value in reference])\n",
        "\n",
        "    # Generate characters using the model\n",
        "    output_sent = ''\n",
        "    for n_char in range(seq_char_length):\n",
        "        x = np.reshape(pattern, (1, len(pattern)))\n",
        "        prediction = text_gen_model.predict(x, verbose=0).astype(np.float64)\n",
        "        prediction = np.log(prediction + 1e-7) / (temperature + 0.01)\n",
        "        exp_preds = np.exp(prediction)\n",
        "        prediction = exp_preds / np.sum(exp_preds)\n",
        "        prediction = np.random.multinomial(1, prediction[0, :], 1)\n",
        "        index = np.argmax(prediction)\n",
        "        result = n_to_char[index]\n",
        "        output_sent += result\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "\n",
        "    # Preprocess reference and candidate text\n",
        "    reference = word_tokenize(reference.lower())\n",
        "    candidate = word_tokenize(output_sent.lower())\n",
        "\n",
        "    # Remove empty strings (if any) after tokenization\n",
        "    reference = list(filter(lambda x: x != '', reference))\n",
        "    candidate = list(filter(lambda x: x != '', candidate))\n",
        "\n",
        "    # Remove incomplete words at the beginning and end of both lists\n",
        "    if len(reference) > 2:\n",
        "        reference = reference[1:-1]\n",
        "    if len(candidate) > 2:\n",
        "        candidate = candidate[1:-1]\n",
        "\n",
        "    # Compute BLEU score for the candidate and reference\n",
        "    bleu_score += sentence_bleu(reference, candidate, smoothing_function=smoother)\n",
        "\n",
        "bleu_score /= n_eval\n",
        "print(\"BLEU Score:\", bleu_score)"
      ],
      "metadata": {
        "id": "qL6GfMfvXiYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEPP1gUthOI0"
      },
      "source": [
        "# Transformers\n",
        "RNNs are still quite used in several architectures and Natural Language Processing (NLP) tasks. However, during the last 3 years (since the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) was published) a new architecture, the Transformer, has started to outperform RNNs in several text/NLP benchmarks. A Transformer does not use any recurrence, it instead uses attention to focus on specific parts of a sentence. If you want an in-depth explanation of the architecture, you can take a look to the [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) blog post.\n",
        "\n",
        "Both text classification and text generation tasks have been succesfully tackled by transformers. Transfomers seem to scale better than recurrent neural networks, they can have great results when using large architectures and large datasets, whereas RNNs seem not to benefit as much from using more parameters and training examples. State-of-the-art transformer models can have even billions of parameters and can benefit from training in TBs of text data.\n",
        "\n",
        "We will show examples for both text classification and text generation of models pretrained on large datasets. The aim of the examples is to show you how these large models trained in diverse/large datasets can transfer their knowledge to several tasks (similarly to the transfer learning/finetuning section in the CNN Architectures notebook). However, we will not implement any transformer model from scratch. If you are interested in doing so, besides the theoretical explanation of the [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) blog post, you can also check the [transformers example](https://keras.io/examples/nlp/text_classification_with_transformer/) in the Keras docs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPNbRN4lCkkL"
      },
      "source": [
        "##Text Classification\n",
        "\n",
        "For the text classification task, the model called BERT from Google, and all the subsequent variants have achieved state-of-the-art results. BERT is a method to train language representations, meaning that you then can use those represenations/embeddings to predict the label of the sentence in a text classification task.\n",
        "\n",
        "We will now show a quick example of finetuning BERT in IMDB to show you the capabilities of this model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import wget\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def download_and_extract_imdb():\n",
        "    if not os.path.exists('aclImdb'):\n",
        "        print(\"Downloading IMDB dataset...\")\n",
        "        url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "        wget.download(url, bar=wget.bar_adaptive)\n",
        "        print(\"\\nExtracting dataset...\")\n",
        "        with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
        "            for member in tqdm(iterable=tar.getmembers(), desc='Extracting files'):\n",
        "                tar.extract(member)\n",
        "        os.remove('aclImdb_v1.tar.gz')\n",
        "        print(\"Dataset extraction completed!\")\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data_dir, tokenizer, max_length=256):\n",
        "        self.texts = []\n",
        "        self.labels = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Load positive reviews\n",
        "        print(f\"\\nLoading data from {data_dir}\")\n",
        "        pos_path = os.path.join(data_dir, 'pos')\n",
        "        for file_name in tqdm(os.listdir(pos_path), desc='Loading positive reviews'):\n",
        "            with open(os.path.join(pos_path, file_name), 'r', encoding='utf-8') as f:\n",
        "                self.texts.append(f.read())\n",
        "                self.labels.append(1)\n",
        "\n",
        "        # Load negative reviews\n",
        "        neg_path = os.path.join(data_dir, 'neg')\n",
        "        for file_name in tqdm(os.listdir(neg_path), desc='Loading negative reviews'):\n",
        "            with open(os.path.join(neg_path, file_name), 'r', encoding='utf-8') as f:\n",
        "                self.texts.append(f.read())\n",
        "                self.labels.append(0)\n",
        "\n",
        "        print(f\"Loaded {len(self.texts)} reviews in total\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label)\n",
        "        }\n",
        "\n",
        "def train_and_evaluate():\n",
        "    # Download and extract dataset\n",
        "    download_and_extract_imdb()\n",
        "\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    print(\"\\nLoading BERT tokenizer and model...\")\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "    model = model.to(device)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = IMDBDataset('aclImdb/train', tokenizer)\n",
        "    test_dataset = IMDBDataset('aclImdb/test', tokenizer)\n",
        "\n",
        "    batch_size = 16\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training...\")\n",
        "    model.train()\n",
        "    total_steps = len(train_loader)\n",
        "    start_time = time.time()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(train_loader, desc=f'Training', total=total_steps)\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        avg_loss = running_loss / (batch_idx + 1)\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'avg_loss': f'{avg_loss:.4f}',\n",
        "            'batch': f'{batch_idx + 1}/{total_steps}'\n",
        "        })\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nStarting evaluation...\")\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    total_eval_steps = len(test_loader)\n",
        "\n",
        "    progress_bar = tqdm(test_loader, desc='Evaluating', total=total_eval_steps)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label']\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            all_predictions.extend(predictions)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    # Calculate and print metrics\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Print detailed classification report\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(all_labels, all_predictions,\n",
        "                              target_names=['Negative', 'Positive']))\n",
        "\n",
        "train_and_evaluate()"
      ],
      "metadata": {
        "id": "H7p4CcVq7VvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CfPJQK7j3JO"
      },
      "source": [
        "## Text Generation\n",
        "\n",
        "In text generation, the GPT variants from OpenAI have shown impressive results. We will now show an example from a pretrained [GPT2](https://openai.com/blog/better-language-models/) using the [Hugging Face](https://huggingface.co/transformers/index.html#) library, which it contains several state-of-the-art Natural Language Processing (NLP) models. The code to generate text is extracted from this [Colab Notebook](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb#scrollTo=HBtDOdD0wx3l).\n",
        "\n",
        "First, we install the Hugging Face library and load a GPT2 pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj4IgSwrZ58G"
      },
      "source": [
        "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = transformers.TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbs0t7nmkKgi"
      },
      "source": [
        "Now, we generate text using as input a part of the Tiny Shakespeare dataset. Note that this model has not been finetuned using the Tiny Shakespeare dataset, instead it has been trained in a larger more diverse dataset. You can run the following code to get different outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzzXQEW0Z9LO"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode('First Citizen:\\nBefore we proceed any further, hear me speak.\\nAll:\\nSpeak, speak.\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?', return_tensors='tf')\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=200,\n",
        "    top_k=50\n",
        ")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBWzhH76cZTr"
      },
      "source": [
        "What is quite impressive about the output of the GPT model is the capability of matching the style of the input text. The model is not finetuned in our dataset, but it still generates characters names and similary grammar/vocabulary to the given input. This is the result of the large capacity of the model and the diverse training data it used.\n",
        "\n",
        "Let's try to change the input to a completely different one, using in this case an input from a rugby game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RN1CBKGdN8W"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(\"40 mins. Tom Curry receives Stuart Hogg’s kick-off and Youngs kicks poorly to put Scotland back on the attack.\\n\", return_tensors='tf')\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=100,\n",
        "    top_k=50\n",
        ")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z956_MZLetBY"
      },
      "source": [
        "We see how the model understands that the input is related to sports and changes the output style compared to the Tiny Shakespeare example.\n",
        "\n",
        "We just showed examples with GPT2. However, the newer version of GPT, GPT3, which is trained using a quite larger model and dataset, shows largely improved results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwawQJMZ3aQM"
      },
      "source": [
        "# Coursework\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKfTmNC7Bj5E"
      },
      "source": [
        "### **Task 1: RNN Regression**\n",
        "\n",
        "In this task, you are asked to estimate the next value of a time series. Specifically, we have selected the popular airline passenger dataset. This dataset contains the number of passengers that travels with a certain airline company. The data contains 144 entries, each entry corresponds to the number of the passengers that travel in a given month. The dataset starts in 1949, and it lasts until 1960.\n",
        "\n",
        "Similarly to the previous example, we import the data and plot it to see the structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRf3UGQ33bnN"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\n",
        "\n",
        "data = pd.read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
        "plt.plot(data)\n",
        "plt.xlabel(\"Months\")\n",
        "plt.ylabel(\"Passengers\")\n",
        "plt.title(\"Airline Passengers from January 1949 to December 1960 (12 years)\")\n",
        "plt.show()\n",
        "\n",
        "# convert pandas data frame in numpy array of float.\n",
        "data_np = data.values.astype(\"float32\")\n",
        "\n",
        "# normalize data with min max normalization\n",
        "normalizer = sklearn.preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
        "dataset = normalizer.fit_transform(data_np)\n",
        "\n",
        "# Using 70% of data for training, 30% for test.\n",
        "TRAINING_PERC = 0.70\n",
        "\n",
        "train_size = int(len(dataset) * TRAINING_PERC)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
        "print(\"Number of samples training set: \" + str((len(train))))\n",
        "print(\"Number of samples test set: \" + str((len(test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOi9udyY3dHS"
      },
      "source": [
        "First of all, you need to train an RNN on the airline passenger dataset. This exercise expects you to study the impact of the `window_size` variable when defining the `train` and `test` dataset splits. Remember that the `window_size` variable indicates the number of past observations used for predicting the current value. Here, we treat the `test` split as a validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIB08JxD3ezu"
      },
      "source": [
        "def create_dataset(dataset, window_size = 1):\n",
        "    data_x, data_y = [], []\n",
        "    for i in range(len(dataset) - window_size - 1):\n",
        "        sample = dataset[i:(i + window_size), 0]\n",
        "        data_x.append(sample)\n",
        "        data_y.append(dataset[i + window_size, 0])\n",
        "    return(np.array(data_x), np.array(data_y))\n",
        "\n",
        "window_size = 1 #Use this variable to build the dataset with different number of inputs\n",
        "\n",
        "# Create test and training sets for regression with different window sizes.\n",
        "train_X, train_Y = create_dataset(train, window_size)\n",
        "test_X, test_Y = create_dataset(test, window_size)\n",
        "train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))\n",
        "test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 1))\n",
        "\n",
        "print(\"Shape of training inputs: \" + str((train_X.shape)))\n",
        "print(\"Shape of training labels: \" + str((train_Y.shape)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH6W6t5q3gfN"
      },
      "source": [
        "\n",
        "**Report**:\n",
        "\n",
        "*   Create a plot showing the test curves of models trained with different `window_size` values. Report the plot and discuss the main differences you observe between the predicted curves. You can use the style proposed on the Many to One RNNs - Regression section to plot your curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwb9IfiCLKuT"
      },
      "source": [
        "### **Task 2: Text Embeddings**\n",
        "For this task, we tackle a classification problem using the IMDB sentiment dataset as done in the example in the notebook. Labels in IMDB are 0 for negative reviews and 1 for positive reviews. The definitions of the models you will use for this task are given the code below. This task is similar to the transfer learning/finetuning task in the CNN Architectures notebook, however we now test the effect of transfer learning in the embeddings. In this task we use train, validation and test splits with Early Stopping. That means that we will take the best performing model in the validation set and use it in the test set to get a final performance.\n",
        "\n",
        "**Report**\n",
        "* Using embeddings of dimensionality 1, train a model without using any LSTM, only using an average pooling of the input embeddings (called `embeddings_model` in the code given below). Then train another model with an LSTM and trainable embeddings initialized at random (called `lstm_model`). Finally train a model with an LSTM with non-trainable embeddings initialized with GloVe embeddings (called `lstm_glove_model`). The code to train the three models is given below. Report in a table the test accuracy obtained after training with the given code for the three models. Also attach in the Appendix the training and validation accuracy curves for the different models trained. You can report the curves after using EarlyStopping with patience 10 (already given in the code), so you don't have to train for the full 50 epochs the three models. Discuss the results.\n",
        "\n",
        "* Predict the sentiment of the two given example reviews in the code below for the model trained without a LSTM (`embeddings_model`) and for the model trained with a LSTM and GloVe embeddings (`lstm_glove_model`). Report the predictions (you can use the same table as when reporting test accuracies). Discuss the results. Also discuss the differences you can observe between the GloVe embeddings and the embeddings learnt in `embeddings_model` (e.g. what kind of properties the embeddings encode, or differences in the closest words).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gq0lcyLnaSN"
      },
      "source": [
        "We provide the training code you need to use for this exercise below. First we load the dataset as we did in the tutorial. In this exercise, we will use of train, validation and test splits, which are defined in the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0ibhzVdnZ0X"
      },
      "source": [
        "# number of most-frequent words to use\n",
        "nb_words = 5000\n",
        "n_classes = 1\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = datasets.imdb.load_data(num_words=nb_words)\n",
        "print('x_train:', x_train.shape)\n",
        "print('x_test:', x_test.shape)\n",
        "# get_word_index retrieves a mapping word -> index\n",
        "word_index = datasets.imdb.get_word_index()\n",
        "# We make space for the three special tokens\n",
        "word_index_c = dict((w, i+3) for (w, i) in word_index.items())\n",
        "word_index_c['<PAD>'] = 0\n",
        "word_index_c['<START>'] = 1\n",
        "word_index_c['<UNK>'] = 2\n",
        "# Instead of having dictionary word -> index we form\n",
        "# the dictionary index -> word\n",
        "index_word = dict((i, w) for (w, i) in word_index_c.items())\n",
        "# Truncate sentences after this number of words\n",
        "maxlen = 100\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "x_val = x_train[23000:]\n",
        "y_val = y_train[23000:]\n",
        "x_train = x_train[:23000]\n",
        "y_train = y_train[:23000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EDi21HPGU3h"
      },
      "source": [
        "The following code includes the model that uses embeddings of size 1 (so each word is only represented by a single digit) and averages them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7421cnIYoPQ-"
      },
      "source": [
        "# Dimensions of the embeddings\n",
        "embedding_dim = 1\n",
        "\n",
        "print('Build model...')\n",
        "embeddings_model = models.Sequential([\n",
        "    layers.Input(shape=(maxlen,)),\n",
        "    layers.Embedding(nb_words, embedding_dim, trainable=True),\n",
        "    layers.GlobalAvgPool1D(),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "embeddings_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "embeddings_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRm3ql3UE9w7"
      },
      "source": [
        "We use Early Stopping, so the best validation model is then used to compute the result in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gom37wigpZAn"
      },
      "source": [
        "## We train the model for at most 50 epochs\n",
        "epochs = 50\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0,\n",
        "    patience=10,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = embeddings_model.fit(x_train, y_train, batch_size=32, epochs=epochs, callbacks=[early_stop], validation_data=(x_val,y_val), verbose=0)\n",
        "embeddings_model_loss, embeddings_model_accuracy = embeddings_model.evaluate(x_test, y_test)\n",
        "\n",
        "print(f'Final test loss is: {embeddings_model_loss}')\n",
        "print(f'Final test accuracy is: {embeddings_model_accuracy}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_DTYhcl68GF"
      },
      "source": [
        "Now we have `embedding_model` trained. The code below will print the embedding of any `query_word`, which in this case is a single number. We also give you the code to compute the `top_k` closest embeddings to `query_word`. The metric used is the L2 distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9uJoVAt7EWk"
      },
      "source": [
        "weights = embeddings_model.layers[0].get_weights()[0]\n",
        "query_word = '8'\n",
        "dist = ((weights - weights[word_index_c[query_word]])**2).sum(1).argsort()\n",
        "top_k = 10\n",
        "print(\"Embedding value of {:s} is {:f}\".format(query_word, weights[word_index_c[query_word]][0]))\n",
        "print('Most {:d} similar words to {:s}'.format(top_k, query_word))\n",
        "for k in range(1, top_k+1):\n",
        "  print(\"{:d}: {:s}\".format(k, index_word[dist[k]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhD9hosS7cKQ"
      },
      "source": [
        "The code below gives the prediction for two example reviews we input. Remember that predictions close to 0 refer to a negative review, and predictions close to 1 refer to a positive review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqatHlu8glYQ"
      },
      "source": [
        "negative_review = \"the movie is boring and not good\"\n",
        "words = negative_review.split()\n",
        "seq_input = [word_index_c[w] for w in words]\n",
        "seq_input = np.array([seq_input])\n",
        "seq_input = preprocessing.sequence.pad_sequences(seq_input,maxlen=100)\n",
        "negative_review_score = embeddings_model.predict(seq_input)\n",
        "\n",
        "positive_review = \"the movie is good and not boring\"\n",
        "words = positive_review.split()\n",
        "seq_input = [word_index_c[w] for w in words]\n",
        "seq_input = np.array([seq_input])\n",
        "seq_input = preprocessing.sequence.pad_sequences(seq_input,maxlen=100)\n",
        "positive_review_score = embeddings_model.predict(seq_input)\n",
        "\n",
        "print(\"The score for the negative review is:\", negative_review_score[0,0])\n",
        "print(\"The score for the positive review is:\", positive_review_score[0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FzItW89aLVj"
      },
      "source": [
        "With the above code, we trained a model that classifies the sentiment of the sentence using the average of all the embeddings, which were only of size 1. Now we will increase the capacity of the embeddings to 300 and will also add a LSTM to process the embeddings. Hence, the model has a much higher capacity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-z4K9iAaKR1"
      },
      "source": [
        "## Model parameters:\n",
        "# Dimensions of the embeddings\n",
        "embedding_dim = 300\n",
        "\n",
        "## LSTM dimensionality\n",
        "lstm_units = 50\n",
        "\n",
        "print('Build model...')\n",
        "lstm_model = models.Sequential([\n",
        "    layers.Input(shape=(maxlen,)),\n",
        "    layers.Embedding(nb_words, embedding_dim, trainable=True),\n",
        "    ### Do not modify the layers below\n",
        "    layers.Dropout(0.2),\n",
        "    layers.LSTM(lstm_units, dropout=0.2),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "lstm_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msBDHkgfFhtq"
      },
      "source": [
        "Similarly, we use EarlyStopping for this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXpHfGa-aoWW"
      },
      "source": [
        "## We train the model for 50 epochs\n",
        "epochs = 50\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0,\n",
        "    patience=10,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = lstm_model.fit(x_train, y_train, batch_size=32, epochs=epochs, callbacks=[early_stop], validation_data=(x_val,y_val), verbose=0)\n",
        "lstm_model_loss, lstm_model_accuracy = lstm_model.evaluate(x_test, y_test)\n",
        "\n",
        "print(f'Final test loss is: {lstm_model_loss}')\n",
        "print(f'Final test accuracy is: {lstm_model_accuracy}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3F0IRz2FqVX"
      },
      "source": [
        "We just trained a model with a large number of parameters in the IMDB, which is a small dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7dnHR3za1xB"
      },
      "source": [
        "The last model we train is the same model as the `lstm_model` above, but in this case we use the embeddings from the GloVe method (which were introduced in this notebook) without any finetuning. First, we download them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7ZjGnuWb4H9"
      },
      "source": [
        "!wget https://imperialcollegelondon.box.com/shared/static/c9trfhhwl9ohje5g3sapu3xk2zoywp3c.txt -O gensim_glove_vectors.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vhK-7g7GIZX"
      },
      "source": [
        "Then we load the GloVe embeddings with dimensionality 300 we just downloaded. This takes some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KeP7idDb-Wj"
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV_QslV7ctj4"
      },
      "source": [
        "We now load the pretrained embeddings in a numpy array to then pass it to the Keras Embedding layer. Change the shape of `embedding_matrix` to the corresponding embedding dimensionality you are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npBJO9pOcA9D"
      },
      "source": [
        "embedding_matrix = np.zeros((nb_words, 300))\n",
        "for word, i in word_index_c.items():\n",
        "    if word in glove_model:\n",
        "      embedding_vector = glove_model[word]\n",
        "      if embedding_vector is not None and i < nb_words:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXEgmD5CdbEo"
      },
      "source": [
        "To initialize the Keras Embedding layer with the embeddings we loaded, we can use the argument `weights=[embedding_matrix]`. Also, to freeze the embeddings during training, we use `trainable=False`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTDZJhNqbFiz"
      },
      "source": [
        "## Model parameters:\n",
        "# Dimensions of the embeddings\n",
        "embedding_dim = 300\n",
        "\n",
        "## LSTM dimensionality\n",
        "lstm_units = 50\n",
        "\n",
        "print('Build model...')\n",
        "lstm_glove_model = models.Sequential([\n",
        "    layers.Input(shape=(maxlen,)),\n",
        "    layers.Embedding(nb_words, embedding_dim, weights=[embedding_matrix], trainable=False),\n",
        "    ### Do not modify the layers below\n",
        "    layers.Dropout(0.2),\n",
        "    layers.LSTM(lstm_units, dropout=0.2),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_glove_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "lstm_glove_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7c-u98zbJaf"
      },
      "source": [
        "## We train the model for 50 epochs\n",
        "epochs = 50\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0,\n",
        "    patience=10,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = lstm_glove_model.fit(x_train, y_train, batch_size=32, epochs=epochs, callbacks=[early_stop], validation_data=(x_val,y_val), verbose=0)\n",
        "lstm_glove_model_loss, lstm_glove_model_accuracy = lstm_glove_model.evaluate(x_test, y_test)\n",
        "\n",
        "print(f'Final test loss is: {lstm_glove_model_loss}')\n",
        "print(f'Final test accuracy is: {lstm_glove_model_accuracy}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0v3mUULGkfV"
      },
      "source": [
        "We can also compute the closest words in the GloVe embeddings to any `query_word` using the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQDqhKtOc56X"
      },
      "source": [
        "query_word = '8'\n",
        "weights = embedding_matrix\n",
        "\n",
        "dist = ((weights - weights[word_index_c[query_word]])**2).sum(1).argsort()\n",
        "top_k = 10\n",
        "print('Most {:d} similar words to {:s}'.format(top_k, query_word))\n",
        "for k in range(1, top_k+1):\n",
        "  print(\"{:d}: {:s}\".format(k, index_word[dist[k]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXiBKBgtG2xf"
      },
      "source": [
        "We use the same example reviews as for the `embedding_model` case and we compute the predictions using the `lstm_glove_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z10tY2Nyg9px"
      },
      "source": [
        "negative_review = \"the movie is boring and not good\"\n",
        "words = negative_review.split()\n",
        "seq_input = [word_index_c[w] for w in words]\n",
        "seq_input = np.array([seq_input])\n",
        "seq_input = preprocessing.sequence.pad_sequences(seq_input,maxlen=100)\n",
        "negative_review_score = lstm_glove_model.predict(seq_input)\n",
        "\n",
        "positive_review = \"the movie is good and not boring\"\n",
        "words = positive_review.split()\n",
        "seq_input = [word_index_c[w] for w in words]\n",
        "seq_input = np.array([seq_input])\n",
        "seq_input = preprocessing.sequence.pad_sequences(seq_input,maxlen=100)\n",
        "positive_review_score = lstm_glove_model.predict(seq_input)\n",
        "\n",
        "print(\"The score for the negative review is:\", negative_review_score[0,0])\n",
        "print(\"The score for the positive review is:\", positive_review_score[0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CywiPmy1eZoF"
      },
      "source": [
        "### **Task 3: Text Generation**\n",
        "In this task we focus on the text generation problem. For this purpose, we will download the scripts of the TV show Game of Thrones and try to generate some text resembling the style of the scripts.\n",
        "\n",
        "\n",
        "**Report**\n",
        "* Plot the retrieved BLEU for different temperature values (from 0 to 2 in the x-axis) for both the character-level model and the word-level model. To compute the BLEU score, use a minimum of 20 generated samples per temperature used to reduce variability (you can increase it at the cost of higher computational time for lower variability). Each sample should contain 100 characters for the char-level model or 30 words for the word-level model (the code given uses these parameters by default). Do you see any relationship between the obtained BLEU score and temperature used? If you generate sentences at different temperatures what differences can you observe? Are the generated sentences grammatically correct? Do the generated sentences make sense?\n",
        "\n",
        "We give below the code needed to download the dataset and to compute the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD9gEFKXqukY"
      },
      "source": [
        "We first download and read the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_I1pRickXKl"
      },
      "source": [
        "!git clone https://github.com/shekharkoirala/Game_of_Thrones\n",
        "\n",
        "data = open('./Game_of_Thrones/Data/final_data.txt', 'r').read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHqzKZQ8aYkp"
      },
      "source": [
        "**Character-level model**\n",
        "\n",
        "We first include the code to build the character-level dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnwDgj_unBhW"
      },
      "source": [
        "characters = sorted(list(set(data)))\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}\n",
        "x_char = []\n",
        "y_char = []\n",
        "length = len(data)\n",
        "seq_char_length = 100\n",
        "for i in range(0, length-seq_char_length):\n",
        "  sequence = data[i:i + seq_char_length]\n",
        "  label = data[i + seq_char_length]\n",
        "  x_char.append([char_to_n[char] for char in sequence])\n",
        "  y_char.append(char_to_n[label])\n",
        "n_samples = len(x_char)\n",
        "print ('Total Samples:' , n_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcmI8iV7afK5"
      },
      "source": [
        "The splits used for training are given below, although we already give the model trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VkGsFkMnCuB"
      },
      "source": [
        "n_samples_train = int(n_samples*0.7)\n",
        "n_samples_test = int(n_samples*0.2)\n",
        "n_samples_val = n_samples - n_samples_train - n_samples_test\n",
        "\n",
        "x_train_char = x_char[:n_samples_train]\n",
        "x_val_char = x_char[n_samples_train:n_samples_train+n_samples_val]\n",
        "x_test_char = x_char[n_samples_test:]\n",
        "\n",
        "y_train_char = y_char[:n_samples_train]\n",
        "y_val_char = y_char[n_samples_train:n_samples_train+n_samples_val]\n",
        "y_test_char = y_char[n_samples_test:]\n",
        "\n",
        "## Transform the list to a numpy array\n",
        "x_train_char = np.reshape(x_train_char, (len(x_train_char), seq_char_length))\n",
        "x_val_char = np.reshape(x_val_char, (len(x_val_char), seq_char_length))\n",
        "x_test_char = np.reshape(x_test_char, (len(x_test_char), seq_char_length))\n",
        "\n",
        "## Onehot encoding of labels\n",
        "y_train_char = utils.to_categorical(np.asarray(y_train_char))\n",
        "y_val_char = utils.to_categorical(np.asarray(y_val_char))\n",
        "y_test_char = utils.to_categorical(np.asarray(y_test_char))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcyzP955aukz"
      },
      "source": [
        "The definition of the model is the one given below. You will not train the model, so this piece of code is only for you to know what kind of model we trained for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO3FoRM1nEOe"
      },
      "source": [
        "# define the LSTM model\n",
        "embedding_size = 300\n",
        "lstm_units = 256\n",
        "\n",
        "char_gen_model = models.Sequential([\n",
        "    layers.Input(shape=(seq_char_length,)),\n",
        "    layers.Embedding(y_train_char.shape[1], embedding_size),\n",
        "    layers.LSTM(lstm_units),\n",
        "    layers.Dense(y_train_char.shape[1], activation='softmax'),\n",
        "])\n",
        "\n",
        "char_gen_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "char_gen_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSgGhwym_WLx"
      },
      "source": [
        "As the training takes a while, we include a saved model that you can load to skip the training step. Use this model to compute your results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train and test the model\n",
        "\n",
        "epochs = 100\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0,\n",
        "    patience=10,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = char_gen_model.fit(x_train_char, y_train_char, validation_data=(x_val_char, y_val_char), batch_size=128, epochs=epochs, callbacks=[early_stop])\n",
        "char_gen_model_loss, char_gen_model_accuracy = char_gen_model.evaluate(x_test_char, y_test_char)\n",
        "\n",
        "print(f'Final test loss is: {char_gen_model_loss}')\n",
        "print(f'Final test accuracy is: {char_gen_model_accuracy}')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7J6m8nQmt1eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "!wget 'https://raw.githubusercontent.com/MatchLab-Imperial/deep-learning-course/master/asset/05_RNN/char_gen_model.keras' -O char_gen_model.keras\n",
        "\n",
        "char_gen_model = models.load_model('char_gen_model.keras')"
      ],
      "metadata": {
        "id": "qr7SMrHtfim_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsuUE8vua77e"
      },
      "source": [
        "The code you need to evaluate the BLEU score is given below. Vary the temperature to the different needed values. It takes around 1 minute in average per temperature if `n_eval` is set to 20."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ufMSdgp0A6"
      },
      "source": [
        "characters = sorted(list(set(data)))\n",
        "n_to_char = {n: char for n, char in enumerate(characters)}\n",
        "char_to_n = {char: n for n, char in enumerate(characters)}\n",
        "\n",
        "# Vary the temperature here\n",
        "temperature = 0.5\n",
        "n_eval = 20\n",
        "seq_char_length = 100\n",
        "smoother = SmoothingFunction().method1\n",
        "bleu_score = 0.0\n",
        "\n",
        "for _ in range(n_eval):\n",
        "    # Randomly select a starting point in the test data\n",
        "    start = np.random.randint(0, len(x_test_char) - seq_char_length - 1)\n",
        "    pattern = x_test_char[start].tolist()\n",
        "    reference = x_test_char[start + seq_char_length].tolist()\n",
        "\n",
        "    # Convert reference from numbers to characters\n",
        "    reference = ''.join([n_to_char[value] for value in reference])\n",
        "\n",
        "    # Generate characters using the model\n",
        "    output_sent = ''\n",
        "    for n_char in range(seq_char_length):\n",
        "        x = np.reshape(pattern, (1, len(pattern)))\n",
        "        prediction = char_gen_model.predict(x, verbose=0).astype(np.float64)\n",
        "        prediction = np.log(prediction + 1e-7) / (temperature+0.01)\n",
        "        exp_preds = np.exp(prediction)\n",
        "        prediction = exp_preds / np.sum(exp_preds)\n",
        "        prediction = np.random.multinomial(1, prediction[0, :], 1)\n",
        "        index = np.argmax(prediction)\n",
        "        result = n_to_char[index]\n",
        "        output_sent += result\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "\n",
        "    # Preprocess reference and candidate text\n",
        "    reference = word_tokenize(reference.lower())\n",
        "    candidate = word_tokenize(output_sent.lower())\n",
        "\n",
        "    # Remove empty strings (if any) after tokenization\n",
        "    reference = list(filter(lambda x: x != '', reference))\n",
        "    candidate = list(filter(lambda x: x != '', candidate))\n",
        "\n",
        "    # Remove incomplete words at the beginning and end of both lists\n",
        "    if len(reference) > 2:\n",
        "        reference = reference[1:-1]\n",
        "    if len(candidate) > 2:\n",
        "        candidate = candidate[1:-1]\n",
        "\n",
        "    # Compute BLEU score for the candidate and reference\n",
        "    bleu_score += sentence_bleu([reference], candidate, smoothing_function=smoother)\n",
        "\n",
        "bleu_score /= n_eval\n",
        "print(\"BLEU Score:\", bleu_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVEF_WPycOnD"
      },
      "source": [
        "The code below allows you to generate sentences for different input patterns and different temperature values. You can test how the temperature values affect the quality of the output sentences for the character-level model by generating a few examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yshz7OZGnIWX"
      },
      "source": [
        "# Change the temperature here\n",
        "temperature = 0.5\n",
        "# You can modify the input pattern here.\n",
        "pattern = [char_to_n[value] for value in list(\"TYRION pours himself some wine and drinks it down. He pours another glass, and walks back to CERSEI \")]\n",
        "pattern = pattern[:seq_char_length]\n",
        "print(\"\\nPredicted:\")\n",
        "# generate 300 characters\n",
        "for i in range(300):\n",
        "  x = np.reshape(pattern, (1, len(pattern)))\n",
        "  prediction = char_gen_model.predict(x, verbose=0).astype(np.float64)\n",
        "  ## We put the constant 0.02 to avoid dividing by zero\n",
        "  ## We sum by 1e-7 to avoid log(0)\n",
        "  prediction = np.log(prediction + 1e-7) / (temperature + 0.01)\n",
        "  exp_preds = np.exp(prediction)\n",
        "  prediction = exp_preds / np.sum(exp_preds)\n",
        "  ## We applied the smoothing with the temperature\n",
        "  ## Now we predict following the probabilities in the variable prediction\n",
        "  prediction = np.random.multinomial(1, prediction[0,:], 1)\n",
        "  index = np.argmax(prediction)\n",
        "  result = n_to_char[index]\n",
        "  seq_in = [n_to_char[value] for value in pattern]\n",
        "  ## Print the result\n",
        "  sys.stdout.write(result)\n",
        "\n",
        "  ## Create the input sequence for the next character by appending the predicted\n",
        "  ## character and dropping the first one to always have constant seq. length\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeNwJWRN0Mue"
      },
      "source": [
        "**Word-level model**\n",
        "\n",
        "We now give the code to run the word-level model. The code is similar to the char-level model. The main difference is that we only try to predict the 2000 words most commonly used in the dataset. The reason for this limitation is to limit the size of the output layer and number of input embeddings for memory constraints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-XfzutEv9Ta"
      },
      "source": [
        "n_words = 2000\n",
        "# We preprocess the data to separate the punctuation from the words so e.g.\n",
        "# \"sword.\" and \"sword\" are not treated as separate words.\n",
        "# We also use only lower-cased words\n",
        "data_p = data.replace('.', ' . ').replace(',', ' , ').replace(':', ' : ').replace('?', ' ? ')\n",
        "data_p = data_p.replace('!', ' !').replace('\\n', ' \\n ').replace('[', ' [ ').replace(']', ' ] ')\n",
        "data_p = data_p.replace(')', ' ) ').replace('(', ' ( ').lower().split(' ')\n",
        "data_p = list(filter(lambda x: x != '' and x != ' ', data_p))\n",
        "common = dict(Counter(data_p).most_common(n_words))\n",
        "n_to_word = {n:word for n, word in enumerate(common.keys())}\n",
        "word_to_n = {word:n for n, word in enumerate(common.keys())}\n",
        "x_word = []\n",
        "y_word = []\n",
        "length = len(data_p)\n",
        "seq_length = 30\n",
        "for i in range(0, length-seq_length):\n",
        "  sequence = data_p[i:i + seq_length]\n",
        "  label = data_p[i + seq_length]\n",
        "  seq = []\n",
        "  if label in common:\n",
        "    for word in sequence:\n",
        "      if word in word_to_n:\n",
        "        seq.append(word_to_n[word])\n",
        "      else:\n",
        "        # If the word in the input sequence is not from the most common 2000\n",
        "        # words, we use a special token. We use the same token for all the\n",
        "        # non-common words.\n",
        "        seq.append(len(word_to_n))\n",
        "    x_word.append(seq)\n",
        "    y_word.append(word_to_n[label])\n",
        "n_samples = len(x_word)\n",
        "print('Total Samples:' , n_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teAOmchqxvQd"
      },
      "source": [
        "n_samples_train = int(n_samples*0.7)\n",
        "n_samples_test = int(n_samples*0.2)\n",
        "n_samples_val = n_samples - n_samples_train - n_samples_test\n",
        "\n",
        "x_train_word = x_word[:n_samples_train]\n",
        "x_val_word = x_word[n_samples_train:n_samples_train+n_samples_val]\n",
        "x_test_word = x_word[n_samples_test:]\n",
        "\n",
        "y_train_word = np.array(y_word[:n_samples_train])\n",
        "y_val_word = np.array(y_word[n_samples_train:n_samples_train+n_samples_val])\n",
        "y_test_word = np.array(y_word[n_samples_test:])\n",
        "\n",
        "## Transform the list to a numpy array\n",
        "x_train_word = np.reshape(x_train_word, (len(x_train_word), seq_length))\n",
        "x_val_word = np.reshape(x_val_word, (len(x_val_word), seq_length))\n",
        "x_test_word = np.reshape(x_test_word, (len(x_test_word), seq_length))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJvV2NaP0yam"
      },
      "source": [
        "The definition of the word-level model we train is given below. The model is the same as in the char-level case, the only difference is the size of the output vector and the number of input embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myJRka3BxyUf"
      },
      "source": [
        "# define the LSTM model\n",
        "embedding_size = 300\n",
        "lstm_units = 256\n",
        "\n",
        "word_gen_model = models.Sequential([\n",
        "    layers.Input(shape=(seq_length,)),\n",
        "    layers.Embedding(n_words + 1, embedding_size),\n",
        "    layers.LSTM(lstm_units),\n",
        "    layers.Dense(n_words + 1, activation='softmax'),\n",
        "])\n",
        "\n",
        "word_gen_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "word_gen_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train and test the model\n",
        "\n",
        "epochs = 100\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0,\n",
        "    patience=10,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = word_gen_model.fit(x_train_word, y_train_word, validation_data=(x_val_word, y_val_word), batch_size=128, epochs=epochs, callbacks=[early_stop])\n",
        "word_gen_model_loss, word_gen_model_accuracy = word_gen_model.evaluate(x_test_word, y_test_word)\n",
        "\n",
        "print(f'Final test loss is: {word_gen_model_loss}')\n",
        "print(f'Final test accuracy is: {word_gen_model_accuracy}')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kcw8vzdQyROI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/MatchLab-Imperial/deep-learning-course/master/asset/05_RNN/word_gen_model.keras' -O word_gen_model.keras\n",
        "\n",
        "word_gen_model = models.load_model('word_gen_model.keras')"
      ],
      "metadata": {
        "id": "ql_o1vFO5rFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgvJiBOs5CEF"
      },
      "source": [
        "# Vary the temperature here\n",
        "temperature = 0.7\n",
        "n_eval = 20\n",
        "seq_char_length = 100\n",
        "smoother = SmoothingFunction().method1\n",
        "bleu_score = 0.0\n",
        "\n",
        "for _ in range(n_eval):\n",
        "    # We look for references that do not contain any non-common words as we only\n",
        "    # learnt to predict the 2000 most common words\n",
        "    while True:\n",
        "        start = np.random.randint(0, len(x_test_word)-seq_length-1)\n",
        "        pattern = x_test_word[start].tolist()\n",
        "        reference = x_test_word[start+seq_length].tolist()\n",
        "        if n_words not in reference:\n",
        "            break\n",
        "    reference = ' '.join([n_to_word[value] for value in reference])\n",
        "\n",
        "    # generate words\n",
        "    output_sent = ''\n",
        "    for i in range(seq_length):\n",
        "        x = np.reshape(pattern, (1, len(pattern)))\n",
        "        prediction = word_gen_model.predict(x, verbose=0).astype(np.float64)\n",
        "        prediction = np.log(prediction + 1e-7) / (temperature+0.01)\n",
        "        exp_preds = np.exp(prediction)\n",
        "        prediction = exp_preds / np.sum(exp_preds)\n",
        "        prediction = np.random.multinomial(1, prediction[0, :], 1)\n",
        "        index = np.argmax(prediction)\n",
        "        result = n_to_word.get(index, '')\n",
        "        output_sent += result + ' '\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "\n",
        "    # Preprocess reference and candidate text\n",
        "    reference = word_tokenize(reference.lower())\n",
        "    candidate = word_tokenize(output_sent.lower())\n",
        "\n",
        "    # Remove empty strings (if any) after tokenization\n",
        "    reference = list(filter(lambda x: x != '', reference))\n",
        "    candidate = list(filter(lambda x: x != '', candidate))\n",
        "\n",
        "    # Remove incomplete words at the beginning and end of both lists\n",
        "    if len(reference) > 2:\n",
        "        reference = reference[1:-1]\n",
        "    if len(candidate) > 2:\n",
        "        candidate = candidate[1:-1]\n",
        "\n",
        "    # Compute BLEU score for the candidate and reference\n",
        "    bleu_score += sentence_bleu(reference, candidate, smoothing_function=smoother)\n",
        "\n",
        "bleu_score /= n_eval\n",
        "print(\"BLEU Score:\", bleu_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf1cYUeC27W3"
      },
      "source": [
        "import sys\n",
        "# Vary the temperature here\n",
        "temperature = 0.7\n",
        "pattern = \"TYRION pours himself some wine and drinks it down. He pours another glass, and walks back to CERSEI placing his cup on her desk. He takes another glass.\\nTYRION: \"\n",
        "# Process input pattern\n",
        "pattern = pattern.replace('.', ' . ').replace(',', ' , ').replace(':', ' : ').replace('?', ' ? ')\n",
        "pattern = pattern.replace('!', ' !').replace('\\n', ' \\n ').replace('[', ' [ ').replace(']', ' ] ')\n",
        "pattern = pattern.replace(')', ' ) ').replace('(', ' ( ').lower().split(' ')\n",
        "pattern = list(filter(lambda x: x != '' and x != ' ', pattern))\n",
        "\n",
        "pattern = pattern[:seq_length]\n",
        "\n",
        "print(\"\\nInput Pattern:\\n\", ' '.join(pattern))\n",
        "pattern = [word_to_n.get(value, n_words) for value in pattern]\n",
        "print(\"\\nPredicted:\")\n",
        "# generate 100 words\n",
        "for i in range(100):\n",
        "  x = np.reshape(pattern, (1, len(pattern)))\n",
        "  prediction = word_gen_model.predict(x, verbose=0).astype(np.float64)\n",
        "  ## We put the constant 0.02 to avoid dividing by zero\n",
        "  ## We sum by 1e-7 to avoid log(0)\n",
        "  prediction = np.log(prediction + 1e-32) / (temperature + 0.01)\n",
        "  exp_preds = np.exp(prediction)\n",
        "  prediction = exp_preds / np.sum(exp_preds)\n",
        "  ## We applied the smoothing with the temperature\n",
        "  ## Now we predict following the probabilities in the variable prediction\n",
        "  prediction = np.random.multinomial(1, prediction[0,:], 1)\n",
        "  index = np.argmax(prediction)\n",
        "  result = n_to_word[index]\n",
        "  ## Print the result\n",
        "  sys.stdout.write(result+' ')\n",
        "\n",
        "  ## Create the input sequence for the next character by appending the predicted\n",
        "  ## character and dropping the first one to always have constant seq. length\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}