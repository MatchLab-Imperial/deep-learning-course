{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb9t6NTe-6BL"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/07_VAE_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtCVMgi8iygt"
      },
      "source": [
        "# Variational Autoencoders\n",
        "\n",
        "<a href=\"https://ibb.co/GMMVqft\"><img src=\"https://i.ibb.co/X55zqf3/vae.jpg\" alt=\"vae\" border=\"0\"></a>\n",
        "\n",
        "Image taken from [here](http://kvfrans.com/variational-autoencoders-explained/)\n",
        "\n",
        "In the autoencoder tutorial we showed how to learn a meaningful representation of the data by using an autoencoder. In an autoencoder, the input image was transformed into a vector which encoded the information from the image in a lower dimensionality space. Then, we decoded that vector to get a reconstruction of the input image. However, the model was focused on encoding existing data for representation learning or similar purposes. To tackle the generation of new data, we will use a Variational Autoencoder (VAE) approach. The image shows an overview of the VAE method.\n",
        "\n",
        "Parts of the code are taken from [here](https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/), which contains a more in-depth explanation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNlgJvqul7W2"
      },
      "source": [
        "Before starting to define the different parts of the VAE, let's import the needed modules for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91g-k-uqdRW7"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import datetime\n",
        "import sys\n",
        "from IPython.display import HTML, display, clear_output\n",
        "from PIL import Image as pil_image\n",
        "import ipywidgets as widgets\n",
        "from matplotlib import animation\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(123)  # for reproducibility\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndKHD2XrmDej"
      },
      "source": [
        "Now we load MNIST, which will be our toy dataset for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY6F0Ln8czZA"
      },
      "outputs": [],
      "source": [
        "original_dim = 784\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9_UOneRBn1q"
      },
      "source": [
        "Now let's do a quick recap of the Variational AutoEncoder (VAE) theory. As stated in the lecture, we want to find the $\\hat{\\theta}$ that approximates $P_\\theta(x)$ by doing:\n",
        "\n",
        "$$\n",
        "\\hat{\\theta} = \\text{argmax}_\\theta \\sum_{i=1}^n \\mathbb{E}_{Q_\\phi(z|x_i)}[\\log(P_\\theta(x_i|z))] - \\text{KL}(Q_\\phi(z|x_i) || P_\\theta(z))\n",
        "$$\n",
        "Se will minimize the negative of that term in order to maximize it, where\n",
        "we will train our model using a stochastic approach by sampling mini-batches from the dataset. First, we define the two losses we will use. The loss `nll` is the first term of the equation, whereas the `KLDivergenceLayer` is the second term of the loss. The `KLDivergenceLayer`will be used to compute an extra loss in the middle of the model via the `self.add_loss` function, but it does not change its inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvWDTeNodH78"
      },
      "outputs": [],
      "source": [
        "# Negative log likelihood loss (Bernoulli)\n",
        "def nll(y_true, y_pred):\n",
        "    return F.binary_cross_entropy(y_pred, y_true, reduction='none').sum(dim=-1)\n",
        "\n",
        "# KL Divergence Layer\n",
        "class KLDivergenceLayer(nn.Module):\n",
        "    def forward(self, mu, log_var):\n",
        "        kl_batch = -0.5 * (1 + log_var - mu.pow(2) - log_var.exp()).sum(dim=1)\n",
        "        self.kl_loss = kl_batch.mean()\n",
        "        return mu, log_var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUNQrpSjDC1Q"
      },
      "source": [
        "In the last block, we just defined the two losses we will use. Here we build the whole VAE model. First, we build the encoder. The goal of the encoder $\\phi$ is to approximate $P_\\theta(z|x_i)$ via $Q_\\phi(z|x_i)$.  We assume that $P(z)$ is a Normal distribution with zero mean and unit variance. We also assume that $Q(z|x)={N}(\\mu_x, \\sigma_x)$ so the encoder tries to recover the parameters $\\mu_x, \\sigma_x$ for the different $x$ (which are the input images), i.e. the encoder outputs for each latent dimension a mean and standard deviation. The code for this encoder is the following:\n",
        "\n",
        "```python\n",
        "# Define latent dimension\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Compute μ and log(σ²)\n",
        "        self.z_mu = nn.Linear(128*7*7, latent_dim)\n",
        "        self.z_log_var = nn.Linear(128*7*7, latent_dim)\n",
        "        # To Apply KL divergence layer\n",
        "        self.kl_layer = KLDivergenceLayer()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJKpM3JjEyRK"
      },
      "source": [
        "As we mentioned, the encoder will output the parameters $\\mu_x, \\sigma_x$. Now we will sample from the normal distribution defined by those parameters to pass it to the decoder. However, we now face one of the problems of implementing a VAE: we want to optimize both decoder and encoder at the same time to i) encourage good reconstruction and ii) to make $z$ follow a normal distribution. What is the problem here? Using a standard sampling method, i.e by directly sampling using the mean and standard deviation output by the encoder, we cannot train it in an end-to-end manner as sampling is not a differentiable operation.\n",
        "\n",
        "We need a trick to solve this issue. We can use one of the properties of the Normal probability distribution, which is that sampling $\\mathcal{N}(\\mu_\\psi, \\sigma_\\psi)$ is the same as $\\mu_\\psi + \\sigma_\\psi\\mathcal{N}(0, 1)$. Now, the sampling process is just a factor multiplying by the prediction $\\sigma_\\phi$ of the encoder, meaning we can propagate the gradients from the output back to the encoder. This is called the reparametrisation trick. We use this trick in the following block.\n",
        "\n",
        "\n",
        "```python\n",
        "def reparameterize(self, mu, log_var):\n",
        "        ##### Reparametrisation trick\n",
        "        # Log_var to sigma\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        # Define a separate noise input (which must be provided during training)\n",
        "        eps = torch.randn_like(std)\n",
        "        # Multiply sigma with the external noise and add mu to obtain the latent vector\n",
        "        return mu + eps * std\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6MSQqJ3ot_r"
      },
      "source": [
        "Now we define the decoder, which will take the sampling from $\\mathcal{N}(\\mu_\\psi, \\sigma_\\psi)$ as input and output an image.\n",
        "\n",
        "\n",
        "```python\n",
        "# Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128*7*7),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Unflatten(1, (128, 7, 7)),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, kernel_size=5, padding=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(64, 1, kernel_size=5, padding=2),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LElZ521BrhrA"
      },
      "source": [
        "We have defined both the encoder and the decoder, and we are ready to train the model. We now build the model, which will have two inputs: the image `x`  for the encoder; and the sample `eps` (which refers to the sample from $\\mathcal{N}(0, 1)$ we mentioned before) for the decoder, which will use $\\mu_x, \\sigma_x$ via the reparametrisation trick. The code below will put together all the previous parts as a single class and compile."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VAE Model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=2):\n",
        "        super(VAE, self).__init__()\n",
        "        # Define latent dimension\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Compute μ and log(σ²)\n",
        "        self.z_mu = nn.Linear(128*7*7, latent_dim)\n",
        "        self.z_log_var = nn.Linear(128*7*7, latent_dim)\n",
        "        # To Apply KL divergence layer\n",
        "        self.kl_layer = KLDivergenceLayer()\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128*7*7),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Unflatten(1, (128, 7, 7)),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, kernel_size=5, padding=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(64, 1, kernel_size=5, padding=2),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        return self.z_mu(h), self.z_log_var(h)\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        ##### Reparametrisation trick\n",
        "        # Log_var to sigma\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        # Define a separate noise input (which must be provided during training)\n",
        "        eps = torch.randn_like(std)\n",
        "        # Multiply sigma with the external noise and add mu to obtain the latent vector\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        mu, log_var = self.encode(x)\n",
        "        mu, log_var = self.kl_layer(mu, log_var)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return self.decode(z), mu, log_var, self.kl_layer.kl_loss"
      ],
      "metadata": {
        "id": "djjsTvVUVCfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqOeBPW6c3sj"
      },
      "outputs": [],
      "source": [
        "# Training setup\n",
        "latent_dim = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VAE(latent_dim).to(device)\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhZfdpBBrl2O"
      },
      "source": [
        "Now we train the model for some epochs to see if we can model our data. After the training process we will use this trained VAE to generate new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZd94z0Fc6WT"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "# Early stopping implementation\n",
        "best_loss = float('inf')\n",
        "patience = 3\n",
        "no_improve_epochs = 0\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        recon_batch, mu, log_var, kl_loss = model(data)\n",
        "        nll_loss = nll(data.view(-1, 784), recon_batch).mean()\n",
        "        loss = nll_loss + kl_loss\n",
        "\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, log_var, kl_loss = model(data)\n",
        "            nll_loss = nll(data.view(-1, 784), recon_batch).mean()\n",
        "            val_loss += (nll_loss + kl_loss).item()\n",
        "\n",
        "    val_loss /= len(test_loader.dataset)\n",
        "    print(f'Epoch: {epoch}, Train Loss: {train_loss / len(train_loader.dataset):.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        no_improve_epochs = 0\n",
        "        torch.save(model.state_dict(), 'best_vae_model.pth')\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "        if no_improve_epochs >= patience:\n",
        "            print(f'Early stopping after {epoch} epochs')\n",
        "            model.load_state_dict(torch.load('best_vae_model.pth'))\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byac9m8MtpnI"
      },
      "source": [
        "Assuming we input into the decoder $\\mu_x$ obtained after encoding the image with the encoder, we can retrieve the MSE in the test set using the following piece of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsM1nhLPGwbw"
      },
      "outputs": [],
      "source": [
        "# Calculate MSE on test set\n",
        "model.eval()\n",
        "total_mse = 0\n",
        "with torch.no_grad():\n",
        "    for data, _ in test_loader:\n",
        "        data = data.to(device)\n",
        "        recon_batch, mu, _, _ = model(data)\n",
        "        mse = F.mse_loss(recon_batch, data.view(-1, 784), reduction='sum')\n",
        "        total_mse += mse.item()\n",
        "\n",
        "print(f\"The MSE is: {total_mse / len(test_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz5MmFMzD48G"
      },
      "source": [
        "We trained an encoder to model $Q(z|x)$ which outputs two parameters per latent dimension for each image $x$, which are  $\\mu_x, \\sigma_x$. We now plot the distribution of the $\\mu_x$ when encoding the different images from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4H3u0n9dBLc"
      },
      "outputs": [],
      "source": [
        "# Plot latent space distribution\n",
        "model.eval()\n",
        "latent_vectors = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data = data.to(device)\n",
        "        mu, _ = model.encode(data)\n",
        "        latent_vectors.append(mu.cpu())\n",
        "        labels.append(target.cpu())\n",
        "\n",
        "z_test = torch.cat(latent_vectors, dim=0).numpy()\n",
        "y_test = torch.cat(labels, dim=0).numpy()\n",
        "\n",
        "# Display a 2D plot of the digit classes in the latent space\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "cm = plt.get_cmap('gist_rainbow')\n",
        "ax.set_prop_cycle(color=[cm(1.*i/(10)) for i in range(10)])\n",
        "for l in range(10):\n",
        "    # Only select indices for corresponding label\n",
        "    ind = y_test == l\n",
        "    ax.scatter(z_test[ind, 0], z_test[ind, 1], label=str(l), s=10)\n",
        "ax.legend()\n",
        "plt.title(\"Latent distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwjzHuHIEiMF"
      },
      "source": [
        "The distribution of the encoded means shows how they are clustered by class too, as in the autoencoder case. However, in this case, the distribution also follows a kind of circular distribution around the centre due to the Kullback Leibler divergence term in the loss. We made the $\\mu_x$ output by the encoder to be close to zero, and $\\sigma_x$ to be close to 1.\n",
        "\n",
        "Now let's start with the generation of data, which is the main reason we trained this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyplRgt6e3Lm"
      },
      "outputs": [],
      "source": [
        "# Display a 2D manifold of the digits\n",
        "n = 15  # figure with 15x15 digits\n",
        "digit_size = 28\n",
        "\n",
        "# Linearly spaced coordinates transformed through inverse CDF of Gaussian\n",
        "z1 = stats.norm.ppf(np.linspace(0.01, 0.99, n))\n",
        "z2 = stats.norm.ppf(np.linspace(0.01, 0.99, n))\n",
        "z_grid = np.dstack(np.meshgrid(z1, z2))\n",
        "\n",
        "# Generate images from grid\n",
        "with torch.no_grad():\n",
        "    z_tensor = torch.FloatTensor(z_grid.reshape(n*n, latent_dim)).to(device)\n",
        "    x_pred = model.decoder(z_tensor).cpu().numpy()\n",
        "\n",
        "x_pred_grid = x_pred.reshape(n, n, digit_size, digit_size)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(np.block(list(map(list, x_pred_grid))), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP7Il4dTeTAv"
      },
      "source": [
        "You can see in the image how there is a smooth transition between the different generated numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A36-s7n3tBpv"
      },
      "source": [
        "Now let's generate a nice animation for the latent variable, where we show the point we used in $z$ to generate the data and the corresponding image generated."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We create a 2d array\n",
        "# Number of points to use, increase it for smoother animation\n",
        "# Using more points makes the function slower\n",
        "n_points = 50\n",
        "# theta from 0 to 2pi\n",
        "theta = np.linspace(0, 2*np.pi, n_points)\n",
        "# radius of the circle (change it depending on your reprentation space plot)\n",
        "r = 1\n",
        "# compute x and y (you can add an offset depending on your latent space)\n",
        "offset_x = 0\n",
        "offset_y = 0\n",
        "x = r*np.cos(theta) + offset_x\n",
        "y = r*np.sin(theta) + offset_y\n",
        "latent = np.stack([x, y], -1)\n",
        "\n",
        "## We now plot as before the 2d scatter with the images from the test set\n",
        "## and the corresponding label\n",
        "# Get test latent vectors\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_test = []\n",
        "    for data, _ in test_loader:\n",
        "        mu, _ = model.encode(data.to(device))\n",
        "        z_test.append(mu.cpu())\n",
        "    z_test = torch.cat(z_test).numpy()\n",
        "\n",
        "# Create figure\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# First subplot: latent space with test points\n",
        "ax[0].set_prop_cycle(color=[plt.cm.gist_rainbow(1.*i/10) for i in range(10)])\n",
        "for l in range(10):\n",
        "    ind = y_test == l\n",
        "    ax[0].scatter(z_test[ind, 0], z_test[ind, 1], label=str(l), s=10)\n",
        "ax[0].legend()\n",
        "scat = ax[0].scatter(latent[0,0], latent[0,1], s=200, c='k')\n",
        "\n",
        "# Second subplot: generated image\n",
        "with torch.no_grad():\n",
        "    latent_tensor = torch.FloatTensor(latent).to(device)\n",
        "    latent_im = model.decoder(latent_tensor).cpu().numpy()\n",
        "\n",
        "im = ax[1].imshow(latent_im[0].reshape(28, 28), cmap='gray')\n",
        "ax[1].axis('off')\n",
        "\n",
        "# Animation update function\n",
        "def updatefig(i):\n",
        "    scat.set_offsets(latent[i])\n",
        "    im.set_array(latent_im[i].reshape(28, 28))\n",
        "    return im,\n",
        "\n",
        "# Create animation\n",
        "anim = animation.FuncAnimation(fig, updatefig, frames=n_points, interval=200, blit=True)\n",
        "plt.close()\n",
        "HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "pnobt5GeY-7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hc-XviqI44H"
      },
      "source": [
        "# Generative Adversarial Networks\n",
        "\n",
        "\n",
        "\n",
        "![](http://2018.igem.org/wiki/images/4/48/T--Vilnius-Lithuania-OG--introduction1.png)\n",
        "[Image source](https://2018.igem.org/Team:Vilnius-Lithuania-OG/Gan_Introduction)\n",
        "\n",
        "Generative Adversarial Networks have been shown to improve the generation of data compared to approaches such as VAE.\n",
        "\n",
        "As you learnt in the lecture, we have two networks playing what is called a min max game between them. The Generator, $G$, tries to generate data that looks similar to real data, whereas the discriminator $D$ tries to distinguish between real and fake data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZJHL5nGTM-M"
      },
      "source": [
        "We first define $G$ here, and in this case we will use a convolutional network. Notice that we use what is called LeakyReLU for the activation functions, which have been shown to work well when using GANs. $G$ will take a vector of noise (in this case of dimensionality 10) sampled from $\\mathcal{N}(0, 1)$ and output a generated image. We use `tanh` as the last activation because the data will be normalized to be between $[-1, 1]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvCu8786qNK1"
      },
      "outputs": [],
      "source": [
        "# Build Generator model\n",
        "randomDim = 5\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is random noise vector of size randomDim\n",
        "            nn.Linear(randomDim, 128 * 7 * 7),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Unflatten(1, (128, 7, 7)),  # Reshape to (128, 7, 7)\n",
        "\n",
        "            # Upsample to (128, 14, 14)\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, kernel_size=5, padding=2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Upsample to (64, 28, 28)\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(64, 1, kernel_size=5, padding=2),\n",
        "            nn.Tanh()  # Output in range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Create generator and optimizer\n",
        "generator = Generator()\n",
        "optimizer_gen = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "# Print model summary\n",
        "print(generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x2J0DyBNRW0"
      },
      "source": [
        "Here, we define the discriminator $D$. The discriminator is trained with the `categorical_crossentropy` loss, but you could also use a binary loss, as its job is to discriminate between real and fake data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8Ds7WLJqUGw"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input: 1x28x28 image\n",
        "            nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            # 64x14x14\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            # 128x7x7\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 7 * 7, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Create discriminator and optimizer\n",
        "discriminator = Discriminator()\n",
        "optimizer_disc = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "# Print model summary\n",
        "print(discriminator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCmi_IDwNasg"
      },
      "source": [
        "We defined the combined network, which combines the generator and discriminator in a network. The discriminator, however, will not be updated when using this combined network. We will explain later why, but basically we will use the `discriminator` object (not the combined network) whenever we need to update the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77v6wY77qupP"
      },
      "outputs": [],
      "source": [
        "class GAN(nn.Module):\n",
        "    def __init__(self, generator, discriminator):\n",
        "        super(GAN, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "        # Freeze discriminator parameters\n",
        "        for param in self.discriminator.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"Forward pass through generator then discriminator\"\"\"\n",
        "        generated_images = self.generator(z)\n",
        "        return self.discriminator(generated_images)\n",
        "\n",
        "# Create combined GAN model\n",
        "gan = GAN(generator, discriminator)\n",
        "optimizer_gan = optim.Adam(gan.generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "# Print model summary\n",
        "print(gan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQchMjunN0PJ"
      },
      "source": [
        "Now, let's define some helper functions that will be used to plot the loss and some generated images during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3kiJsWU2JXM"
      },
      "outputs": [],
      "source": [
        "def plot_loss(losses):\n",
        "    \"\"\"Plot discriminator and generator loss during training\"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(losses[\"d\"], label='Discriminator Loss')\n",
        "    plt.plot(losses[\"g\"], label='Generator Loss')\n",
        "    plt.title(\"Training Losses\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_gen(n_ex=16, dim=(4,4), figsize=(10,10)):\n",
        "    \"\"\"Generate and plot images from the generator\"\"\"\n",
        "    # Generate random noise\n",
        "    noise = torch.randn(n_ex, randomDim, device=device)\n",
        "\n",
        "    # Generate images\n",
        "    with torch.no_grad():\n",
        "        generated_images = generator(noise).cpu().numpy()\n",
        "\n",
        "    # Reshape and plot\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(n_ex):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        img = generated_images[i].reshape(28, 28)  # Reshape to 28x28\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('./images.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "razKxofMN60c"
      },
      "source": [
        "We will not use the regular fit method to train the model as in the usual approach. The reason for this is that the min max game that the GANs do, in practice is implemented in each of the batches by using two different training steps.\n",
        "* **First step - Training the discriminator:** In this step only the discriminator is trained.  The generator will output some fake images using noise as input. Then, we will give the discriminator these generated images and some images sampled from the real dataset and train it to distinguish between the two of them.\n",
        "\n",
        "* **Second step - Updating the generator**: We use the generator to output fake images again, and the discriminator will try to guess if these newly generated images are real or fake. However, the aim in this second step is not to update the discriminator, only the generator. This is why in the combined network we made the discriminator not to be trainable. The discriminator is used to pass information (in the form of gradients in this case) to update the generator. Hence, the generator will try to change its weights to make the discriminator think the data comes from the real distribution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up output widgets\n",
        "progress_out = widgets.Output()\n",
        "plot_out = widgets.Output()\n",
        "display(progress_out)\n",
        "display(plot_out)\n",
        "\n",
        "# Initialize variables\n",
        "losses = {\"d\": [], \"g\": []}\n",
        "total_iter = 0\n",
        "saved_images = []\n",
        "saved_iterations = []\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move models to device\n",
        "generator = generator.to(device)\n",
        "discriminator = discriminator.to(device)\n",
        "gan = GAN(generator, discriminator).to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "def update_plots(iteration):\n",
        "    global saved_images, saved_iterations\n",
        "    with plot_out:\n",
        "        plot_out.clear_output(wait=True)\n",
        "        plot_gen()\n",
        "        # Read the saved image file and append the image data\n",
        "        img_data = mpimg.imread('images.png')\n",
        "        saved_images.append(img_data)\n",
        "        saved_iterations.append(iteration)\n",
        "\n",
        "        # Display the last saved image\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "        ax.imshow(img_data)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'Iteration: {iteration}')\n",
        "        plt.tight_layout()\n",
        "        display(fig)\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "def train_epoch(epoch_number, plt_frq=200):\n",
        "    global total_iter\n",
        "\n",
        "    with progress_out:\n",
        "        # Initialize progress bar outside the loop\n",
        "        pbar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch_number}\", leave=True)\n",
        "\n",
        "        for i, (real_images, _) in enumerate(train_loader):\n",
        "            total_iter += 1\n",
        "            batch_size = real_images.size(0)\n",
        "\n",
        "            # Move to device\n",
        "            real_images = real_images.to(device)\n",
        "\n",
        "            # Create labels\n",
        "            real_labels = torch.ones(batch_size, 1, device=device)\n",
        "            fake_labels = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            # Enable discriminator gradients\n",
        "            for param in discriminator.parameters():\n",
        "                param.requires_grad = True\n",
        "            discriminator.zero_grad()\n",
        "\n",
        "            # Train with real images\n",
        "            outputs_real = discriminator(real_images)\n",
        "            d_loss_real = criterion(outputs_real, real_labels)\n",
        "\n",
        "            # Train with fake images\n",
        "            noise = torch.randn(batch_size, randomDim, device=device)\n",
        "            fake_images = generator(noise).detach()\n",
        "            outputs_fake = discriminator(fake_images)\n",
        "            d_loss_fake = criterion(outputs_fake, fake_labels)\n",
        "\n",
        "            # Total discriminator loss\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "            d_loss.backward()\n",
        "            optimizer_disc.step()\n",
        "            losses[\"d\"].append(d_loss.item())\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "            # Disable discriminator gradients\n",
        "            for param in discriminator.parameters():\n",
        "                param.requires_grad = False\n",
        "            generator.zero_grad()\n",
        "\n",
        "            # Generate fake images\n",
        "            noise = torch.randn(batch_size, randomDim, device=device)\n",
        "            outputs = gan(noise)\n",
        "\n",
        "            # Generator tries to fool discriminator\n",
        "            g_loss = criterion(outputs, real_labels)\n",
        "            g_loss.backward()\n",
        "            optimizer_gan.step()\n",
        "            losses[\"g\"].append(g_loss.item())\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'd_loss': f\"{d_loss.item():.4f}\",\n",
        "                'g_loss': f\"{g_loss.item():.4f}\"\n",
        "            })\n",
        "            pbar.update(1)\n",
        "\n",
        "            if i % plt_frq == 0:\n",
        "                update_plots(total_iter)\n",
        "\n",
        "        # Close progress bar at end of epoch\n",
        "        pbar.close()"
      ],
      "metadata": {
        "id": "qVzi97dTgwg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3EgAKi5IwIa"
      },
      "source": [
        "Let's start the training process. We will use MNIST again. A standard practice for GANs is to normalize images to the range $[-1, 1]$, which we do. We train it for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "n_epoch = 10\n",
        "for e in range(n_epoch):\n",
        "    train_epoch(e+1)"
      ],
      "metadata": {
        "id": "9Cl3vk_HiPrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot final losses\n",
        "plot_loss(losses)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "img = mpimg.imread('loss.png')\n",
        "ax.imshow(img)\n",
        "ax.axis('off')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "LxE_JEUdbwjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create animation\n",
        "fig_anim, ax_anim = plt.subplots(figsize=(5, 5))\n",
        "# Use the first saved image data directly\n",
        "im_anim = ax_anim.imshow(saved_images[0])\n",
        "ax_anim.axis('off')\n",
        "title_text = ax_anim.set_title(f'Iteration: {saved_iterations[0]}')\n",
        "\n",
        "def update_frame(i):\n",
        "    # Use the saved image data for updating the frame\n",
        "    im_anim.set_array(saved_images[i])\n",
        "    title_text.set_text(f'Iteration: {saved_iterations[i]}')\n",
        "    return im_anim, title_text\n",
        "\n",
        "anim = animation.FuncAnimation(fig_anim, update_frame, frames=len(saved_images),\n",
        "                              interval=500, blit=False, repeat=False)\n",
        "\n",
        "# Display animation\n",
        "HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "W2lZHbPGbwfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQPlGIFoI2Pv"
      },
      "source": [
        "The plots show how the training loss for both the generator and the discriminator does not change much. Usually we have a model which we optimize to reduce some metric/loss we pass it. However, in this case we have two models which are 'competing' against each other, so their losses are approximately stable. Now let's plot some results, as in the VAE case, we plot the results when sampling from 2 of the dimensions of the input noise $z$. However, as in this case we used more than 2 dimensions for $z$, the results vary due to choosing at random the 2 dimensions where to sample from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGTD49suQD5F"
      },
      "outputs": [],
      "source": [
        "# display a 2D manifold of the digits\n",
        "n = 15  # figure with 15x15 digits\n",
        "digit_size = 28\n",
        "\n",
        "# linearly spaced coordinates on the unit square were transformed\n",
        "# through the inverse CDF (ppf) of the Gaussian to produce values\n",
        "# of the latent variables z, since the prior of the latent space\n",
        "# is Gaussian\n",
        "z1 = stats.norm.ppf(np.linspace(0.01, 0.99, n))\n",
        "z2 = stats.norm.ppf(np.linspace(0.01, 0.99, n))\n",
        "z_grid = np.dstack(np.meshgrid(z1, z2))\n",
        "z_grid = z_grid.astype(np.float32)\n",
        "\n",
        "# Create random values for the remaining dimensions\n",
        "z_fill = np.random.normal(0, 1, size=[randomDim-2])\n",
        "z_fill = np.expand_dims(z_fill, 0)\n",
        "z_fill = np.expand_dims(z_fill, 0)\n",
        "z_fill = np.repeat(z_fill, n, 0)\n",
        "z_fill = np.repeat(z_fill, n, 1)\n",
        "\n",
        "# Combine the grid coordinates with random values for other dimensions\n",
        "z_grid = np.concatenate([z_grid, z_fill], -1)\n",
        "np.random.shuffle(z_grid[:,:,-1])  # Shuffle one dimension for more variation\n",
        "\n",
        "# Generate images from the grid points\n",
        "with torch.no_grad():\n",
        "    # Convert numpy array to PyTorch tensor and move to device\n",
        "    z_tensor = torch.from_numpy(z_grid.reshape(n*n, randomDim)).float().to(device)\n",
        "    # Generate images\n",
        "    x_pred = generator(z_tensor).cpu().numpy()\n",
        "\n",
        "# Reshape the predictions into a grid\n",
        "x_pred_grid = np.reshape(x_pred, (n, n, digit_size, digit_size))\n",
        "\n",
        "# Display the grid of generated digits\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(np.block(list(map(list, x_pred_grid))), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObF7kxiDR06j"
      },
      "source": [
        "**Training instability**\n",
        "\n",
        "GANs are quite difficult to train as there are several factors that can hurt their performance. Some of the most frequent issues are:\n",
        "\n",
        "* [Mode collapsing](https://www.youtube.com/watch?v=ktxhiKhWoEE): The generator is not capable of creating diverse images, it only generates a limited set of images.\n",
        "* Discriminator loss decreases quickly: in some cases the discriminator may be too powerful, so it quickly learns at the beginning which images are fake and which real, leading to small gradients passed to the generator.\n",
        "* Hyper parameter sensitivity\n",
        "* Non-convergence\n",
        "\n",
        "There are some tricks that have been shown to improve convergence, some of them are shared in this [repo](https://github.com/soumith/ganhacks).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENoiS-6xSYfy"
      },
      "source": [
        "# Inception Score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9bXrKeu6ZG_"
      },
      "source": [
        "The evaluation of a generative model quantitatively is a problem that is being research. The metric should evaluate the coverage, i.e. how diverse the generated data is, and sample quality, which is related to the visual quality of the sample. A widely used metric is the Inception Score, which is defined as:\n",
        "$$\n",
        "\\text{IS}(G)= \\exp(\\mathbb{E}_{x\\sim p_a}\\ D_{KL}(p(y|x)||p(y)))\n",
        "$$\n",
        "Let's explain the term a little bit. We aim to have a high IS, which means that we want a high $D_{KL}$ between $p(y|x)$ and $p(y)$. Ideally, what we want is $p(y|x)$ to be \"peaky\" and $p(y)$ to be uniform. $p(y|x)$ is the predicted probability vector $y$ given a generated sample $x$ by a model, in this case an Inception model (hence, the name of the score). If the sample $x$ is of high quality, the model should classify it with confidence in one of the available classes, hence the \"peaky\" $p(y|x)$. The term $p(y)$ is related to the diversity of the data, and is the marginal probability computed as $\\int_z p(y|x=G(z))dz$, hence we average the predicted probabilites of the Inception model for the given samples. If the data is diverse, we will see approximately obtain a flat $p(y)$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Define the classifier architecture matching the Keras model\n",
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTClassifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # Conv2d_1\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Conv2d_2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Conv2d_3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Flatten and dense\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128*7*7, 10),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Initialize model\n",
        "model = MNISTClassifier()\n",
        "print(model)\n",
        "\n",
        "# Training setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Data loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# Test function\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({accuracy:.0f}%)\\n')\n",
        "    return accuracy\n",
        "\n",
        "# Training loop\n",
        "best_accuracy = 0\n",
        "for epoch in range(1, 11):  # 10 epochs\n",
        "    train(epoch)\n",
        "    current_accuracy = test()\n",
        "\n",
        "    # Save best model\n",
        "    if current_accuracy > best_accuracy:\n",
        "        best_accuracy = current_accuracy\n",
        "        torch.save(model.state_dict(), 'mnist_classifier_best.pth')\n",
        "        print(\"Saved new best model\")\n",
        "\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), 'mnist_classifier_final.pth')\n",
        "print(\"Training complete. Models saved.\")"
      ],
      "metadata": {
        "id": "7YECp5erAG0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved model from file\n",
        "model.load_state_dict(torch.load('mnist_classifier_best.pth'))"
      ],
      "metadata": {
        "id": "otbcnAnAWqQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inception_score(x, batch_size=32, denorm_im=True, device='cuda'):\n",
        "    # Load the trained model\n",
        "    model = MNISTClassifier().to(device)\n",
        "    model.load_state_dict(torch.load('mnist_classifier_best.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    n_batches = (x.shape[0] + batch_size - 1) // batch_size\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_batches):\n",
        "            batch = x[i*batch_size:(i+1)*batch_size]\n",
        "            batch = torch.from_numpy(batch).float().to(device)\n",
        "            if batch.ndim == 3:\n",
        "                batch = batch.unsqueeze(1)\n",
        "            elif batch.shape[-1] == 1:\n",
        "                batch = batch.permute(0, 3, 1, 2)\n",
        "            if denorm_im:\n",
        "                batch = (batch + 1) / 2  # Scale from [-1,1] to [0,1]\n",
        "            logits = model(batch)\n",
        "            preds.append(logits.cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "\n",
        "    # Calculate p(y)\n",
        "    p_y = np.mean(preds, axis=0)\n",
        "    # Calculate KL divergence\n",
        "    # p(y|x)log(P(y|x)/P(y))\n",
        "    kl_div = preds * (np.log(preds + 1e-16) - np.log(p_y + 1e-16))  # Added epsilon for numerical stability\n",
        "    # KL(x) = Σ_y p(y|x)log(P(y|x)/P(y))\n",
        "    kl_div = np.sum(kl_div, axis=1)\n",
        "    # Calculate mean KL divergence\n",
        "    avg_kl_div = np.mean(kl_div)\n",
        "    # Calculate inception score\n",
        "    is_score = np.exp(avg_kl_div)\n",
        "    return is_score\n",
        "\n",
        "def image_inception_score(generator, n_ex=10000, dim_random=10, input_noise=None, denorm_im=True, device='cuda'):\n",
        "    generator.eval()\n",
        "    if input_noise is None:\n",
        "        input_noise = torch.randn(n_ex, dim_random, device=device)\n",
        "    else:\n",
        "        input_noise = torch.from_numpy(input_noise).float().to(device)\n",
        "    # Generate images\n",
        "    with torch.no_grad():\n",
        "        x_pred = generator(input_noise)\n",
        "        # Convert to numpy and reshape if needed\n",
        "        x_pred = x_pred.cpu().numpy()\n",
        "        if len(x_pred.shape) == 2:  # Flattened images\n",
        "            x_pred = x_pred.reshape(n_ex, 28, 28, 1)\n",
        "        elif x_pred.shape[1] == 1:  # NCHW format\n",
        "            x_pred = x_pred.transpose(0, 2, 3, 1)  # Convert to NHWC\n",
        "    return inception_score(x_pred, batch_size=128, denorm_im=denorm_im, device=device)"
      ],
      "metadata": {
        "id": "lQfcZbDHAIBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHTkEqtOHo2D"
      },
      "source": [
        "Now let's check the Inception Score for the GAN model we have trained before."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Inception Score for the trained GAN generator\n",
        "is_score = image_inception_score(generator,\n",
        "                               dim_random=randomDim,\n",
        "                               denorm_im=True)\n",
        "\n",
        "print(f\"Inception Score: {is_score:.4f}\")"
      ],
      "metadata": {
        "id": "z1UDZFIu_9cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDwVU-M3sAkN"
      },
      "source": [
        "Due to the way we defined the generator for the GAN, images are generated in the range $[-1, 1]$ as we use a `tanh` activation function. That is why we use the `denorm_im` variable to map them to the range $[0, 1]$.\n",
        "\n",
        "The decoder of the VAE can also be evaluated using the same `image_inception_score` function. A VAE uses a prior $\\mathcal{N}(0,1)$ as an extra constraint in the loss, hence if we sample from $\\mathcal{N}(0,1)$ as done in `image_inception_score`, the decoder should be able to generate new samples. When using the function for the VAE, use `image_inception_score(..., denorm_im=0)` as the decoder in the VAE already uses the range $[0,1]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCVwckWQLHno"
      },
      "source": [
        "# Conditional Generative Adversarial Networks (cGANs)\n",
        "\n",
        "A Conditional GAN (cGAN) is a variant of classical Generative Adversarial Networks, where the task is conditioned on some extra information instead of random noise. They were introduced in 2014, [link to the cGANs paper](https://arxiv.org/abs/1411.1784), and set a new state of the art in many image tasks.\n",
        "\n",
        "In this setting, cGANs expect an input image instead of a random noise vector. A simple example of image translation is the drawing to real object transformation. To do so, imagine that we have two different paired domains, images of the real objects (domain A) and drawing of objects (domain B), and we want to go from one to the other. In classical GANs, we used these two domains for training the architectures, but here we can also condition the input to one of the images in the domain B, and use the relationship between the domains to guide the network to the desired result:\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/rF2w5Tt/Screenshot-from-2019-02-25-15-47-18.png\" alt=\"Screenshot-from-2019-02-25-15-47-18\" border=\"0\"></a>\n",
        "\n",
        "As in regular GANs, the discriminator learns to classify between fake (synthesized by the generator) and real {drawing, photo} tuples. Hence, the generator will try to fool the discriminator with better and better synthesised images. In contrast to GANS, as we feed the drawing directly to the generator, the information that the network can use to seek convergence is much wider."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B3XO3spH0ou"
      },
      "source": [
        "## Image-to-image Translation with cGANs\n",
        "\n",
        "\n",
        "[Image-to-image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004) is a paper presented at the Computer Vision and Pattern Recognition (CVPR) conference in 2017. In their paper, the authors introduced Pix2pix, a conditional GAN, to transform images from one domain to another. The authors in the paper investigated conditional adversarial networks as a general-purpose solution to image-to-image translation problems, see some examples from the paper of different Image-to-Image tasks:\n",
        "\n",
        ">\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/Mf08787/examples.jpg)\n",
        "\n",
        "In this tutorial, we will explain and implement Pix2pix model for colouring images step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrEuqkIINKxi"
      },
      "source": [
        "## Colouring Black and White Images\n",
        "The tutorial aims to build the Pix2pix code for colouring images. We will use CIFAR10 dataset to perform the experiment, however, you could try to use any other dataset. For instance, check [this repo](https://github.com/kvfrans/deepcolor) which shows how to colour manga-style images.\n",
        "\n",
        "The idea is quite intuitive, first of all, we will transform images into black and white (BW) and train a generator that will transform them back to RGB images. As in classical GANs, we use the discriminator to differentiate between *Fake* and *Real* images.\n",
        "\n",
        "Thus, we have domain A (colour) and domain B (B&W)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zaEwTyYXcrj"
      },
      "source": [
        "## Dataset Generator\n",
        "\n",
        "When training a cGAN, or any regular GAN architecture, we need to train independently the generator and discriminator networks. As seen, the training is a min-max game between the two nets.\n",
        "\n",
        "Firstly, we aim to train the generator to be able to fool the discriminator, while in the next iteration, we will train the discriminator to be able to distinguish between the generated (fake) images and the real ones.\n",
        "\n",
        "We are going to switch the training between networks in every step. Instead of training per a whole epoch as has been seen in the previous tutorials, we will generate batches and train first the discriminator and then switch to train the generator.\n",
        "\n",
        "Let's define our class *DataLoader()*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r7Y-RLnTlqw"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, dataset_name, img_res=(32, 32)):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.img_res = img_res\n",
        "        self.load_dataset()\n",
        "        self.convert_to_bw = None  # Will be set later\n",
        "        self._start_epoch()\n",
        "\n",
        "    def _start_epoch(self):\n",
        "        # Shuffle training data at start of each epoch\n",
        "        self.indices = np.random.permutation(len(self.im_A_train))\n",
        "\n",
        "    def load_dataset(self):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1] range\n",
        "        ])\n",
        "\n",
        "        if self.dataset_name == 'CIFAR10':\n",
        "            train_set = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
        "            test_set = datasets.CIFAR10('./data', train=False, download=True, transform=transform)\n",
        "        elif self.dataset_name == 'CIFAR100':\n",
        "            train_set = datasets.CIFAR100('./data', train=True, download=True, transform=transform)\n",
        "            test_set = datasets.CIFAR100('./data', train=False, download=True, transform=transform)\n",
        "        else:\n",
        "            raise Exception('Please select a valid dataset')\n",
        "\n",
        "        # Store as numpy arrays for compatibility with original interface\n",
        "        self.im_A_train = train_set.data\n",
        "        self.im_A_test = test_set.data\n",
        "        self.y_train = np.array(train_set.targets)\n",
        "        self.y_test = np.array(test_set.targets)\n",
        "\n",
        "        # Convert to [0,1] range to match original implementation\n",
        "        self.im_A_train = self.im_A_train.astype('float32') / 255.0\n",
        "        self.im_A_test = self.im_A_test.astype('float32') / 255.0\n",
        "\n",
        "    def get_dataset_shape(self, is_training=True):\n",
        "        if is_training:\n",
        "            return self.im_A_train.shape\n",
        "        else:\n",
        "            return self.im_A_test.shape\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return int(np.ceil(len(self.im_A_train) / batch_size))\n",
        "\n",
        "    def set_image_transformations(self, convert_to_bw):\n",
        "        self.convert_to_bw = convert_to_bw\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_training=True):\n",
        "        if is_training:\n",
        "            data = self.im_A_train\n",
        "            indices = self.indices\n",
        "        else:\n",
        "            data = self.im_A_test\n",
        "            indices = np.arange(len(data))\n",
        "\n",
        "        num_batches = self.get_num_batches(batch_size) if is_training else int(np.ceil(len(data) / batch_size))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = (i + 1) * batch_size\n",
        "            batch_indices = indices[start_idx:end_idx]\n",
        "\n",
        "            batch = data[batch_indices]\n",
        "            batch = torch.from_numpy(batch).permute(0, 3, 1, 2)  # NHWC to NCHW\n",
        "\n",
        "            if self.convert_to_bw:\n",
        "                bw_batch = self.convert_to_bw(batch)\n",
        "                yield batch, bw_batch\n",
        "            else:\n",
        "                yield batch, None\n",
        "\n",
        "    def get_random_batch(self, batch_size=1, is_training=True):\n",
        "        if is_training:\n",
        "            data = self.im_A_train\n",
        "            indices = np.random.choice(len(data), batch_size, replace=False)\n",
        "        else:\n",
        "            data = self.im_A_test\n",
        "            indices = np.random.choice(len(data), batch_size, replace=False)\n",
        "\n",
        "        batch = data[indices]\n",
        "        batch = torch.from_numpy(batch).permute(0, 3, 1, 2)  # NHWC to NCHW\n",
        "\n",
        "        if self.convert_to_bw:\n",
        "            bw_batch = self.convert_to_bw(batch)\n",
        "            return batch, bw_batch\n",
        "        else:\n",
        "            return batch, None\n",
        "\n",
        "class BWConverter:\n",
        "    def predict(self, batch):\n",
        "        if isinstance(batch, torch.Tensor):\n",
        "            # Convert RGB to grayscale using standard weights\n",
        "            return 0.2989 * batch[:,0] + 0.5870 * batch[:,1] + 0.1140 * batch[:,2]\n",
        "        else:\n",
        "            return np.dot(batch[...,:3], [0.2989, 0.5870, 0.1140])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDfgTUXZZM_G"
      },
      "source": [
        "*DataLoader* class needs only the name of the dataset to load. Now, it can load CIFAR10 and CIFAR100. Let's see what some of *DataLoader*'s methods do:\n",
        "\n",
        "\n",
        "*   *_start_epoch()* is a private method. It shuffles the dataset every time that *load_batch()* is called.\n",
        "*   *load_dataset()* loads the dataset into *DataLoader* object.\n",
        "*  *load_batch()* creates a python generator. In each iteration, it will return a batch from the shuffled dataset.\n",
        "\n",
        "Let's now define the dataset class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMe042E2cDH7"
      },
      "outputs": [],
      "source": [
        "# Load the data, shuffled and split between train and test sets\n",
        "dataset_loader = DataLoader(dataset_name='CIFAR10')\n",
        "dataset_loader.set_image_transformations(BWConverter())\n",
        "\n",
        "training_shape = dataset_loader.get_dataset_shape()\n",
        "test_shape = dataset_loader.get_dataset_shape(is_training=False)\n",
        "\n",
        "print('Shape of Training Images:', training_shape)\n",
        "print('Shape of Test Images:', test_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGKwoeF2UZ5U"
      },
      "source": [
        "## Defining the Domains\n",
        "\n",
        "As explained above, we will go from B&W to RGB colour images. To get the B&W images, we need to define *transform_bw*, a method that will transform images from **domain A** (colour) to images in **domain B** (B&W). We can do that with a custom transform class as seen below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to grayscale color space\n",
        "class BWTransform(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch, 3, H, W) in [-1,1] range\n",
        "        # Using standard RGB to grayscale conversion weights\n",
        "        grayscale = 0.2989 * x[:,0] + 0.5870 * x[:,1] + 0.1140 * x[:,2]\n",
        "        return grayscale.unsqueeze(1)  # Add channel dim back\n",
        "\n",
        "transform_bw = BWTransform()\n",
        "\n",
        "# Set predefined transformation in DataLoader Class\n",
        "dataset_loader.set_image_transformations(transform_bw)"
      ],
      "metadata": {
        "id": "yPWpvjbNIeXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7JgJGacphw"
      },
      "source": [
        "We can visualise images from both domains before starting to code our cGAN. Rerun the following code to check different examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7Of7_EXLHSH"
      },
      "outputs": [],
      "source": [
        "# Load random batch from dataset\n",
        "random_batch = dataset_loader.get_random_batch(batch_size=9)\n",
        "color_images, bw_images = random_batch\n",
        "\n",
        "# Convert tensors to numpy and permute dimensions for visualization\n",
        "color_images = color_images.permute(0, 2, 3, 1).cpu().numpy()  # NCHW -> NHWC\n",
        "bw_images = bw_images.permute(0, 2, 3, 1).cpu().numpy()  # NCHW -> NHWC\n",
        "\n",
        "# Denormalize from [-1,1] to [0,1] range\n",
        "color_images = (color_images + 1) / 2\n",
        "bw_images = (bw_images + 1) / 2\n",
        "\n",
        "# Repeat grayscale channel for visualization\n",
        "bw_images_rgb = np.repeat(bw_images, 3, axis=3)\n",
        "\n",
        "# Plot comparison\n",
        "N = 3\n",
        "fig, axes = plt.subplots(N, N, figsize=(10, 10))\n",
        "plt.suptitle('Domain A (Color) VS Domain B (B&W)', fontsize=18)\n",
        "for row in range(N):\n",
        "    for col in range(N):\n",
        "        idx = row * N + col\n",
        "\n",
        "        # Concatenate color and BW images horizontally\n",
        "        combined = np.concatenate((color_images[idx], bw_images_rgb[idx]), axis=1)\n",
        "\n",
        "        axes[row, col].imshow(np.clip(combined, 0, 1))\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUIdgtbpUg6j"
      },
      "source": [
        "## Generator & Discriminator Models\n",
        "\n",
        "For many image translation problems, there is a great deal of low-level information shared between the input and output. Thus, it is desirable to shuttle this information directly across the net. For example, in the case of image colourization, the input and output share the location of prominent edges. To give the generator a means to circumvent the bottleneck for information like this, we use an architecture with skip connections, following the general shape of the UNet introduced in previous tutorials.\n",
        "\n",
        "The generator introduced in this tutorial is a simpler version of the one in the paper since we are using low-resolution images (32x32).\n",
        "\n",
        "In Pix2pix, the authors found that mixing the GAN objective with a more traditional loss, such as L1 loss, improved the final performance. The discriminator’s job remains unchanged, but the generator is tasked to not only fool the discriminator but also to be near the ground-truth output in an L1 sense.\n",
        "\n",
        "We can define our cGAN generator model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, im_shape=(32, 32)):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.drop4 = nn.Dropout2d(0.5)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "        self.conv5 = nn.Conv2d(512, 1024, 3, padding=1)\n",
        "        self.drop5 = nn.Dropout2d(0.5)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        self.up6 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(1024, 512, 3, padding=1)\n",
        "        )\n",
        "        self.conv6 = nn.Conv2d(1024, 512, 3, padding=1)  # Accounts for concatenation\n",
        "\n",
        "        self.up7 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(512, 256, 3, padding=1)\n",
        "        )\n",
        "        self.conv7 = nn.Conv2d(512, 256, 3, padding=1)\n",
        "\n",
        "        self.up8 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(256, 128, 3, padding=1)\n",
        "        )\n",
        "        self.conv8 = nn.Conv2d(256, 128, 3, padding=1)\n",
        "\n",
        "        self.up9 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, padding=1)\n",
        "        )\n",
        "        self.conv9 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "\n",
        "        self.conv10 = nn.Conv2d(64, 3, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        conv1 = F.relu(self.conv1(x))\n",
        "        pool1 = self.pool1(conv1)\n",
        "        conv2 = F.relu(self.conv2(pool1))\n",
        "        pool2 = self.pool2(conv2)\n",
        "        conv3 = F.relu(self.conv3(pool2))\n",
        "        pool3 = self.pool3(conv3)\n",
        "        conv4 = F.relu(self.conv4(pool3))\n",
        "        drop4 = self.drop4(conv4)\n",
        "        pool4 = self.pool4(drop4)\n",
        "        conv5 = F.relu(self.conv5(pool4))\n",
        "        drop5 = self.drop5(conv5)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        up6 = self.up6(drop5)\n",
        "        merge6 = torch.cat([drop4, up6], dim=1)\n",
        "        conv6 = F.relu(self.conv6(merge6))\n",
        "\n",
        "        up7 = self.up7(conv6)\n",
        "        merge7 = torch.cat([conv3, up7], dim=1)\n",
        "        conv7 = F.relu(self.conv7(merge7))\n",
        "\n",
        "        up8 = self.up8(conv7)\n",
        "        merge8 = torch.cat([conv2, up8], dim=1)\n",
        "        conv8 = F.relu(self.conv8(merge8))\n",
        "\n",
        "        up9 = self.up9(conv8)\n",
        "        merge9 = torch.cat([conv1, up9], dim=1)\n",
        "        conv9 = F.relu(self.conv9(merge9))\n",
        "\n",
        "        conv10 = self.conv10(conv9)\n",
        "\n",
        "        return torch.tanh(conv10)  # Output in [-1, 1] range\n",
        "\n",
        "generator = Generator()"
      ],
      "metadata": {
        "id": "Pq3YbFltJHM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkhWs2Fwztka"
      },
      "source": [
        "Next, we follow the discriminator model definition from the paper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usNum3W7zvar"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, im_shape=(32, 32)):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.im_shape = im_shape\n",
        "\n",
        "        # Define the model architecture\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(4, 64, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.BatchNorm2d(128, momentum=0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
        "            nn.BatchNorm2d(256, momentum=0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256 * (im_shape[0]//8) * (im_shape[1]//8), 1024),\n",
        "            nn.Linear(1024, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, img_A, img_B):\n",
        "        # Concatenate inputs along channel dimension\n",
        "        x = torch.cat((img_A, img_B), dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "discriminator = Discriminator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etn3wYwi8A2k"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "We arrive at the tricky part of the cGAN tutorial, where we need to define the training loop of our model. Do not focus much on this part of the code, this is only provided in case you want to further train the architectures.\n",
        "\n",
        "First, we need to define the optimisers, inputs, outputs, and losses for each one of the networks. Let's go line by line:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizers\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=2e-5, betas=(0.5, 0.999))\n",
        "\n",
        "# Loss functions\n",
        "criterion_mse = nn.MSELoss()  # For adversarial loss\n",
        "criterion_l1 = nn.L1Loss()    # For pixel-wise loss\n",
        "\n",
        "# Input size\n",
        "im_shape = (32, 32)\n",
        "\n",
        "# Move models to device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = generator.to(device)\n",
        "discriminator = discriminator.to(device)"
      ],
      "metadata": {
        "id": "o5P2CVcVKDa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNzo7ijs2THo"
      },
      "source": [
        "We have created the models and compiled them. Besides, before starting the training loop, we are going to define two auxiliary functions that will allow us to print images during training to have an idea of how well the generator is colouring the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpZvidvfSdrC"
      },
      "outputs": [],
      "source": [
        "def showColoredIms(imB, fake_imA, real_imA):\n",
        "    # Convert tensors to numpy and denormalize\n",
        "    imB = imB[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
        "    # Detach fake_imA before converting to numpy\n",
        "    fake_imA = fake_imA[0].detach().cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
        "    real_imA = real_imA[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
        "\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.subplot(131)\n",
        "    plt.imshow(imB[:,:,0], cmap='gray')\n",
        "    plt.title('Domain B', fontsize=20)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(132)\n",
        "    plt.imshow(fake_imA)\n",
        "    plt.title('Fake A', fontsize=20)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(133)\n",
        "    plt.imshow(real_imA)\n",
        "    plt.title('Real A', fontsize=20)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def showColored_two_models_Ims(imB, fake_imA_MAE, fake_imA_cGAN, real_imA):\n",
        "    # Convert tensors to numpy and denormalize\n",
        "    imB = imB[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
        "    # Detach fake_imA_MAE and fake_imA_cGAN before converting to numpy\n",
        "    fake_imA_MAE = fake_imA_MAE[0].detach().cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
        "    fake_imA_cGAN = fake_imA_cGAN[0].detach().cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
        "    real_imA = real_imA[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
        "\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(141)\n",
        "    plt.imshow(imB[:,:,0], cmap='gray')\n",
        "    plt.title('BW', fontsize=20)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(142)\n",
        "    plt.imshow(fake_imA_MAE)\n",
        "    plt.title('MAE', fontsize=20)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(143)\n",
        "    plt.imshow(fake_imA_cGAN)\n",
        "    plt.title('cGAN', fontsize=20)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(144)\n",
        "    plt.imshow(real_imA)\n",
        "    plt.title('Real', fontsize=20)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxLWgeEV5dT5"
      },
      "source": [
        "In the next section, we provide trained weights so you could resume or skip the training. These weights were obtained by training the net for 20 epochs. Note that if you want to further train the networks, GAN training is characterized for being really long.\n",
        "\n",
        "Let's see the training loop."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "batch_size = 128\n",
        "n_batches = dataset_loader.get_num_batches(batch_size)\n",
        "\n",
        "# Adversarial loss ground truths\n",
        "valid = torch.ones(batch_size, 1, device=device)\n",
        "fake = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    g_avg_loss = []\n",
        "    d_avg_loss = []\n",
        "    d_avg_acc = []\n",
        "\n",
        "    for batch_i, (imgs_A, imgs_B) in enumerate(dataset_loader.load_batch(batch_size)):\n",
        "        # Get current batch size (last batch might be smaller)\n",
        "        current_batch_size = imgs_A.size(0)\n",
        "\n",
        "        # Move data to device\n",
        "        imgs_A = imgs_A.to(device)\n",
        "        imgs_B = imgs_B.to(device)\n",
        "\n",
        "        # Create properly sized target tensors\n",
        "        valid = torch.ones(current_batch_size, 1, device=device)\n",
        "        fake = torch.zeros(current_batch_size, 1, device=device)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        optimizer_d.zero_grad()\n",
        "\n",
        "        # Real images - pass both images separately\n",
        "        real_validity = discriminator(imgs_A, imgs_B)\n",
        "        d_loss_real = criterion_mse(real_validity, valid)\n",
        "\n",
        "        # Fake images\n",
        "        fake_A = generator(imgs_B)\n",
        "        fake_validity = discriminator(fake_A.detach(), imgs_B)\n",
        "        d_loss_fake = criterion_mse(fake_validity, fake)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = (d_loss_real + d_loss_fake) / 2\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        optimizer_g.zero_grad()\n",
        "\n",
        "        # Generate fake images\n",
        "        fake_A = generator(imgs_B)\n",
        "\n",
        "        # Adversarial loss\n",
        "        validity = discriminator(fake_A, imgs_B)\n",
        "        g_loss_adv = criterion_mse(validity, valid)\n",
        "\n",
        "        # Pixel-wise loss\n",
        "        g_loss_l1 = criterion_l1(fake_A, imgs_A)\n",
        "\n",
        "        # Total generator loss\n",
        "        g_loss = g_loss_adv + 100 * g_loss_l1\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        # Calculate discriminator accuracy\n",
        "        real_acc = torch.mean((real_validity > 0.5).float())\n",
        "        fake_acc = torch.mean((fake_validity < 0.5).float())\n",
        "        d_acc = (real_acc + fake_acc) / 2\n",
        "\n",
        "        # Record metrics\n",
        "        d_avg_loss.append(d_loss.item())\n",
        "        d_avg_acc.append(d_acc.item())\n",
        "        g_avg_loss.append(g_loss.item())\n",
        "\n",
        "        elapsed_time = datetime.datetime.now() - start_time\n",
        "        remaining_time = (elapsed_time/(batch_i+1)) * (n_batches-batch_i-1)\n",
        "\n",
        "        if batch_i % 50 == 0:\n",
        "            showColoredIms(imgs_B, fake_A, imgs_A)\n",
        "\n",
        "        if batch_i % 10 == 0:\n",
        "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {batch_i}/{n_batches}] \"\n",
        "                  f\"[D loss: {np.mean(d_avg_loss):.4f}, acc: {100*np.mean(d_avg_acc):.0f}%] \"\n",
        "                  f\"[G loss: {np.mean(g_avg_loss):.4f}] elapsed: {elapsed_time} remaining: {remaining_time}\")\n",
        "\n",
        "\n",
        "# Save the models after training\n",
        "torch.save(generator.state_dict(), 'generator.pth')\n",
        "torch.save(discriminator.state_dict(), 'discriminator.pth')"
      ],
      "metadata": {
        "id": "Ljuh5h-5KW0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WC5CPlk2x-V"
      },
      "source": [
        "If desired, you could download the model you just trained. Those weights can be used for resuming future trainings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVftcAJb2uiU"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the generator weights\n",
        "files.download('generator.pth')\n",
        "\n",
        "# Download the discriminator weights\n",
        "files.download('discriminator.pth')\n",
        "\n",
        "# Optionally: Save a complete checkpoint including optimizer states\n",
        "checkpoint = {\n",
        "    'generator_state_dict': generator.state_dict(),\n",
        "    'discriminator_state_dict': discriminator.state_dict(),\n",
        "    'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "    'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "    'epoch': num_epochs,\n",
        "    'losses': {\n",
        "        'g_avg_loss': g_avg_loss,\n",
        "        'd_avg_loss': d_avg_loss,\n",
        "        'd_avg_acc': d_avg_acc\n",
        "    }\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, 'cgan_checkpoint.pth')\n",
        "files.download('cgan_checkpoint.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8aVvc8k2_NP"
      },
      "source": [
        "## Colouring Test Images\n",
        "\n",
        "We are ready to visualise how the network colours the test images. If you have not to train your model, you can optionally load the provided trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2ExvhyT5Oxh"
      },
      "outputs": [],
      "source": [
        "# Load model from saved file\n",
        "generator.load_state_dict(torch.load('generator.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apZzJgmM-2oD"
      },
      "source": [
        "Let's see the results, you can rerun the following code for visualizing more examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0ljS-uv1jMI"
      },
      "outputs": [],
      "source": [
        "# Put generator in evaluation mode\n",
        "generator.eval()\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for inference\n",
        "    for i in range(3):\n",
        "        # Get random batch from test set\n",
        "        im_A_real, im_B_test = dataset_loader.get_random_batch(batch_size=1, is_training=False)\n",
        "\n",
        "        # Move to device and generate fake image\n",
        "        im_B_test = im_B_test.to(device)\n",
        "        im_A_fake = generator(im_B_test)  # Forward pass instead of predict()\n",
        "\n",
        "        # Move back to CPU for visualization\n",
        "        im_A_real = im_A_real.cpu()\n",
        "        im_B_test = im_B_test.cpu()\n",
        "        im_A_fake = im_A_fake.cpu()\n",
        "\n",
        "        # Show results\n",
        "        showColoredIms(im_B_test, im_A_fake, im_A_real)\n",
        "\n",
        "# Put generator back in training mode if continuing training\n",
        "generator.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QfiXDhB3TSi"
      },
      "source": [
        "The colouring may not be perfect, but the network does a good job at guessing plausible colours. We have shown a simplified version of the Pix2pix method. Training GANs is a hard task, and there are many elements that could be added to improve results. For instance, [this repo](https://github.com/soumith/ganhacks), like many others, shows common tricks to improve GAN performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcG_R3OAGe5m"
      },
      "source": [
        "# Coursework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpQekp8aNVt_"
      },
      "source": [
        "\n",
        "## Task 1: MNIST generation using VAE and GAN\n",
        "\n",
        "**Report**\n",
        "* Train the given VAE model in the tutorial also using the same hyper parameters (batch size, optimizer, number of epochs, etc...) but increasing the latent dimensionality to 10. Compute the MSE on the reconstructed `x_test` images and Inception Score (IS). Now train the same model without the KL divergence loss, and compute again the MSE and IS score. Report the results in a table and discuss them. For the IS you can use in this case `image_inception_score(decoder, dim_random=10, denorm_im=False)`.\n",
        "\n",
        "* Train the GAN given in the tutorial with increased dimensionality of the initial random sample to 10 for 10 epochs and report its IS (use the same table as in the VAE case). Discuss the difference in the obtained IS for VAE and GAN and link it to the qualitative results. For the IS use in this case `image_inception_score(generator, dim_random=10, denorm_im=True)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKkjwCF6uw3K"
      },
      "source": [
        "## Task 2: Quantitative VS Qualitative Results\n",
        "\n",
        "In this task, we will observe the difference between two trained models for colouring images. One is the model trained during the tutorial, which uses a cGAN approach to predict the RGB pixel-wise values of a B&W image. The other one is a simple UNet autoencoder trained with a Mean Absolute Error (MAE) loss, which is trained to predict directly the RBG image without any GAN based learning strategy. We refer to the first and second models as cGAN and MAE models, respectively. For this task, 20 epochs trained weights for the cGAN and MAE models are provided. If desired, the code to train the MAE model can be found below:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the generator\n",
        "generator_mae = Generator(im_shape).to(device)\n",
        "\n",
        "# Define optimizer and loss\n",
        "optimizer_mae = torch.optim.Adam(generator_mae.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "criterion_mae = nn.L1Loss()  # Mean Absolute Error\n",
        "\n",
        "num_epochs = 1\n",
        "batch_size = 128\n",
        "n_batches = dataset_loader.get_num_batches(batch_size)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = datetime.datetime.now()\n",
        "    g_avg_loss = []\n",
        "\n",
        "    for batch_i, (imgs_A, imgs_B) in enumerate(dataset_loader.load_batch(batch_size)):\n",
        "        # Move data to device\n",
        "        imgs_A = imgs_A.to(device)\n",
        "        imgs_B = imgs_B.to(device)\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator (MAE)\n",
        "        # -----------------\n",
        "        optimizer_mae.zero_grad()\n",
        "\n",
        "        # Generate fake images\n",
        "        fake_A = generator_mae(imgs_B)\n",
        "\n",
        "        # Calculate MAE loss\n",
        "        g_loss = criterion_mae(fake_A, imgs_A)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        g_loss.backward()\n",
        "        optimizer_mae.step()\n",
        "\n",
        "        g_avg_loss.append(g_loss.item())\n",
        "\n",
        "        elapsed_time = datetime.datetime.now() - start_time\n",
        "        remaining_time = (elapsed_time/(batch_i+1)) * (n_batches-batch_i-1)\n",
        "\n",
        "        # Plot examples\n",
        "        if batch_i % 50 == 0:\n",
        "            with torch.no_grad():\n",
        "                fake_A = generator_mae(imgs_B)\n",
        "                showColoredIms(imgs_B, fake_A, imgs_A)\n",
        "\n",
        "        # Print progress\n",
        "        if batch_i % 10 == 0:\n",
        "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {batch_i}/{n_batches}] \"\n",
        "                  f\"[G loss: {np.mean(g_avg_loss):.4f}] \"\n",
        "                  f\"elapsed: {elapsed_time} remaining: {remaining_time}\")\n",
        "\n",
        "    # Save model\n",
        "    torch.save({\n",
        "        'generator_state_dict': generator_mae.state_dict(),\n",
        "        'optimizer_state_dict': optimizer_mae.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'loss': g_avg_loss\n",
        "    }, 'generator_mae.pth')"
      ],
      "metadata": {
        "id": "cTojDadnariK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9aTGM7qwtyO"
      },
      "source": [
        "Instead of training the models, we can directly load their pre-trained weights by running:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-load previously trained cGAN generator\n",
        "generator_cGAN = Generator(im_shape).to(device)\n",
        "generator_cGAN.load_state_dict(torch.load('generator.pth'))"
      ],
      "metadata": {
        "id": "9w71OzOuaMmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyXI9hCTzUn2"
      },
      "source": [
        "We have loaded both models, and we are ready to compare them. In this task, you are asked to analyse the difference between the quantitative versus the qualitative results. To do so, we provided two pieces of code. The first one will compute the MAE metric for both models in the test dataset. As we know, this metric is widely used on image generation tasks, such as image upsampling, image reconstruction, image translation, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t46neFcx6nyg"
      },
      "outputs": [],
      "source": [
        "# Evaluation for MAE-trained generator\n",
        "g_mae_avg_mae = []\n",
        "generator_mae.eval()  # Set to evaluation mode\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for batch_i, (imgs_A, imgs_B) in enumerate(dataset_loader.load_batch(128, is_training=False)):\n",
        "        imgs_A = imgs_A.to(device)\n",
        "        imgs_B = imgs_B.to(device)\n",
        "\n",
        "        fake_A = generator_mae(imgs_B)\n",
        "        mae = F.l1_loss(fake_A, imgs_A).item()  # Calculate MAE\n",
        "        g_mae_avg_mae.append(mae)\n",
        "\n",
        "print(\"MAE (Trained MAE): {:.4f}\".format(np.mean(g_mae_avg_mae)))\n",
        "\n",
        "# Evaluation for cGAN-trained generator\n",
        "g_cgan_avg_mae = []\n",
        "generator_cGAN.eval()  # Set to evaluation mode\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_i, (imgs_A, imgs_B) in enumerate(dataset_loader.load_batch(128, is_training=False)):\n",
        "        imgs_A = imgs_A.to(device)\n",
        "        imgs_B = imgs_B.to(device)\n",
        "\n",
        "        fake_A = generator_cGAN(imgs_B)\n",
        "        mae = F.l1_loss(fake_A, imgs_A).item()  # Calculate MAE\n",
        "        g_cgan_avg_mae.append(mae)\n",
        "\n",
        "print(\"MAE (Trained cGAN): {:.4f}\".format(np.mean(g_cgan_avg_mae)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddZgs-Il8F9B"
      },
      "source": [
        "The next piece of code will show coloured examples for both networks, so you can check them visually and discuss which model is better. First, we need to create an iterator object to go through the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAVQ-Dyj8ECJ"
      },
      "outputs": [],
      "source": [
        "iterator = iter(dataset_loader.load_batch(1, is_training=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjTR797V8eys"
      },
      "source": [
        "Run multiple examples so that you have a clear idea of how both methods differ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test example\n",
        "[imgs_A, imgs_B] = next(iterator)\n",
        "\n",
        "if not isinstance(imgs_A, torch.Tensor):\n",
        "    imgs_A = torch.from_numpy(imgs_A)\n",
        "    imgs_B = torch.from_numpy(imgs_B)\n",
        "\n",
        "imgs_A = imgs_A.to(device)\n",
        "imgs_B = imgs_B.to(device)\n",
        "\n",
        "# Generate predictions for both models\n",
        "with torch.no_grad():\n",
        "    fake_A_cGAN = generator_cGAN(imgs_B).cpu()\n",
        "    fake_A_MAE = generator_mae(imgs_B).cpu()\n",
        "\n",
        "# Plot all images\n",
        "showColored_two_models_Ims(imgs_B.cpu(), fake_A_MAE, fake_A_cGAN, imgs_A.cpu())"
      ],
      "metadata": {
        "id": "JuPRF6DZeHyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss_1bTVE-Us_"
      },
      "source": [
        "We showed that both models obtain a similar MAE value. If we would only take into account the quantitative metric, as done in many scientific articles, we would say that the MAE model is better. However, in addition to the quantitative results, we need to analyse visually the results produced by the two networks to declare which is the best model.\n",
        "\n",
        "**Report**\n",
        "\n",
        "\n",
        "*   Run the previous code to analyse several coloured images for both models. Based on previous results and linked to GAN theory, discuss from the numerical and visual perspective if both models are similar, or whether there is a better one. You can provide in the report visual examples together with their MAE values to support your arguments. The figure of this task can be included in the Appendix. Discussion still needs to go into the main text."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}