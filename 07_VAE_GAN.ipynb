{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/07_VAE_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb9t6NTe-6BL"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/07_VAE_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtCVMgi8iygt"
      },
      "source": [
        "# Variational Autoencoders\n",
        "\n",
        "<a href=\"https://ibb.co/GMMVqft\"><img src=\"https://i.ibb.co/X55zqf3/vae.jpg\" alt=\"vae\" border=\"0\"></a>\n",
        "\n",
        "Image taken from [here](http://kvfrans.com/variational-autoencoders-explained/)\n",
        "\n",
        "In the autoencoder tutorial we showed how to learn a meaningful representation of the data by using an autoencoder. In an autoencoder, the input image was transformed into a vector which encoded the information from the image in a lower dimensionality space. Then, we decoded that vector to get a reconstruction of the input image. However, the model was focused on encoding existing data for representation learning or similar purposes. To tackle the generation of new data, we will use a Variational Autoencoder (VAE) approach. The image shows an overview of the VAE method.\n",
        "\n",
        "Parts of the code are taken from [here](https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/), which contains a more in-depth explanation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNlgJvqul7W2"
      },
      "source": [
        "Before starting to define the different parts of the VAE, let's import the needed modules for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "vuMgWc5dpsnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91g-k-uqdRW7"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import sys\n",
        "\n",
        "from IPython.display import HTML, clear_output, display\n",
        "from PIL import Image as pil_image\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from mpl_toolkits import mplot3d\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "import torchinfo\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9_UOneRBn1q"
      },
      "source": [
        "Now let's do a quick recap of the Variational AutoEncoder (VAE) theory. As stated in the lecture, we want to find the $\\hat{\\theta}$ that approximates $P_\\theta(x)$ by doing:\n",
        "\n",
        "$$\n",
        "\\hat{\\theta} = \\text{argmax}_\\theta \\sum_{i=1}^n \\mathbb{E}_{Q_\\phi(z|x_i)}[\\log(P_\\theta(x_i|z))] - \\text{KL}(Q_\\phi(z|x_i) || P_\\theta(z))\n",
        "$$\n",
        "Se will minimize the negative of that term in order to maximize it, where\n",
        "we will train our model using a stochastic approach by sampling mini-batches from the dataset. First, we define the two losses we will use. The loss `nll` is the first term of the equation, whereas the `kl` is the second term of the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvWDTeNodH78"
      },
      "outputs": [],
      "source": [
        "# Negative log likelihood loss (Bernoulli)\n",
        "def nll(y_true, y_pred, reduction='sum'):\n",
        "    loss = F.binary_cross_entropy(y_pred, y_true, reduction='none').sum(dim=-1)\n",
        "\n",
        "    if reduction == 'sum':\n",
        "        return loss.sum()\n",
        "    elif reduction == 'mean':\n",
        "        return loss.mean()\n",
        "    else:\n",
        "        return loss\n",
        "\n",
        "# KL Divergence\n",
        "def kl(mu, log_var, reduction='sum'):\n",
        "    loss = -0.5 * (1 + log_var - mu.pow(2) - log_var.exp()).sum(dim=1)\n",
        "    if reduction == 'sum':\n",
        "        return loss.sum()\n",
        "    elif reduction == 'mean':\n",
        "        return loss.mean()\n",
        "    else:\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUNQrpSjDC1Q"
      },
      "source": [
        "In the last block, we just defined the two losses we will use. Here we build the whole VAE model. First, we build the encoder. The goal of the encoder $\\phi$ is to approximate $P_\\theta(z|x_i)$ via $Q_\\phi(z|x_i)$.  We assume that $P(z)$ is a Normal distribution with zero mean and unit variance. We also assume that $Q(z|x)={N}(\\mu_x, \\sigma_x)$ so the encoder tries to recover the parameters $\\mu_x, \\sigma_x$ for the different $x$ (which are the input images), i.e. the encoder outputs for each latent dimension a mean and standard deviation. The code for this encoder is the following:\n",
        "\n",
        "```python\n",
        "# Define latent dimension\n",
        "self.latent_dim = latent_dim\n",
        "\n",
        "# Encoder\n",
        "self.encoder = nn.Sequential(\n",
        "    nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Flatten()\n",
        ")\n",
        "\n",
        "# Compute μ and log(σ²)\n",
        "self.z_mu = nn.Linear(128*7*7, latent_dim)\n",
        "self.z_log_var = nn.Linear(128*7*7, latent_dim)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJKpM3JjEyRK"
      },
      "source": [
        "As we mentioned, the encoder will output the parameters $\\mu_x, \\sigma_x$. Now we will sample from the normal distribution defined by those parameters to pass it to the decoder. However, we now face one of the problems of implementing a VAE: we want to optimize both decoder and encoder at the same time to i) encourage good reconstruction and ii) to make $z$ follow a normal distribution. What is the problem here? Using a standard sampling method, i.e by directly sampling using the mean and standard deviation output by the encoder, we cannot train it in an end-to-end manner as sampling is not a differentiable operation.\n",
        "\n",
        "We need a trick to solve this issue. We can use one of the properties of the Normal probability distribution, which is that sampling $N(\\mu_\\psi, \\sigma_\\psi)$ is the same as $z = \\mu_\\psi + \\sigma_\\psi \\odot \\epsilon,\\ \\epsilon \\sim N(0, I)$. Now, the sampling process is just a factor multiplying by the prediction $\\sigma_\\phi$ of the encoder, meaning we can propagate the gradients from the output back to the encoder. This is called the reparametrisation trick. We use this trick in the following block.\n",
        "\n",
        "\n",
        "```python\n",
        "def reparameterize(self, mu, log_var):\n",
        "    ##### Reparametrisation trick\n",
        "    # Log_var to sigma\n",
        "    std = torch.exp(0.5 * log_var)\n",
        "    # Define a separate noise input (which must be provided during training)\n",
        "    eps = torch.randn_like(std)\n",
        "    # Multiply sigma with the external noise and add mu to obtain the latent vector\n",
        "    return mu + eps * std\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6MSQqJ3ot_r"
      },
      "source": [
        "Now we define the decoder, which will take the sampling from $\\mathcal{N}(\\mu_\\psi, \\sigma_\\psi)$ as input and output an image.\n",
        "\n",
        "\n",
        "```python\n",
        "# Decoder\n",
        "self.decoder = nn.Sequential(\n",
        "    nn.Linear(latent_dim, 128*7*7),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Unflatten(1, (128, 7, 7)),\n",
        "    nn.Upsample(scale_factor=2),\n",
        "    nn.Conv2d(128, 64, kernel_size=5, padding=2),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Upsample(scale_factor=2),\n",
        "    nn.Conv2d(64, 1, kernel_size=5, padding=2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Flatten()\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LElZ521BrhrA"
      },
      "source": [
        "We have defined both the encoder and the decoder, and we are ready to train the model. We now build the model, which will have two inputs: the image `x`  for the encoder; and the sample `eps` (which refers to the sample from $\\mathcal{N}(0, 1)$ we mentioned before) for the decoder, which will use $\\mu_x, \\sigma_x$ via the reparametrisation trick. The code below will put together all the previous parts as a single class and compile."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VAE Model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=2):\n",
        "        super(VAE, self).__init__()\n",
        "        # Define latent dimension\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Compute μ and log(σ²)\n",
        "        self.z_mu = nn.Linear(128*7*7, latent_dim)\n",
        "        self.z_log_var = nn.Linear(128*7*7, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128*7*7),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Unflatten(1, (128, 7, 7)),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, kernel_size=5, padding=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(64, 1, kernel_size=5, padding=2),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        return self.z_mu(h), self.z_log_var(h)\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        ##### Reparametrisation trick\n",
        "        # Log_var to sigma\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        # Define a separate noise input (which must be provided during training)\n",
        "        eps = torch.randn_like(std)\n",
        "        # Multiply sigma with the external noise and add mu to obtain the latent vector\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return self.decode(z), mu, log_var\n",
        "\n",
        "torchinfo.summary(VAE(latent_dim=2), input_size=(1, 1, 28, 28))"
      ],
      "metadata": {
        "id": "djjsTvVUVCfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhZfdpBBrl2O"
      },
      "source": [
        "Now we train the model for some epochs to see if we can model our data. After the training process we will use this trained VAE to generate new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZd94z0Fc6WT"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Data\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "latent_dim = 2\n",
        "model = VAE(latent_dim).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training the model\n",
        "best_loss = float('inf')\n",
        "patience = 3\n",
        "no_improve_epochs = 0\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        recon_batch, mu, log_var = model(data)\n",
        "        nll_loss = nll(data.view(-1, 784), recon_batch)\n",
        "        kl_loss = kl(mu, log_var)\n",
        "\n",
        "        loss = nll_loss + kl_loss\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            data = data.to(DEVICE)\n",
        "            recon_batch, mu, log_var = model(data)\n",
        "            nll_loss = nll(data.view(-1, 784), recon_batch)\n",
        "            kl_loss = kl(mu, log_var)\n",
        "            val_loss += (nll_loss + kl_loss).item()\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(test_loader.dataset)\n",
        "    print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        no_improve_epochs = 0\n",
        "        torch.save(model.state_dict(), 'best_vae_model.pth')\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "        if no_improve_epochs >= patience:\n",
        "            print(f'Early stopping after {epoch} epochs')\n",
        "            model.load_state_dict(torch.load('best_vae_model.pth'))\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byac9m8MtpnI"
      },
      "source": [
        "Assuming we input into the decoder $\\mu_x$ obtained after encoding the image with the encoder, we can retrieve the MSE in the test set using the following piece of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsM1nhLPGwbw"
      },
      "outputs": [],
      "source": [
        "# Calculate MSE on test set\n",
        "model.eval()\n",
        "total_mse = 0\n",
        "with torch.no_grad():\n",
        "    for data, _ in test_loader:\n",
        "        data = data.to(DEVICE)\n",
        "        recon_batch, mu, _, = model(data)\n",
        "        mse = F.mse_loss(recon_batch, data.view(-1, 784), reduction='sum')\n",
        "        total_mse += mse.item()\n",
        "\n",
        "print(f\"Test MSE: {total_mse / len(test_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz5MmFMzD48G"
      },
      "source": [
        "We trained an encoder to model $Q(z|x)$ which outputs two parameters per latent dimension for each image $x$, which are  $\\mu_x, \\sigma_x$. We now plot the distribution of the $\\mu_x$ when encoding the different images from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4H3u0n9dBLc"
      },
      "outputs": [],
      "source": [
        "# Plot latent space distribution\n",
        "model.eval()\n",
        "latent_vectors = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data = data.to(DEVICE)\n",
        "        mu, _ = model.encode(data)\n",
        "        latent_vectors.append(mu.cpu())\n",
        "        labels.append(target.cpu())\n",
        "\n",
        "z_test = torch.cat(latent_vectors, dim=0).numpy()\n",
        "y_test = torch.cat(labels, dim=0).numpy()\n",
        "\n",
        "# Display a 2D plot of the digit classes in the latent space\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "cm = plt.get_cmap('gist_rainbow')\n",
        "ax.set_prop_cycle(color=[cm(1.*i/(10)) for i in range(10)])\n",
        "for l in range(10):\n",
        "    # Only select indices for corresponding label\n",
        "    ind = y_test == l\n",
        "    ax.scatter(z_test[ind, 0], z_test[ind, 1], label=str(l), s=10)\n",
        "ax.legend()\n",
        "plt.title(\"Latent distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwjzHuHIEiMF"
      },
      "source": [
        "The distribution of the encoded means shows how they are clustered by class too, as in the autoencoder case. However, in this case, the distribution also follows a kind of circular distribution around the centre due to the Kullback Leibler divergence term in the loss. We made the $\\mu_x$ output by the encoder to be close to zero, and $\\sigma_x$ to be close to 1.\n",
        "\n",
        "Now let's start with the generation of data, which is the main reason we trained this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyplRgt6e3Lm"
      },
      "outputs": [],
      "source": [
        "# Display a 2D manifold of the digits\n",
        "n = 15  # figure with 15x15 digits\n",
        "digit_size = 28\n",
        "\n",
        "# Linearly spaced coordinates transformed through inverse CDF of Gaussian\n",
        "z1 = stats.norm.ppf(np.linspace(0.01, 0.99, n))\n",
        "z2 = stats.norm.ppf(np.linspace(0.01, 0.99, n))\n",
        "z_grid = np.dstack(np.meshgrid(z1, z2))\n",
        "\n",
        "# Generate images from grid\n",
        "with torch.no_grad():\n",
        "    z_tensor = torch.FloatTensor(z_grid.reshape(n*n, latent_dim)).to(DEVICE)\n",
        "    x_pred = model.decoder(z_tensor).cpu().numpy()\n",
        "\n",
        "x_pred_grid = x_pred.reshape(n, n, digit_size, digit_size)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(np.block(list(map(list, x_pred_grid))), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP7Il4dTeTAv"
      },
      "source": [
        "You can see in the image how there is a smooth transition between the different generated numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A36-s7n3tBpv"
      },
      "source": [
        "Now let's generate a nice animation for the latent variable, where we show the point we used in $z$ to generate the data and the corresponding image generated."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We create a 2d array\n",
        "# Number of points to use, increase it for smoother animation\n",
        "# Using more points makes the function slower\n",
        "n_points = 50\n",
        "# theta from 0 to 2pi\n",
        "theta = np.linspace(0, 2*np.pi, n_points)\n",
        "# radius of the circle (change it depending on your reprentation space plot)\n",
        "r = 1\n",
        "# compute x and y (you can add an offset depending on your latent space)\n",
        "offset_x = 0\n",
        "offset_y = 0\n",
        "x = r*np.cos(theta) + offset_x\n",
        "y = r*np.sin(theta) + offset_y\n",
        "latent = np.stack([x, y], -1)\n",
        "\n",
        "## We now plot as before the 2d scatter with the images from the test set\n",
        "## and the corresponding label\n",
        "# Get test latent vectors\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_test = []\n",
        "    for data, _ in test_loader:\n",
        "        mu, _ = model.encode(data.to(DEVICE))\n",
        "        z_test.append(mu.cpu())\n",
        "    z_test = torch.cat(z_test).numpy()\n",
        "\n",
        "# Create figure\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# First subplot: latent space with test points\n",
        "ax[0].set_prop_cycle(color=[plt.cm.gist_rainbow(1.*i/10) for i in range(10)])\n",
        "for l in range(10):\n",
        "    ind = y_test == l\n",
        "    ax[0].scatter(z_test[ind, 0], z_test[ind, 1], label=str(l), s=10)\n",
        "ax[0].legend()\n",
        "scat = ax[0].scatter(latent[0,0], latent[0,1], s=200, c='k')\n",
        "\n",
        "# Second subplot: generated image\n",
        "with torch.no_grad():\n",
        "    latent_tensor = torch.FloatTensor(latent).to(DEVICE)\n",
        "    latent_im = model.decoder(latent_tensor).cpu().numpy()\n",
        "\n",
        "im = ax[1].imshow(latent_im[0].reshape(28, 28), cmap='gray')\n",
        "ax[1].axis('off')\n",
        "\n",
        "# Animation update function\n",
        "def updatefig(i):\n",
        "    scat.set_offsets(latent[i])\n",
        "    im.set_array(latent_im[i].reshape(28, 28))\n",
        "    return im,\n",
        "\n",
        "# Create animation\n",
        "anim = animation.FuncAnimation(fig, updatefig, frames=n_points, interval=200, blit=True)\n",
        "plt.close()\n",
        "HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "pnobt5GeY-7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hc-XviqI44H"
      },
      "source": [
        "# Generative Adversarial Networks\n",
        "\n",
        "\n",
        "\n",
        "![](http://2018.igem.org/wiki/images/4/48/T--Vilnius-Lithuania-OG--introduction1.png)\n",
        "[Image source](https://2018.igem.org/Team:Vilnius-Lithuania-OG/Gan_Introduction)\n",
        "\n",
        "Generative Adversarial Networks have been shown to improve the generation of data compared to approaches such as VAE.\n",
        "\n",
        "As you learnt in the lecture, we have two networks playing what is called a min max game between them. The Generator, $G$, tries to generate data that looks similar to real data, whereas the discriminator $D$ tries to distinguish between real and fake data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZJHL5nGTM-M"
      },
      "source": [
        "We first define $G$ here, and in this case we will use a convolutional network. Notice that we use what is called LeakyReLU for the activation functions, which have been shown to work well when using GANs. $G$ will take a vector of noise sampled from $N(0, 1)$ and output a generated image. We use `tanh` as the last activation because the data will be normalized to be between $[-1, 1]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvCu8786qNK1"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, dim_latent=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.dim_latent = dim_latent\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is random noise vector of size dim_latent\n",
        "            nn.Linear(self.dim_latent, 128 * 7 * 7),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Unflatten(1, (128, 7, 7)),  # Reshape to (128, 7, 7)\n",
        "\n",
        "            # Upsample to (128, 14, 14)\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=128,\n",
        "                out_channels=128,\n",
        "                kernel_size=(4, 4),\n",
        "                stride=(2, 2),\n",
        "                padding=(1, 1),\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            # Upsample to (128, 28, 28)\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=128,\n",
        "                out_channels=128,\n",
        "                kernel_size=(4, 4),\n",
        "                stride=(2, 2),\n",
        "                padding=(1, 1),\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            # Output\n",
        "            nn.Conv2d(\n",
        "               in_channels=128, out_channels=1, kernel_size=(3, 3), padding=(1, 1)\n",
        "            ),\n",
        "            nn.Tanh()  # range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "# Show model summary\n",
        "torchinfo.summary(Generator(), input_size=(1, 1024))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x2J0DyBNRW0"
      },
      "source": [
        "Here, we define the discriminator $D$. The discriminator is trained with the `nn.BCELoss`, as its job is to discriminate between real and fake data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8Ds7WLJqUGw"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input: 1x28x28 image\n",
        "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            # 64x14x14\n",
        "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            # 64x7x7\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "# Print model summary\n",
        "torchinfo.summary(Discriminator(), input_size=(1, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCmi_IDwNasg"
      },
      "source": [
        "We defined the combined network, which wraps the generator and discriminator in a single network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77v6wY77qupP"
      },
      "outputs": [],
      "source": [
        "class GAN(nn.Module):\n",
        "    def __init__(self, dim_latent=1024):\n",
        "        super(GAN, self).__init__()\n",
        "        self.dim_latent = dim_latent\n",
        "        self.generator = Generator(self.dim_latent)\n",
        "        self.discriminator = Discriminator()\n",
        "\n",
        "    def generate(self, z):\n",
        "        return self.generator(z)\n",
        "\n",
        "    def discriminate(self, image):\n",
        "        return self.discriminator(image)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQchMjunN0PJ"
      },
      "source": [
        "We also need some other helper functions for the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3kiJsWU2JXM"
      },
      "outputs": [],
      "source": [
        "# Data Generation\n",
        "def create_real_label(batch_size):\n",
        "    \"\"\"Create labels as 1 for real images.\"\"\"\n",
        "    return torch.ones(batch_size, 1, device=DEVICE)\n",
        "\n",
        "def create_fake_label(batch_size):\n",
        "    \"\"\"Create labels as 0 for generated images.\"\"\"\n",
        "    return torch.zeros(batch_size, 1, device=DEVICE)\n",
        "\n",
        "def create_noise(batch_size, dim_latent=1024):\n",
        "    \"\"\"Create random noise.\"\"\"\n",
        "    return torch.randn(batch_size, dim_latent, device=DEVICE)\n",
        "\n",
        "\n",
        "# Plot\n",
        "def plot_loss(losses):\n",
        "    \"\"\"Plot discriminator and generator loss during training\"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(losses[\"d\"], label='Discriminator Loss')\n",
        "    plt.plot(losses[\"g\"], label='Generator Loss')\n",
        "    plt.title(\"Training Losses\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"BCELoss\")\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss.png')\n",
        "    plt.close()\n",
        "\n",
        "def generate_and_plot_images(gan, num_images=16, dim=(4,4), figsize=(10,10)):\n",
        "    \"\"\"Generate and plot images from the generator\"\"\"\n",
        "    # Generate images\n",
        "    noise = create_noise(num_images)\n",
        "    with torch.no_grad():\n",
        "        images = gan.generate(noise).cpu().numpy()\n",
        "    images = images.reshape(-1, 28, 28)\n",
        "    images = (images + 1) / 2.0  # From [-1, 1] to [0, 1] for plotting\n",
        "\n",
        "    # Reshape and plot\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(images[i], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('./images.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "razKxofMN60c"
      },
      "source": [
        "We will not use the regular training pipeline for GAN, as it is optimized in an adversarial style. In practice, the training at each iteration contains two parts:\n",
        "* **First part - Training the discriminator:** In this part, only the discriminator is trained.  The generator will output some fake images using noise as input. Then, we will give the discriminator these generated images and some images sampled from the real dataset and train it to distinguish between the two of them.\n",
        "\n",
        "* **Second part: Training the generator**: We use the generator to output fake images again, and the discriminator will try to guess if these newly generated images are real or fake. However, the aim in this second step is not to update the discriminator, only the generator. This is why in the combined network we made the discriminator not to be trainable. The discriminator is used to pass information (i.e., gradient) to update the generator. Hence, the generator will try to change its weights to make the discriminator think the data comes from the real distribution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator Training\n",
        "def train_discriminator(image_real, gan, optimizer_d, criterion):\n",
        "    # Generate labels and noise\n",
        "    batch_size = image_real.size(0)\n",
        "    label_real = create_real_label(batch_size)\n",
        "    label_fake = create_fake_label(batch_size)\n",
        "    noise = create_noise(batch_size)\n",
        "\n",
        "    # Generate image\n",
        "    image_generated = gan.generate(noise).detach()  # Detach is required as only discriminator is updated in this step\n",
        "\n",
        "    # Train discriminator\n",
        "    optimizer_d.zero_grad()\n",
        "    ## Train discriminator to discriminate real images\n",
        "    label_real_predicted = gan.discriminate(image_real)\n",
        "    loss_real = criterion(label_real_predicted, label_real)\n",
        "    ## Train discriminator to discrminate fake images\n",
        "    label_fake_predicted = gan.discriminate(image_generated)\n",
        "    loss_fake = criterion(label_fake_predicted, label_fake)\n",
        "    ## Backpropagate and optimize\n",
        "    loss = (loss_real + loss_fake) / 2.0  # Dividing by 2 to keep the loss scale\n",
        "    loss.backward()\n",
        "    optimizer_d.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "iv3IFaDZcGG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Training\n",
        "def train_generator(image_real, gan, optimizer_g, criterion):\n",
        "    # Generate labels and noise\n",
        "    batch_size = image_real.size(0)\n",
        "    label_real = create_real_label(batch_size)\n",
        "    noise = create_noise(batch_size)\n",
        "\n",
        "    # Train generator\n",
        "    optimizer_g.zero_grad()\n",
        "    ## Generate fake images\n",
        "    image_generated = gan.generate(noise)\n",
        "    ## Use discriminator to judge whether the generated images are real or not\n",
        "    label_predicted = gan.discriminate(image_generated)\n",
        "    loss = criterion(label_predicted, label_real)\n",
        "    ## Backpropagate and update\n",
        "    loss.backward()\n",
        "    optimizer_g.step()  # Discriminator is not updated as optimizer_g doesn't contain its parameters\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "XHbHxk20gj02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3EgAKi5IwIa"
      },
      "source": [
        "Let's start the training process. We will use MNIST again. A standard practice for GANs is to normalize images to the range $[-1, 1]$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Training Configuration\n",
        "epochs = 10\n",
        "batch_size = 1024\n",
        "lr = 2.0e-4\n",
        "betas = (0.5, 0.999)\n",
        "dim_latent = 1024\n",
        "plt_frq = 10\n",
        "\n",
        "\n",
        "# Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model, Optimizer, Criterion\n",
        "gan = GAN(dim_latent).to(DEVICE)\n",
        "optimizer_d = optim.Adam(gan.discriminator.parameters(), lr=lr, betas=betas)\n",
        "optimizer_g = optim.Adam(gan.generator.parameters(), lr=lr, betas=betas)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Monitoring\n",
        "losses = {\"d\": [], \"g\": []}\n",
        "total_iter = 0\n",
        "saved_images = []\n",
        "saved_iterations = []\n",
        "\n",
        "progress_out = widgets.Output()\n",
        "plot_out = widgets.Output()\n",
        "display(progress_out)\n",
        "display(plot_out)\n",
        "\n",
        "def update_plots(iteration, gan):\n",
        "    global saved_images, saved_iterations\n",
        "    with plot_out:\n",
        "        plot_out.clear_output(wait=True)\n",
        "        generate_and_plot_images(gan)\n",
        "\n",
        "        # Read the saved image file and append the image data\n",
        "        img_data = mpimg.imread('images.png')\n",
        "        saved_images.append(img_data)\n",
        "        saved_iterations.append(iteration)\n",
        "\n",
        "        # Display the last saved image\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "        ax.imshow(img_data)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'Iteration: {iteration}')\n",
        "        plt.tight_layout()\n",
        "        display(fig)\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "# Training\n",
        "for epoch in range(1, epochs+1):\n",
        "    with progress_out:\n",
        "        pbar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch}\", leave=True)\n",
        "\n",
        "        for i, (image_real, _) in enumerate(train_loader):\n",
        "            total_iter += 1\n",
        "            image_real = image_real.to(DEVICE)\n",
        "\n",
        "            # ---------------------\n",
        "            # Train Discriminator\n",
        "            # ---------------------\n",
        "            loss_d = train_discriminator(image_real, gan, optimizer_d, criterion)\n",
        "            losses[\"d\"].append(loss_d)\n",
        "\n",
        "            # -----------------\n",
        "            # Train Generator\n",
        "            # -----------------\n",
        "            loss_g = train_generator(image_real, gan, optimizer_g, criterion)\n",
        "            losses[\"g\"].append(loss_g)\n",
        "\n",
        "            # Progress bar\n",
        "            pbar.set_postfix({\"loss_d\": f\"{loss_d:.4f}\", \"loss_g\": f\"{loss_g:.4f}\"})\n",
        "            pbar.update(1)\n",
        "\n",
        "            # Plot periodically\n",
        "            if total_iter % plt_frq == 0:\n",
        "                update_plots(total_iter, gan)\n",
        "\n",
        "        pbar.close()"
      ],
      "metadata": {
        "id": "qVzi97dTgwg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot final losses\n",
        "plot_loss(losses)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "img = mpimg.imread('loss.png')\n",
        "ax.imshow(img)\n",
        "ax.axis('off')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "LxE_JEUdbwjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create animation\n",
        "fig_anim, ax_anim = plt.subplots(figsize=(5, 5))\n",
        "# Use the first saved image data directly\n",
        "im_anim = ax_anim.imshow(saved_images[0])\n",
        "ax_anim.axis('off')\n",
        "title_text = ax_anim.set_title(f'Iteration: {saved_iterations[0]}')\n",
        "\n",
        "def update_frame(i):\n",
        "    # Use the saved image data for updating the frame\n",
        "    im_anim.set_array(saved_images[i])\n",
        "    title_text.set_text(f'Iteration: {saved_iterations[i]}')\n",
        "    return im_anim, title_text\n",
        "\n",
        "anim = animation.FuncAnimation(fig_anim, update_frame, frames=len(saved_images),\n",
        "                              interval=500, blit=False, repeat=False)\n",
        "\n",
        "# Display animation\n",
        "HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "W2lZHbPGbwfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQPlGIFoI2Pv"
      },
      "source": [
        "The plots show that: (1) Before 200 iterations, generator loss quickly drops, meaning that it is quickly learning how to generate images from random noise; (2) After 200 iterations, the generator and discriminator loss fluctuates around a certain value, meaning that they are competing with each other and no one is taking a dominant role. It is worth noting that although there is no clear loss drop/convergence in this case, it can be seen from the visualization that the generator is getting more realistic images as the training goes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s explore the generator’s latent space using a random walk. Starting from a single randomly sampled latent vector $z \\in \\mathbb{R}^{1024}$, we iteratively add small random perturbations to create a sequence of latent vectors.\n",
        "Each step moves slightly in a random direction within the high-dimensional latent space.\n",
        "By generating images along this trajectory, we can observe how the generator smoothly transforms digits as we “walk” through the latent space."
      ],
      "metadata": {
        "id": "-_2h_s_IQ-1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change seed here to see different samples\n",
        "set_seed(42)\n",
        "\n",
        "# Random walk in latent space (dim_latent = 1024)\n",
        "n = 15  # number of steps in the walk\n",
        "digit_size = 28\n",
        "\n",
        "# Start from a random latent vector\n",
        "z_start = np.random.normal(0, 1, size=(dim_latent,))\n",
        "\n",
        "# Define step size for the walk (smaller = smoother changes)\n",
        "step_size = 0.7\n",
        "\n",
        "# Collect latent vectors along the walk\n",
        "z_walk = [z_start]\n",
        "for i in range(1, n):\n",
        "    step = np.random.normal(0, step_size, size=(dim_latent,))\n",
        "    z_next = z_walk[-1] + step\n",
        "    z_walk.append(z_next)\n",
        "\n",
        "z_walk = np.stack(z_walk, axis=0).astype(np.float32)\n",
        "\n",
        "# Generate images from the latent walk\n",
        "with torch.no_grad():\n",
        "    z_tensor = torch.from_numpy(z_walk).float().to(DEVICE)\n",
        "    x_pred = gan.generate(z_tensor).cpu().numpy()\n",
        "\n",
        "# Rescale from [-1, 1] to [0, 1]\n",
        "x_pred = (x_pred + 1) / 2.0\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(15, 2))\n",
        "for i in range(n):\n",
        "    plt.subplot(1, n, i+1)\n",
        "    img = x_pred[i].reshape(digit_size, digit_size)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Random walk in 1024-d latent space\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xwdZ9xihQXj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObF7kxiDR06j"
      },
      "source": [
        "**Training instability**\n",
        "\n",
        "GANs are quite difficult to train as there are several factors that can hurt their performance. Some of the most frequent issues are:\n",
        "\n",
        "* [Mode collapsing](https://www.youtube.com/watch?v=ktxhiKhWoEE): The generator is not capable of creating diverse images, it only generates a limited set of images.\n",
        "* Discriminator loss decreases quickly: in some cases the discriminator may be too powerful, so it quickly learns at the beginning which images are fake and which real, leading to small gradients passed to the generator.\n",
        "* Hyper parameter sensitivity\n",
        "* Non-convergence\n",
        "\n",
        "There are some tricks that have been shown to improve convergence, some of them are shared in this [repo](https://github.com/soumith/ganhacks).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENoiS-6xSYfy"
      },
      "source": [
        "# Inception Score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9bXrKeu6ZG_"
      },
      "source": [
        "The evaluation of a generative model quantitatively is a problem that is being research. The metric should evaluate the coverage, i.e. how diverse the generated data is, and sample quality, which is related to the visual quality of the sample. A widely used metric is the Inception Score, which is defined as:\n",
        "$$\n",
        "\\text{IS}(G)= \\exp(\\mathbb{E}_{x\\sim p_a}\\ D_{KL}(p(y|x)||p(y)))\n",
        "$$\n",
        "Let's explain the term a little bit. We aim to have a high IS, which means that we want a high $D_{KL}$ between $p(y|x)$ and $p(y)$. Ideally, what we want is $p(y|x)$ to be \"peaky\" and $p(y)$ to be uniform. $p(y|x)$ is the predicted probability vector $y$ given a generated sample $x$ by a model, in this case an Inception model (hence, the name of the score). If the sample $x$ is of high quality, the model should classify it with confidence in one of the available classes, hence the \"peaky\" $p(y|x)$. The term $p(y)$ is related to the diversity of the data, and is the marginal probability computed as $\\int_z p(y|x=G(z))dz$, hence we average the predicted probabilites of the Inception model for the given samples. If the data is diverse, we will see approximately obtain a flat $p(y)$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we will be using a pre-trained MNIST classifier as the inception model."
      ],
      "metadata": {
        "id": "ZhlWerR1ud6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/MatchLab-Imperial/deep-learning-course/master/asset/07_VAE_GAN/mnist_classifier_best.pth -O mnist_classifier_best.pth"
      ],
      "metadata": {
        "id": "FerfntFYuWW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inception model\n",
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTClassifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # Conv2d_1\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Conv2d_2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Conv2d_3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Pool and dense\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Print the model summary\n",
        "torchinfo.summary(MNISTClassifier(), input_size=(1, 1, 28, 28))"
      ],
      "metadata": {
        "id": "SJKXexCvoU-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_inception_score(model, dim_latent, num_images=10000, batch_size=128, denormalize=True, eps=1.0e-16):\n",
        "    \"\"\"Compute Inception Score directly from a generator.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Generate images\n",
        "    noise = create_noise(num_images, dim_latent)\n",
        "    with torch.no_grad():\n",
        "        x_pred = model(noise)\n",
        "\n",
        "    # Pre-process images\n",
        "    if len(x_pred.shape) == 2:  # flattened\n",
        "        x_pred = x_pred.view(num_images, 1, 28, 28)\n",
        "    if denormalize:\n",
        "        x_pred = (x_pred + 1.0) / 2.0  # from [-1,1] → [0,1]\n",
        "\n",
        "    # Compute probabilities\n",
        "    classifier = MNISTClassifier().to(DEVICE)\n",
        "    classifier.load_state_dict(torch.load('./mnist_classifier_best.pth'))\n",
        "    classifier.eval()\n",
        "\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, num_images, batch_size):\n",
        "            logits = classifier(x_pred[i:i+batch_size])\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            preds.append(probs)\n",
        "    preds = torch.concat(preds, axis=0)\n",
        "\n",
        "    # Compute inception score\n",
        "    # Marginal distribution p(y)\n",
        "    p_y = preds.mean(dim=0, keepdim=True)  # [1, C]\n",
        "\n",
        "    # KL divergence: sum_y p(y|x) log (p(y|x)/p(y))\n",
        "    kl_div = preds * (torch.log(preds + eps) - torch.log(p_y + eps))\n",
        "    kl_div = kl_div.sum(dim=1)  # [N]\n",
        "\n",
        "    # Mean KL divergence → IS\n",
        "    return torch.exp(kl_div.mean()).item()"
      ],
      "metadata": {
        "id": "lQfcZbDHAIBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHTkEqtOHo2D"
      },
      "source": [
        "Now let's check the Inception Score for the GAN model we have trained before."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Inception Score for the trained GAN generator\n",
        "is_score = compute_inception_score(gan, gan.dim_latent)\n",
        "\n",
        "print(f\"Inception Score: {is_score:.4f}\")"
      ],
      "metadata": {
        "id": "z1UDZFIu_9cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCVwckWQLHno"
      },
      "source": [
        "# Conditional Generative Adversarial Networks (cGANs)\n",
        "\n",
        "A [Conditional GAN (cGAN)](https://arxiv.org/abs/1411.1784) is a variant of classical Generative Adversarial Networks, where the task is conditioned on some extra information instead of random noise. They were introduced in 2014, , and set a new state of the art in many image tasks.\n",
        "\n",
        "In this setting, cGANs expect an input image instead of a random noise vector. A simple example of image translation is the drawing to real object transformation. To do so, imagine that we have two different paired domains, images of the real objects (domain A) and drawing of objects (domain B), and we want to go from one to the other. In classical GANs, we used these two domains for training the architectures, but here we can also condition the input to one of the images in the domain B, and use the relationship between the domains to guide the network to the desired result:\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/rF2w5Tt/Screenshot-from-2019-02-25-15-47-18.png\" alt=\"Screenshot-from-2019-02-25-15-47-18\" border=\"0\"></a>\n",
        "\n",
        "As in regular GANs, the discriminator learns to classify between fake (synthesized by the generator) and real {drawing, photo} tuples. Hence, the generator will try to fool the discriminator with better and better synthesised images. In contrast to GANS, as we feed the drawing directly to the generator, the information that the network can use to seek convergence is much wider."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B3XO3spH0ou"
      },
      "source": [
        "## Image-to-image Translation with cGANs\n",
        "\n",
        "\n",
        "[Image-to-image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004) is a paper presented at the Computer Vision and Pattern Recognition (CVPR) conference in 2017. In their paper, the authors introduced Pix2pix, a conditional GAN, to transform images from one domain to another. The authors in the paper investigated conditional adversarial networks as a general-purpose solution to image-to-image translation problems, see some examples from the paper of different Image-to-Image tasks:\n",
        "\n",
        ">\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/Mf08787/examples.jpg)\n",
        "\n",
        "In this tutorial, we will explain and implement Pix2pix model for colouring images step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrEuqkIINKxi"
      },
      "source": [
        "## Colouring Black and White Images\n",
        "The tutorial aims to build the Pix2pix code for colouring images. We will use CIFAR10 dataset to perform the experiment, however, you could try to use any other dataset. For instance, check [this repo](https://github.com/kvfrans/deepcolor) which shows how to colour manga-style images.\n",
        "\n",
        "The idea is quite simple. First, we will transform images into black and white (BW) and train a generator that will transform them back to RGB images. As in classical GANs, we use the discriminator to differentiate between *Fake* and *Real* images.\n",
        "\n",
        "Thus, we have domain A (colour) and domain B (B&W)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a customized data loader that can load both coloured and grayscaled images."
      ],
      "metadata": {
        "id": "LH10bfRSii8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r7Y-RLnTlqw"
      },
      "outputs": [],
      "source": [
        "class Cifar(Dataset):\n",
        "    def __init__(self, name=\"CIFAR10\", root=\"./data\", train=True, download=True):\n",
        "        self.rgb_norm = transforms.Compose([\n",
        "            transforms.ToTensor(),                        # [0,1]\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # → [-1,1]\n",
        "        ])\n",
        "        self.bw_norm = transforms.Compose([\n",
        "            transforms.ToTensor(),                        # [0,1]\n",
        "            transforms.Normalize((0.5,), (0.5,))          # → [-1,1]\n",
        "        ])\n",
        "\n",
        "        if name == \"CIFAR10\":\n",
        "            self.dataset = datasets.CIFAR10(root=root, train=train, download=download)\n",
        "        elif name == \"CIFAR100\":\n",
        "            self.dataset = datasets.CIFAR100(root=root, train=train, download=download)\n",
        "        else:\n",
        "            raise ValueError(\"Dataset must be CIFAR10 or CIFAR100\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.dataset[idx]  # PIL image, int label\n",
        "\n",
        "        # RGB normalized [-1,1]\n",
        "        img_rgb = self.rgb_norm(img)\n",
        "\n",
        "        # Grayscale normalized [-1,1]\n",
        "        img_bw = transforms.functional.rgb_to_grayscale(img, num_output_channels=1)\n",
        "        img_bw = self.bw_norm(img_bw)\n",
        "\n",
        "        return img_rgb, img_bw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMe042E2cDH7"
      },
      "outputs": [],
      "source": [
        "# Check image size\n",
        "train_dataset = Cifar(train=True)\n",
        "img_rgb, img_bw = next(iter(train_dataset))\n",
        "\n",
        "print(\"RGB image shape:\", img_rgb.shape)   # (C, H, W)\n",
        "print(\"BW image shape:\", img_bw.shape)  # (C, H, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7JgJGacphw"
      },
      "source": [
        "We can visualise images from both domains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7Of7_EXLHSH"
      },
      "outputs": [],
      "source": [
        "# Change seed to see different samples\n",
        "set_seed(42)\n",
        "\n",
        "# Pick a random batch\n",
        "num_samples = 9\n",
        "indices = np.random.choice(len(train_dataset), size=num_samples, replace=False)\n",
        "samples = [train_dataset[i] for i in indices]\n",
        "imgs_rgb, imgs_bw = zip(*samples)\n",
        "\n",
        "imgs_rgb = torch.stack(imgs_rgb)\n",
        "imgs_bw = torch.stack(imgs_bw)\n",
        "\n",
        "# Convert tensors to numpy and permute dimensions for visualization\n",
        "imgs_rgb = imgs_rgb.permute(0, 2, 3, 1).cpu().numpy()  # NCHW -> NHWC\n",
        "imgs_bw = imgs_bw.permute(0, 2, 3, 1).cpu().numpy()      # NCHW -> NHWC\n",
        "\n",
        "# Denormalize from [-1,1] to [0,1]\n",
        "imgs_rgb = (imgs_rgb + 1) / 2\n",
        "imgs_bw = (imgs_bw + 1) / 2\n",
        "\n",
        "# Repeat grayscale channel to make it 3-channel for visualization\n",
        "imgs_bw = np.repeat(imgs_bw, 3, axis=3)\n",
        "\n",
        "# Plot comparison\n",
        "N = 3\n",
        "fig, axes = plt.subplots(N, N, figsize=(10, 10))\n",
        "plt.suptitle('Domain A (Color) VS Domain B (B&W)', fontsize=18)\n",
        "for row in range(N):\n",
        "    for col in range(N):\n",
        "        idx = row * N + col\n",
        "        combined = np.concatenate((imgs_rgb[idx], imgs_bw[idx]), axis=1)\n",
        "        axes[row, col].imshow(np.clip(combined, 0, 1))\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUIdgtbpUg6j"
      },
      "source": [
        "## Generator & Discriminator Models\n",
        "\n",
        "For many image translation problems, there is a great deal of low-level information shared between the input and output. Thus, it is desirable to shuttle this information directly across the net. For example, in the case of image colourization, the input and output share the location of prominent edges. To give the generator a means to circumvent the bottleneck for information like this, we use an architecture with skip connections, following the general shape of the UNet introduced in previous tutorials.\n",
        "\n",
        "The generator introduced in this tutorial is a simpler version of the one in the paper since we are using low-resolution images (32x32).\n",
        "\n",
        "In Pix2pix, the authors found that mixing the GAN objective with a more traditional loss, such as L1 loss, improved the final performance. The discriminator’s job remains unchanged, but the generator is tasked to not only fool the discriminator but also to be near the ground-truth output in an L1 sense.\n",
        "\n",
        "We can define our cGAN generator model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CGenerator(nn.Module):\n",
        "    def __init__(self, im_shape=(32, 32)):\n",
        "        super(CGenerator, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.drop4 = nn.Dropout2d(0.5)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "        self.conv5 = nn.Conv2d(512, 1024, 3, padding=1)\n",
        "        self.drop5 = nn.Dropout2d(0.5)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        self.up6 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(1024, 512, 3, padding=1)\n",
        "        )\n",
        "        self.conv6 = nn.Conv2d(1024, 512, 3, padding=1)  # Accounts for concatenation\n",
        "\n",
        "        self.up7 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(512, 256, 3, padding=1)\n",
        "        )\n",
        "        self.conv7 = nn.Conv2d(512, 256, 3, padding=1)\n",
        "\n",
        "        self.up8 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(256, 128, 3, padding=1)\n",
        "        )\n",
        "        self.conv8 = nn.Conv2d(256, 128, 3, padding=1)\n",
        "\n",
        "        self.up9 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, padding=1)\n",
        "        )\n",
        "        self.conv9 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "\n",
        "        self.conv10 = nn.Conv2d(64, 3, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        conv1 = F.relu(self.conv1(x))\n",
        "        pool1 = self.pool1(conv1)\n",
        "        conv2 = F.relu(self.conv2(pool1))\n",
        "        pool2 = self.pool2(conv2)\n",
        "        conv3 = F.relu(self.conv3(pool2))\n",
        "        pool3 = self.pool3(conv3)\n",
        "        conv4 = F.relu(self.conv4(pool3))\n",
        "        drop4 = self.drop4(conv4)\n",
        "        pool4 = self.pool4(drop4)\n",
        "        conv5 = F.relu(self.conv5(pool4))\n",
        "        drop5 = self.drop5(conv5)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        up6 = self.up6(drop5)\n",
        "        merge6 = torch.cat([drop4, up6], dim=1)\n",
        "        conv6 = F.relu(self.conv6(merge6))\n",
        "\n",
        "        up7 = self.up7(conv6)\n",
        "        merge7 = torch.cat([conv3, up7], dim=1)\n",
        "        conv7 = F.relu(self.conv7(merge7))\n",
        "\n",
        "        up8 = self.up8(conv7)\n",
        "        merge8 = torch.cat([conv2, up8], dim=1)\n",
        "        conv8 = F.relu(self.conv8(merge8))\n",
        "\n",
        "        up9 = self.up9(conv8)\n",
        "        merge9 = torch.cat([conv1, up9], dim=1)\n",
        "        conv9 = F.relu(self.conv9(merge9))\n",
        "\n",
        "        conv10 = self.conv10(conv9)\n",
        "\n",
        "        return torch.tanh(conv10)  # Output in [-1, 1] range\n",
        "\n",
        "torchinfo.summary(CGenerator(), input_size=(1, 1, 32, 32))"
      ],
      "metadata": {
        "id": "Pq3YbFltJHM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkhWs2Fwztka"
      },
      "source": [
        "Next, we follow the discriminator model definition from the paper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usNum3W7zvar"
      },
      "outputs": [],
      "source": [
        "class CDiscriminator(nn.Module):\n",
        "    def __init__(self, im_shape=(32, 32)):\n",
        "        super(CDiscriminator, self).__init__()\n",
        "        self.im_shape = im_shape\n",
        "\n",
        "        # Define the model architecture\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(4, 64, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.BatchNorm2d(128, momentum=0.8),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
        "            nn.BatchNorm2d(256, momentum=0.8),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256 * (im_shape[0]//8) * (im_shape[1]//8), 1024),\n",
        "            nn.Linear(1024, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, img_rgb, img_bw):\n",
        "        # Concatenate inputs along channel dimension\n",
        "        x = torch.cat((img_rgb, img_bw), dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "torchinfo.summary(CDiscriminator(), input_data=(torch.randn(1, 3, 32, 32), torch.randn(1, 1, 32, 32)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we can define a simple wrapper that combines the two."
      ],
      "metadata": {
        "id": "DQtl0bJ_piiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CGAN(nn.Module):\n",
        "    def __init__(self, img_shape=(32, 32)):\n",
        "        super().__init__()\n",
        "        self.img_shape = img_shape\n",
        "        self.generator = CGenerator(self.img_shape)\n",
        "        self.discriminator = CDiscriminator(self.img_shape)\n",
        "\n",
        "    def generate(self, img):\n",
        "        return self.generator(img)\n",
        "\n",
        "    def discriminate(self, img_rgb, img_bw):\n",
        "        return self.discriminator(img_rgb, img_bw)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.generator(img)\n",
        "\n",
        "\n",
        "torchinfo.summary(CGAN(), input_size=(1, 1, 32, 32))"
      ],
      "metadata": {
        "id": "IhGzWAFcpnCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etn3wYwi8A2k"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "The training of cGAN is very similar to the vanilla GAN, where we separately train the discriminator and generator. Don't worry too much about this part as the code is provided in case you want to optionally train your own cGANs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional Discriminator Training\n",
        "def train_conditional_discriminator(img_rgb, img_bw, cgan, optimizer_d, criterion):\n",
        "    # Generate labels\n",
        "    batch_size = img_rgb.size(0)\n",
        "    label_real = create_real_label(batch_size)\n",
        "    label_fake = create_fake_label(batch_size)\n",
        "\n",
        "    # Generate fake color image\n",
        "    img_rgb_generated = cgan.generate(img_bw).detach()  # Detach() is required as only the discriminator is updated in this case\n",
        "\n",
        "    # Discriminate on real and fake pairs\n",
        "    optimizer_d.zero_grad()\n",
        "\n",
        "    label_real_predicted = cgan.discriminate(img_rgb, img_bw)\n",
        "    loss_real = criterion(label_real_predicted, label_real)\n",
        "\n",
        "    label_fake_predicted = cgan.discriminate(img_rgb_generated, img_bw)\n",
        "    loss_fake = criterion(label_fake_predicted, label_fake)\n",
        "\n",
        "    # Backpropagate and update\n",
        "    loss = (loss_real + loss_fake) / 2\n",
        "    loss.backward()\n",
        "    optimizer_d.step()\n",
        "\n",
        "    # Get accuracy\n",
        "    real_acc = torch.mean((label_real_predicted > 0.5).float())\n",
        "    fake_acc = torch.mean((label_fake_predicted < 0.5).float())\n",
        "    acc = 0.5 * (real_acc + fake_acc)\n",
        "\n",
        "    return loss.item(), acc.item()"
      ],
      "metadata": {
        "id": "N3px2DQZujUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional Generator Training\n",
        "def train_conditional_generator(img_rgb, img_bw, cgan, optimizer_g, criterion_gan, criterion_l1, lambda_l1=100):\n",
        "    # Generate labels\n",
        "    batch_size = img_rgb.size(0)\n",
        "    label_real = create_real_label(batch_size)\n",
        "\n",
        "    # Train generator\n",
        "    optimizer_g.zero_grad()\n",
        "    ## Generate fake images\n",
        "    img_rgb_generated = cgan.generate(img_bw)\n",
        "\n",
        "    ## Use discriminator to judge whether the generated images are real or not\n",
        "    label_predicted = cgan.discriminate(img_rgb_generated, img_bw)\n",
        "    loss_gan = criterion_gan(label_predicted, label_real)\n",
        "\n",
        "    ## Add reconstruction loss\n",
        "    loss_l1 = criterion_l1(img_rgb_generated, img_rgb)\n",
        "\n",
        "    ## Total loss\n",
        "    loss = loss_gan + lambda_l1 * loss_l1\n",
        "\n",
        "    ## Backpropagate and update\n",
        "    loss.backward()\n",
        "    optimizer_g.step()  # Discriminator is not updated as optimizer_g doesn't contain its parameters\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "8mjOrAin-nom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNzo7ijs2THo"
      },
      "source": [
        "We can also define some helper functions to visualize images during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpZvidvfSdrC"
      },
      "outputs": [],
      "source": [
        "def show_colored_images(img_rgb, img_bw, img_rgb_generated):\n",
        "    # Convert tensors to numpy and denormalize from [-1,1] → [0,1]\n",
        "\n",
        "    # Grayscale input (BW)\n",
        "    img_bw_np = img_bw[0].cpu().permute(1, 2, 0).numpy()\n",
        "    img_bw_np = (img_bw_np * 0.5 + 0.5).squeeze(-1)  # (H,W)\n",
        "\n",
        "    # Fake RGB (detach to avoid gradients)\n",
        "    fake_rgb_np = img_rgb_generated[0].detach().cpu().permute(1, 2, 0).numpy()\n",
        "    fake_rgb_np = (fake_rgb_np * 0.5 + 0.5)  # (H,W,3)\n",
        "\n",
        "    # Real RGB\n",
        "    real_rgb_np = img_rgb[0].cpu().permute(1, 2, 0).numpy()\n",
        "    real_rgb_np = (real_rgb_np * 0.5 + 0.5)  # (H,W,3)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(131)\n",
        "    plt.imshow(img_bw_np, cmap=\"gray\")\n",
        "    plt.title(\"Input (BW)\", fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(132)\n",
        "    plt.imshow(fake_rgb_np)\n",
        "    plt.title(\"Generated (Fake RGB)\", fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(133)\n",
        "    plt.imshow(real_rgb_np)\n",
        "    plt.title(\"Ground Truth (Real RGB)\", fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def show_colored_two_models(img_rgb, img_bw, img_rgb_generated_mae, img_rgb_generated_cgan):\n",
        "    # --- Convert tensors to numpy and denormalize [-1,1] → [0,1] ---\n",
        "\n",
        "    # Grayscale input (BW)\n",
        "    img_bw_np = img_bw[0].cpu().permute(1, 2, 0).numpy()\n",
        "    img_bw_np = (img_bw_np * 0.5 + 0.5).squeeze(-1)  # (H,W)\n",
        "\n",
        "    # Fake RGB from MAE-only model\n",
        "    fake_rgb_mae_np = img_rgb_generated_mae[0].detach().cpu().permute(1, 2, 0).numpy()\n",
        "    fake_rgb_mae_np = (fake_rgb_mae_np * 0.5 + 0.5)\n",
        "\n",
        "    # Fake RGB from cGAN model\n",
        "    fake_rgb_cgan_np = img_rgb_generated_cgan[0].detach().cpu().permute(1, 2, 0).numpy()\n",
        "    fake_rgb_cgan_np = (fake_rgb_cgan_np * 0.5 + 0.5)\n",
        "\n",
        "    # Real RGB\n",
        "    real_rgb_np = img_rgb[0].cpu().permute(1, 2, 0).numpy()\n",
        "    real_rgb_np = (real_rgb_np * 0.5 + 0.5)\n",
        "\n",
        "    # --- Plot ---\n",
        "    plt.figure(figsize=(20, 5))\n",
        "\n",
        "    plt.subplot(141)\n",
        "    plt.imshow(img_bw_np, cmap=\"gray\")\n",
        "    plt.title(\"Input (BW)\", fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(142)\n",
        "    plt.imshow(fake_rgb_mae_np)\n",
        "    plt.title(\"Generated (MAE)\", fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(143)\n",
        "    plt.imshow(fake_rgb_cgan_np)\n",
        "    plt.title(\"Generated (cGAN)\", fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(144)\n",
        "    plt.imshow(real_rgb_np)\n",
        "    plt.title(\"Ground Truth (RGB)\", fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxLWgeEV5dT5"
      },
      "source": [
        "Let's see the training code. Feel free to skip this part and use the provided pre-trained weights."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Training Hyperparameters\n",
        "img_shape = (32, 32)\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "lr = 2.0e-4\n",
        "betas = (0.5, 0.999)\n",
        "lambda_l1 = 100\n",
        "\n",
        "# Data\n",
        "train_dataset = Cifar('CIFAR10', train=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model and optimizer\n",
        "cgan = CGAN(img_shape).to(DEVICE)\n",
        "optimizer_g = optim.Adam(cgan.generator.parameters(), lr=lr, betas=betas)\n",
        "optimizer_d = optim.Adam(cgan.discriminator.parameters(), lr=lr, betas=betas)\n",
        "cgan.train()\n",
        "\n",
        "# Loss functions\n",
        "criterion_gan = nn.MSELoss()  # For adversarial loss\n",
        "criterion_l1 = nn.L1Loss()    # For pixel-wise loss\n",
        "\n",
        "# Visualization\n",
        "plot_out = widgets.Output()\n",
        "display(plot_out)\n",
        "\n",
        "def update_plots(img_rgb, img_bw, cgan, iteration=None, save_path=\"./images.png\"):\n",
        "    \"\"\"\n",
        "    Update in-place visualization of Pix2Pix results.\n",
        "    Shows: Input BW | Generated RGB | Real RGB\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        img_rgb_generated = cgan.generate(img_bw)\n",
        "\n",
        "    # Convert tensors → numpy [0,1]\n",
        "    bw_np = (img_bw[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5).squeeze(-1)\n",
        "    fake_np = (img_rgb_generated[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
        "    real_np = (img_rgb[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
        "\n",
        "    # Update output widget in place\n",
        "    with plot_out:\n",
        "        plot_out.clear_output(wait=True)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "        axes[0].imshow(bw_np, cmap=\"gray\")\n",
        "        axes[0].set_title(\"Input (BW)\")\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        axes[1].imshow(fake_np)\n",
        "        axes[1].set_title(\"Generated (RGB)\")\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        axes[2].imshow(real_np)\n",
        "        axes[2].set_title(\"Ground Truth (RGB)\")\n",
        "        axes[2].axis(\"off\")\n",
        "\n",
        "        if iteration is not None:\n",
        "            plt.suptitle(f\"Iteration {iteration}\", fontsize=16)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path)\n",
        "        plt.show()\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "# Run training\n",
        "for epoch in range(num_epochs):\n",
        "    g_losses, d_losses, d_accs = [], [], []\n",
        "\n",
        "    pbar = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    for i, (imgs_rgb, imgs_bw) in enumerate(pbar):\n",
        "        imgs_rgb, imgs_bw = imgs_rgb.to(DEVICE), imgs_bw.to(DEVICE)\n",
        "\n",
        "        # --- Train Discriminator ---\n",
        "        d_loss, d_acc = train_conditional_discriminator(\n",
        "            imgs_rgb, imgs_bw, cgan, optimizer_d, criterion_gan)\n",
        "\n",
        "        # --- Train Generator ---\n",
        "        g_loss = train_conditional_generator(\n",
        "            imgs_rgb, imgs_bw, cgan, optimizer_g, criterion_gan, criterion_l1, lambda_l1)\n",
        "\n",
        "        # Record metrics\n",
        "        d_losses.append(d_loss)\n",
        "        d_accs.append(d_acc)\n",
        "        g_losses.append(g_loss)\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            \"D_loss\": f\"{np.mean(d_losses):.4f}\",\n",
        "            \"D_acc\": f\"{100*np.mean(d_accs):.1f}%\",\n",
        "            \"G_loss\": f\"{np.mean(g_losses):.4f}\"\n",
        "        })\n",
        "\n",
        "        # Show some images occasionally\n",
        "        if i % 50 == 0:\n",
        "            update_plots(imgs_rgb, imgs_bw, cgan, iteration=epoch*len(train_loader) + i)"
      ],
      "metadata": {
        "id": "Ljuh5h-5KW0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8aVvc8k2_NP"
      },
      "source": [
        "## Colouring Test Images\n",
        "\n",
        "We are ready to visualise how the network colours the test images."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/MatchLab-Imperial/deep-learning-course/master/asset/07_VAE_GAN/cgan_cifar10_epoch20.pth -O cgan_cifar10_epoch20.pth"
      ],
      "metadata": {
        "id": "ah9iKt0PO-Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2ExvhyT5Oxh"
      },
      "outputs": [],
      "source": [
        "# Load model from saved file\n",
        "cgan = CGAN((32, 32)).to(DEVICE)\n",
        "weight = torch.load('./cgan_cifar10_epoch20.pth')\n",
        "cgan.load_state_dict(weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apZzJgmM-2oD"
      },
      "source": [
        "Let's see the results, you can rerun the following code for visualizing more examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0ljS-uv1jMI"
      },
      "outputs": [],
      "source": [
        "# Change seed here to see different samples\n",
        "set_seed(42)\n",
        "\n",
        "# Data\n",
        "test_dataset = Cifar('CIFAR10', train=False)\n",
        "\n",
        "# Randomly sample indices for visualization\n",
        "num_samples = 3\n",
        "indices = np.random.choice(len(test_dataset), size=num_samples, replace=False)\n",
        "\n",
        "cgan.eval()\n",
        "with torch.no_grad():  # Disable gradient calculation for inference\n",
        "    for idx in indices:\n",
        "        # Get random batch from test set\n",
        "        imgs_real, imgs_bw = test_dataset[idx]\n",
        "        imgs_real = imgs_real.unsqueeze(0).to(DEVICE)\n",
        "        imgs_bw = imgs_bw.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        imgs_real_generated = cgan.generate(imgs_bw)\n",
        "\n",
        "        # Show results\n",
        "        show_colored_images(imgs_real, imgs_bw, imgs_real_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QfiXDhB3TSi"
      },
      "source": [
        "The colouring may not be perfect, but the network does a good job at guessing plausible colours. We have shown a simplified version of the Pix2pix method. Training GANs is a hard task, and there are many elements that could be added to improve results. For instance, [this repo](https://github.com/soumith/ganhacks), like many others, shows common tricks to improve GAN performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcG_R3OAGe5m"
      },
      "source": [
        "# Coursework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpQekp8aNVt_"
      },
      "source": [
        "\n",
        "## Task 1: MNIST generation using VAE and GAN\n",
        "\n",
        "**Report**\n",
        "* Train the given VAE model in the tutorial also using the same hyper parameters (batch size, optimizer, number of epochs, etc...) but increasing the latent dimensionality to 10. Compute the MSE on the reconstructed `x_test` images and Inception Score (IS). Now train the same model without the KL divergence loss, and compute again the MSE and IS score. Report the results in a table and discuss them. For the IS you can use in this case `compute_inception_score(model, 10, denormalize=False)`.\n",
        "\n",
        "* Train the GAN given in the tutorial with increased dimensionality of the initial random sample to 10 for 10 epochs and report its IS (use the same table as in the VAE case). Discuss the difference in the obtained IS for VAE and GAN and link it to the qualitative results. For the IS use in this case `compute_inception_score(model, 10, denormalize=True)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKkjwCF6uw3K"
      },
      "source": [
        "## Task 2: Quantitative VS Qualitative Results\n",
        "\n",
        "In this task, we will observe the difference between two trained models for colouring images. One is the model trained during the tutorial, which uses a cGAN approach to predict the RGB pixel-wise values of a B&W image. The other one is a simple UNet autoencoder trained with a Mean Absolute Error (MAE) loss, which is trained to predict directly the RBG image without any GAN based learning strategy. We refer to the first and second models as cGAN and MAE models, respectively. For this task, 20 epochs trained weights for the cGAN and MAE models are provided. If desired, the code to train the MAE model can be found below:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Training Hyperparameters\n",
        "img_shape = (32, 32)\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "lr = 2.0e-4\n",
        "betas = (0.5, 0.999)\n",
        "\n",
        "# Data\n",
        "train_dataset = Cifar('CIFAR10', train=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model, optimizer and criterion (MAE)\n",
        "generator_mae = CGenerator().to(DEVICE)\n",
        "optimizer_mae = torch.optim.Adam(generator_mae.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "criterion_mae = nn.L1Loss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    g_avg_loss = []\n",
        "\n",
        "    for batch_i, (imgs_rgb, imgs_bw) in enumerate(train_loader):\n",
        "        # Move data to device\n",
        "        imgs_rgb, imgs_bw = imgs_rgb.to(DEVICE), imgs_bw.to(DEVICE)\n",
        "\n",
        "        # Generate fake rgb images\n",
        "        optimizer_mae.zero_grad()\n",
        "        imgs_rgb_generated = generator_mae(imgs_bw)\n",
        "        g_loss = criterion_mae(imgs_rgb_generated, imgs_rgb)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        g_loss.backward()\n",
        "        optimizer_mae.step()\n",
        "\n",
        "        g_avg_loss.append(g_loss.item())\n",
        "\n",
        "        # Plot examples\n",
        "        if batch_i % 50 == 0:\n",
        "            with torch.no_grad():\n",
        "                show_colored_images(imgs_rgb, imgs_bw, imgs_rgb_generated)\n",
        "\n",
        "        # Print progress\n",
        "        if batch_i % 10 == 0:\n",
        "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {batch_i}/{len(train_loader)}] \"\n",
        "                  f\"[G loss: {np.mean(g_avg_loss):.4f}] \")\n",
        "\n",
        "# Save model\n",
        "torch.save(generator_mae.state_dict(), 'generator_mae.pth')"
      ],
      "metadata": {
        "id": "cTojDadnariK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9aTGM7qwtyO"
      },
      "source": [
        "Instead of training the models, we can directly load their pre-trained weights by running:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/MatchLab-Imperial/deep-learning-course/raw/master/asset/07_VAE_GAN/cgan_cifar10_epoch20.pth\n",
        "!wget https://github.com/MatchLab-Imperial/deep-learning-course/raw/master/asset/07_VAE_GAN/cgenerator_mae_cifar10_epoch20.pth"
      ],
      "metadata": {
        "id": "eNkt5HIHBM6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-trained CGAN\n",
        "generator_cGAN = CGAN().to(DEVICE)\n",
        "generator_cGAN.load_state_dict(torch.load('cgan_cifar10_epoch20.pth'))\n",
        "generator_cGAN.eval()\n",
        "\n",
        "# Pre-trained MAE model\n",
        "generator_mae = CGenerator().to(DEVICE)\n",
        "generator_mae.load_state_dict(torch.load('cgenerator_mae_cifar10_epoch20.pth'))\n",
        "generator_mae.eval()"
      ],
      "metadata": {
        "id": "9w71OzOuaMmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyXI9hCTzUn2"
      },
      "source": [
        "We have loaded both models, and we are ready to compare them. In this task, you are asked to analyse the difference between the quantitative versus the qualitative results. To do so, we provided two pieces of code. The first one will compute the MAE metric for both models in the test dataset. As we know, this metric is widely used on image generation tasks, such as image upsampling, image reconstruction, image translation, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t46neFcx6nyg"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "test_dataset = Cifar('CIFAR10', train=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Loss\n",
        "l1_mae, l1_cgan = [], []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for imgs_rgb, imgs_bw in test_loader:\n",
        "        imgs_rgb, imgs_bw = imgs_rgb.to(DEVICE), imgs_bw.to(DEVICE)\n",
        "        imgs_rgb_generated_mae = generator_mae(imgs_bw)\n",
        "        imgs_rgb_generated_cgan = generator_cGAN(imgs_bw)\n",
        "\n",
        "        l1_mae.append(F.l1_loss(imgs_rgb_generated_mae, imgs_rgb).item())\n",
        "        l1_cgan.append(F.l1_loss(imgs_rgb_generated_cgan, imgs_rgb).item())\n",
        "\n",
        "print(\"MAE (Trained MAE): {:.4f}\".format(np.mean(l1_mae)))\n",
        "print(\"MAE (Trained cGAN): {:.4f}\".format(np.mean(l1_cgan)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddZgs-Il8F9B"
      },
      "source": [
        "The next piece of code will show coloured examples for both networks, so you can check them visually and discuss which model is better. First, we need to create an iterator object to go through the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change seed and num_samples here to see different samples\n",
        "set_seed(42)\n",
        "num_samples = 3\n",
        "\n",
        "# Data\n",
        "test_dataset = Cifar('CIFAR10', train=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    img_rgb, img_bw = next(iter(test_loader))\n",
        "    img_rgb, img_bw = img_rgb.to(DEVICE), img_bw.to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_rgb_generated_mae = generator_mae(img_bw)\n",
        "        img_rgb_generated_cgan = generator_cGAN(img_bw)\n",
        "        show_colored_two_models(img_rgb, img_bw, img_rgb_generated_mae, img_rgb_generated_cgan)"
      ],
      "metadata": {
        "id": "JuPRF6DZeHyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss_1bTVE-Us_"
      },
      "source": [
        "We showed that both models obtain a similar MAE value. If we would only take into account the quantitative metric, as done in many scientific articles, we would say that the MAE model is better. However, in addition to the quantitative results, we need to analyse visually the results produced by the two networks to declare which is the best model.\n",
        "\n",
        "**Report**\n",
        "\n",
        "\n",
        "*   Run the previous code to analyse several coloured images for both models. Based on previous results and linked to GAN theory, discuss from the numerical and visual perspective if both models are similar, or whether there is a better one. You can provide in the report visual examples together with their MAE values to support your arguments. The figure of this task can be included in the Appendix. Discussion still needs to go into the main text."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}