{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/06_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk9T6yIbKYmG"
      },
      "source": [
        "# Representation Learning\n",
        "\n",
        "In this notebook we will train a network that will produce a good feature representation for the MNIST images. In this case, however, we will not use the labels available in the dataset to learn a label predictor $g$. We will approach the problem in an unsupervised manner by using the autoencoder method explained in the lecture. Hence, given an image $x$ from MNIST, we will encode it (using an encoder $\\phi$) to a lower dimensionality vector $\\phi(x)$, for example using only 2 values. Then, we will decode the lower dimensionality vector using a decoder $\\psi$, and then minimize a reconstruction error. In this notebook, we will use the $l_2$-norm as reconstruction metric: we will minimize $||x - \\psi(\\phi(x))||^2$. Usually, the encoder $\\phi$ and the decoder $\\psi$ have a mirrored architecture. The following figure from the slides summarizes the idea.\n",
        "\n",
        "<a href=\"https://ibb.co/0hhQ3Zt\"><img src=\"https://i.ibb.co/6ggN5bB/Screenshot-from-2019-02-14-14-49-24.png\" alt=\"Screenshot-from-2019-02-14-14-49-24\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "oTMubcCJsBTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchinfo\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    num_epochs: int = 10,\n",
        "    val_loader: DataLoader = None,\n",
        "    device: torch.device = DEVICE,\n",
        "):\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * x.size(0)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for x, y in val_loader:\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    y_pred = model(x)\n",
        "                    loss = criterion(y_pred, y)\n",
        "                    val_loss += loss.item() * x.size(0)\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "        else:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device = DEVICE,\n",
        "    desc: str = \"Test\"\n",
        ") -> float:\n",
        "    model = model.to(DEVICE)\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in data_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    print(f\"{desc} Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Ws45HmX1sHYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhmrfQ9cznwu"
      },
      "source": [
        "**Loading the dataset**\n",
        "\n",
        "As usual, we load the dataset and import some necessary modules. Both encoder $\\phi$ and decoder $\\psi$ will use `Dense` layers (with some activation functions in some cases), so we load the data in an array form in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZhAkVkgKFdl"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.MNIST(root='.', train=True, transform=transform, download=True)\n",
        "test_dataset  = datasets.MNIST(root='.', train=False, transform=transform, download=True)\n",
        "\n",
        "# Extract data and build TensorDatasets for autoencoder\n",
        "x_train = torch.stack([d[0] for d in train_dataset])  # [60000, 784]\n",
        "x_test  = torch.stack([d[0] for d in test_dataset])   # [10000, 784]\n",
        "\n",
        "y_train = train_dataset.targets.numpy()\n",
        "y_test = test_dataset.targets.numpy()\n",
        "\n",
        "train_tensor_dataset = TensorDataset(x_train, x_train)\n",
        "train_tensor_dataset, val_tensor_dataset = random_split(\n",
        "    train_tensor_dataset,\n",
        "     [50000, 10000],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")  # Extract val dataset from the train set\n",
        "test_tensor_dataset  = TensorDataset(x_test, x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUTAOhYZ01OF"
      },
      "source": [
        "## Linear Autoencoder\n",
        "In our first approach we do not use any activation function, hence the model is completely linear. We encode the input flattened image of dimensionality $784$ ($28\\times28$) in a vector of 2 dimensions, and then decode it back to the $784$ vector. Even though the design autoencoder is of low capacity and the representation is constrained to dimensionality 2, we hope to see some meaningful features. To build our linear autoencoder, we first design a linear model using a couple of `Linear` layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iymquswfK56k"
      },
      "source": [
        "class LinearAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Linear(784, 2)\n",
        "        self.decoder = nn.Linear(2, 784)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        out = self.decoder(z)\n",
        "        return out\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = LinearAutoencoder()\n",
        "print(torchinfo.summary(model, input_size=(1, 784)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_loader = DataLoader(train_tensor_dataset, batch_size=128, shuffle=True),\n",
        "    val_loader = DataLoader(val_tensor_dataset, batch_size=128),\n",
        "    criterion = nn.MSELoss(),\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3),\n",
        "    num_epochs = 10,\n",
        ")"
      ],
      "metadata": {
        "id": "3GtC5KakVaNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkTUh7J6kSea"
      },
      "source": [
        "Now, we will see if we have learnt a good representation of the images by using this linear autoencoder. First, let's check the MSE in the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugLOZ0ev53QJ"
      },
      "source": [
        "evaluate(model, DataLoader(test_tensor_dataset, batch_size=128), nn.MSELoss())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkrpslwJ585S"
      },
      "source": [
        "We have an MSE of around 0.058 in the test set. It may be hard to know if this value means a good reconstruction or not, but we can use the value to compare the performance of our linear autoencoder to other models. To have a better understanding of what this value means qualitatively, let's plot some images, along with the corresponding MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jBf8DAg5i6i"
      },
      "source": [
        "def plot_recons_original(image, label, model, size_image=(28, 28)):\n",
        "    if isinstance(image, np.ndarray):\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "\n",
        "    if image.ndim == 1:\n",
        "        image = image.unsqueeze(0)  # [784] → [1, 784]\n",
        "    elif image.ndim == 2 and image.shape == size_image:\n",
        "        image = image.view(1, -1)   # [28, 28] → [1, 784]\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        reconst = model(image.to(DEVICE))\n",
        "\n",
        "    # Back to CPU numpy for plotting\n",
        "    image_np = image.cpu().numpy().reshape(size_image)\n",
        "    reconst_np = reconst.cpu().numpy().reshape(size_image)\n",
        "\n",
        "    # Compute MSE\n",
        "    mse = np.mean((image_np - reconst_np) ** 2)\n",
        "\n",
        "    # Plot original vs reconstruction\n",
        "    plt.subplots(1, 2)\n",
        "    ax = plt.subplot(121)\n",
        "    plt.imshow(image_np, cmap='gray')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(f\"Original (label={label})\")\n",
        "\n",
        "    ax = plt.subplot(122)\n",
        "    plt.imshow(reconst_np, cmap='gray')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(f\"Reconstruction\\nMSE: {mse:.2f}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Select a random index from the test set\n",
        "idx = np.random.randint(x_test.shape[0] - 1)\n",
        "plot_recons_original(x_test[idx], y_test[idx], model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olPiOwYIh5Qz"
      },
      "source": [
        "We see how the reconstructed images look in most cases blurry, due to the autoencoder $\\psi(\\phi(x))$ not having enough representation capacity. In Task 1, you will try to improve the model by adding some non-linearities/more layers. Now, let's check how the feature vectors of the simple linear autoencoder we just trained are distributed in the representation space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ksfLJJ55j8F"
      },
      "source": [
        "## Representation Space\n",
        "If we have a good feature representation, we hope to see in the representation space clusters of images sharing visual similarities in the image space. For example, images from the same class should be close to each other as they are visually similar.\n",
        "\n",
        "To check how the features are distributed in the representation space, we forward the images from the test set through the encoder part of our PyTorch model to retrieve the representation $\\phi(x)$. As the vector only has dimensionality 2, we do not need to apply any dimensionality reduction technique to plot it.\n",
        "\n",
        "We first compute the representation of the images from the MNIST test set using the `predict_representation` function. Since our encoder is implemented as a separate layer in the `LinearAutoencoder` class, we can directly call `model.encode(x)` to get the representation vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2ddujlKlxHi"
      },
      "source": [
        "def predict_representation(model, data, batch_size=256):\n",
        "    \"\"\"\n",
        "    Compute representation vectors φ(x) using the encoder part of the model.\n",
        "    \"\"\"\n",
        "    model = model.to(DEVICE)\n",
        "    model.eval()\n",
        "    representations = []\n",
        "\n",
        "    data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            encoded = model.encode(batch.to(DEVICE))\n",
        "            representations.append(encoded.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(representations, axis=0)\n",
        "\n",
        "# Compute representations for the test set\n",
        "representation = predict_representation(model, x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ-80Cn32x5b"
      },
      "source": [
        "Now, we plot the computed representation for the test images in a 2D scatter plot. We also assign in the scatter plot different colours to the points depending on their class label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRCerZWnmRKi"
      },
      "source": [
        "def plot_representation_label(representation, labels, plot3d=0):\n",
        "  ## Function used to plot the representation vectors and assign different\n",
        "  ## colors to the different classes\n",
        "\n",
        "  # First create the figure\n",
        "  fig, ax = plt.subplots(figsize=(10,6))\n",
        "  # In case representation dimension is 3, we can plot in a 3d projection too\n",
        "  if plot3d:\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "  # Check number of labels to separate by colors\n",
        "  n_labels = labels.max() + 1\n",
        "  # Color map, and give different colors to every label\n",
        "  cm = plt.get_cmap('gist_rainbow')\n",
        "  ax.set_prop_cycle(color=[cm(1.*i/(n_labels)) for i in range(n_labels)])\n",
        "  # Loop is to plot different color for each label\n",
        "  for l in range(n_labels):\n",
        "    # Only select indices for corresponding label\n",
        "    ind = labels == l\n",
        "    if plot3d:\n",
        "      ax.scatter(representation[ind, 0], representation[ind, 1],\n",
        "                 representation[ind, 2], label=str(l))\n",
        "    else:\n",
        "      ax.scatter(representation[ind, 0], representation[ind, 1], label=str(l))\n",
        "  ax.legend()\n",
        "  plt.title('Features in the representation space with corresponding label')\n",
        "  plt.show()\n",
        "  return fig, ax\n",
        "\n",
        "plot_representation_label(representation, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whLoA1UcNPjK"
      },
      "source": [
        "The scatter plot shows that our linear autoencoder generates representations that are mostly clustered by class. However, some of the classes seem to overlap with each other, and some classes are more clustered than others. For example, the class `1`is well clustered, but the class `9` seems to overlap greatly with the class `7`.  The class `8` is also quite spread out, with some of the features closer to the features of the class `1`, some others closer to the class `6` or `0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsiHWhfUqJSO"
      },
      "source": [
        "## Clustering the Data\n",
        "We have learnt a linear autoencoder capable of producing meaningful representations, and we also plotted it in the last section. We saw how the representations for the different images sharing the same label were close to each other without actually using the label information in the training process. Now, let's do a quick experiment to see how well clustered are the features. We will use a clustering method on these features, and then we will assign each of the clusters to the majority class (i.e., the most represented class in the cluster). We want to check what kind of accuracy we would have using this simple classification method as a way to understand how well clustered they are.\n",
        "\n",
        "First, let's now cluster the features using a standard technique called K-Means. A good guess is to use the same number of clusters as classes, in this case we use 10 clusters. Then, as we mentioned, assign each of the clusters to the majority class and we compute the accuracy of this method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy1y8gyKqkfm"
      },
      "source": [
        "def cluster_plot_data(representation, use_gmm=0, random_state=42, plot=1):\n",
        "  from sklearn.cluster import KMeans\n",
        "  from sklearn import mixture\n",
        "\n",
        "\n",
        "  # Set number of clusters to 10\n",
        "  n_clusters = 10\n",
        "  if use_gmm:\n",
        "    c_pred = mixture.GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=random_state).fit_predict(representation)\n",
        "  else:\n",
        "    c_pred = KMeans(n_clusters=n_clusters, random_state=random_state).fit_predict(representation)\n",
        "  if plot:\n",
        "    _, ax = plt.subplots(1,1)\n",
        "    # Color map, and give different colors to every label\n",
        "    cm = plt.get_cmap('gist_rainbow')\n",
        "    ax.set_prop_cycle(color=[cm(1.*i/(n_clusters)) for i in range(n_clusters)])\n",
        "    # Loop is to plot different color for each label\n",
        "    for c in range(n_clusters):\n",
        "      # Only select indices for corresponding label\n",
        "      ind = c_pred == c\n",
        "      ax.scatter(representation[ind, 0], representation[ind, 1], label=str(c))\n",
        "    ax.legend()\n",
        "    plt.title('Clustered features in the representation space')\n",
        "    plt.show()\n",
        "  return c_pred\n",
        "\n",
        "c_pred = cluster_plot_data(representation)\n",
        "\n",
        "# Compute accuracy by checking cluster by cluster the majority class and\n",
        "# assigning all of the data points in that cluster to the majority class\n",
        "# Then we check the accuracy of doing so\n",
        "correct = 0\n",
        "for i in range(10):\n",
        "  indices_c_pred = c_pred == i\n",
        "  classes = y_test[indices_c_pred]\n",
        "  counts = np.bincount(classes)\n",
        "  class_max = np.argmax(counts)\n",
        "  correct += (classes == class_max).sum()\n",
        "\n",
        "print('Accuracy: {:.3f}'.format(correct/(1.0*y_test.shape[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgwBH2M0pYcI"
      },
      "source": [
        "Even though it is not a good way to classify the samples compared to the supervised case that you have done in past tutorials, the accuracy is still quite higher than random guess, which would be around ~10%. An autoencoder of higher capacity (e.g. using more layers, including activation functions or increasing the size of the representation layer) would likely produce representations with better accuracy using this clustering method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbZcFbHlHX4s"
      },
      "source": [
        "## Relationship to PCA\n",
        "\n",
        "When introducing autoencoders, it is usually mentioned that a linear autoencoder is closely related to PCA, i.e. using an autoencoder with representation dimensionality of $d$ is equivalent to finding the first $d$ principal components. Let's test it by computing the PCA of the train set, and applying it to the test set. Then we will check the representation space plot, and the reconstruction MSE and check if they are consistent with what we obtained with the simple linear autoencoder we just trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT5Y0lDI0AHA"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca = pca.fit(x_train)\n",
        "representation_pca = pca.transform(x_test)\n",
        "plot_representation_label(representation_pca, y_test)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CEnDoiNTkVx"
      },
      "source": [
        "We can see that the distribution of the PCA representations (size and shape of the clusters, and relative positions to the rest of clusters) is similar to the one obtained in the linear autoencoder section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-D9yhnRKfmL"
      },
      "source": [
        "We now check if the MSE in the test set is similar to the linear autoencoder we trained before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1TaREA4KegD"
      },
      "source": [
        "reconst_test = pca.inverse_transform(representation_pca)\n",
        "mse_pca = ((torch.tensor(reconst_test) - x_test)**2).mean()\n",
        "\n",
        "print('PCA Test Loss {:.4f}'.format(mse_pca))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ylWffKkY3Va"
      },
      "source": [
        "The reconstruction error we get is almost the same as the one obtained with the autoencoder case, which seems to validate the claim that a linear autoencoder behaves in a similar way to PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsuFTQpTrhv2"
      },
      "source": [
        "## Detecting Anomalies\n",
        "\n",
        "After checking the representation of the MNIST images, let's focus on how to spot anomalies in the data. The anomalies are the samples that deviate from the usual distribution. Given a data point from a distribution 'B', the reconstruction error will be higher when using an autoencoder that was trained in a distribution 'A'. Anomaly detection has several uses, for example it can be used for quality control or to detect bank fraud.\n",
        "\n",
        "We now will use that fact to detect anomalies. To do so, we will use the Extended MNIST (EMNIST) dataset, which, apart from digits, includes also both lowercase and uppercase characters from 'a' to 'z'. These 'a' to 'z' characters will act as the anomalies that we aim to detect. As our autoencoder was trained only using 0-9 digits, it should have higher reconstruction errors for those lowercase and uppercase characters, which will help us detect them as anomalies.\n",
        "\n",
        "Let's start by downloading and loading the dataset. The following piece of code downloads the data and uncompresses some necessary files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Bn8R29jmk8"
      },
      "source": [
        "!wget https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip -q\n",
        "!unzip -qq ./gzip.zip\n",
        "!mv gzip data\n",
        "# We need to unzip a couple of files with the train\n",
        "# labels and images\n",
        "!gunzip ./data/emnist-byclass-test-images-idx3-ubyte.gz\n",
        "!gunzip ./data/emnist-byclass-test-labels-idx1-ubyte.gz\n",
        "# We also install a package to help us\n",
        "!pip install python-mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-SK83VtmEq_"
      },
      "source": [
        "Now that we have downloaded the data, we will use the module `mnist` to load the images in the same format as the regular MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fdWUJhZkKgY"
      },
      "source": [
        "from mnist import MNIST\n",
        "\n",
        "# Images in folder data\n",
        "mndata = MNIST('data')\n",
        "\n",
        "# This will load the test data from the downloaded files\n",
        "emnist_x_test, emnist_y_test = mndata.load('./data/emnist-byclass-test-images-idx3-ubyte',\n",
        "                               './data/emnist-byclass-test-labels-idx1-ubyte')\n",
        "\n",
        "\n",
        "# Convert data to numpy arrays and normalize images to the interval [0, 1]\n",
        "n_elem = len(emnist_x_test)\n",
        "emnist_x_test = np.array(emnist_x_test).reshape(n_elem,28,28).transpose(0,2,1).reshape(n_elem,28**2) / 255.0\n",
        "emnist_y_test = np.array(emnist_y_test)\n",
        "\n",
        "# Get labels mapping (index in emnist_y_test to character value)\n",
        "emnist_labels = map(lambda x: x.strip('\\r').split(' '), open('./data/emnist-byclass-mapping.txt').read().strip().split('\\n'))\n",
        "emnist_labels = dict(emnist_labels)\n",
        "\n",
        "# This function will be useful to display the actual label, which is given as\n",
        "# an ascii value (https://en.wikipedia.org/wiki/ASCII) instead of characters\n",
        "def label_to_char(label):\n",
        "  ascii_val = emnist_labels[str(label)]\n",
        "  return chr(int(ascii_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCNS0ckukdQ_"
      },
      "source": [
        "Here we just plot a random image from the EMNIST dataset. Each time you run it, you get a random image from the dataset, in case you want to check how the different characters look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu-Lx3RXpgFc"
      },
      "source": [
        "ind_plot = np.random.randint(emnist_x_test.shape[0]-1)\n",
        "_, ax = plt.subplots(1,1)\n",
        "plt.imshow(emnist_x_test[ind_plot].reshape(28, 28), cmap='gray')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "# We use the label_to_char function to plot the actual character in the\n",
        "# figure title\n",
        "plt.title(label_to_char(emnist_y_test[ind_plot]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MwXda7Qpc8P"
      },
      "source": [
        "Now we have loaded the EMNIST dataset. The images contain characters that the autoencoder has not seen before, hence the reconstruction error (we use Mean Squared Error as the reconstruction metric) for the EMNIST dataset should be higher. We will first compute the reconstruction error for both the MNIST test data and the EMNIST data. Then, we compare the distribution of reconstruction error in both sets using a histogram visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PRPfrxJuDdB"
      },
      "source": [
        "def get_reconstruction_and_mse(model, data, batch_size):\n",
        "    \"\"\"\n",
        "    Run reconstruction on data and compute per-sample MSE entirely in torch.\n",
        "    \"\"\"\n",
        "    data_tensor = data if isinstance(data, torch.Tensor) else torch.tensor(data, dtype=torch.float32)\n",
        "    loader = DataLoader(data_tensor, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    reconstructions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(DEVICE)\n",
        "            recon = model(batch)\n",
        "            reconstructions.append(recon.cpu())\n",
        "\n",
        "    # Concatenate all reconstructions into one tensor\n",
        "    recon_all = torch.cat(reconstructions, dim=0)\n",
        "\n",
        "    # Flatten inputs and reconstructions\n",
        "    data_flat = data_tensor.view(data_tensor.size(0), -1)\n",
        "    recon_flat = recon_all.view(recon_all.size(0), -1)\n",
        "\n",
        "    # Per-sample MSE (tensor on CPU)\n",
        "    mse = ((recon_flat - data_flat) ** 2).mean(dim=1)\n",
        "\n",
        "    return recon_all, mse\n",
        "\n",
        "\n",
        "def compute_errors(model, mnist_data, emnist_data, batch_size=256):\n",
        "    \"\"\"\n",
        "    Compute and plot MSE distributions for MNIST vs EMNIST.\n",
        "    \"\"\"\n",
        "    model = model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    # Compute for MNIST\n",
        "    reconst_mnist, mse_mnist = get_reconstruction_and_mse(model, mnist_data, batch_size)\n",
        "    # Compute for EMNIST\n",
        "    reconst_emnist, mse_emnist = get_reconstruction_and_mse(model, emnist_data, batch_size)\n",
        "\n",
        "    # Convert only the MSE tensors to numpy for plotting\n",
        "    plt.hist(mse_mnist.numpy(), bins=20, label='MNIST', alpha=0.5, density=True)\n",
        "    plt.hist(mse_emnist.numpy(), bins=20, label='EMNIST', alpha=0.5, density=True)\n",
        "    plt.xlabel('MSE')\n",
        "    plt.title('Distribution of MSE for MNIST/EMNIST')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return reconst_mnist, mse_mnist, reconst_emnist, mse_emnist\n",
        "\n",
        "reconst_mnist, mse_mnist, reconst_emnist, mse_emnist = compute_errors(model, x_test, emnist_x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-bAu3pilWfW"
      },
      "source": [
        "As we expected, the distribution of errors for the EMNIST dataset has a higher mean and variance compared to the original MNIST. Let's now check how the image with the lowest reconstruction error and the image with the highest reconstruction error look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7IVFRq2uVGX"
      },
      "source": [
        "# Most anomalous (highest error)\n",
        "plot_recons_original(emnist_x_test[mse_emnist.argmax()], emnist_y_test[mse_emnist.argmax()], model)\n",
        "\n",
        "# Least anomalous (lowest error)\n",
        "plot_recons_original(emnist_x_test[mse_emnist.argmin()], emnist_y_test[mse_emnist.argmin()], model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9Jl7BnvuaCx"
      },
      "source": [
        "It seems that the model is not capable of representing images from the EMNIST dataset that deviate too much from those in the MNIST dataset, as for example images representing the letter 'm'. However, the autoencoder behaves well for images that are similar to those from MNIST, which is what we expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgkeHE1SlVS1"
      },
      "source": [
        "We now perform an anomaly detection exercise. We set the threshold $\\tau$, which we use to mark a data point as an anomaly if $||x-\\psi(\\phi(x))||^2>\\tau$, as $\\tau = \\mu + 2\\sigma$, where $\\mu$ is the average MSE for the whole MNIST test set, and $\\sigma$ is the standard deviation of the MSE in the MNIST test set. We will now plot the ratio of data points marked as anomalies for each of the classes in both MNIST and EMNIST. Additionally, we also report the average MSE per class in both datasets. We expect to see that the number of MNIST anomalies is quite lower compared to the number of anomalies in the EMNIST dataset, as our autoencoder has been trained with MNIST data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRXneSb4s5nd"
      },
      "source": [
        "def print_mse_anomalies(mse_mnist, mse_emnist, emnist_labels_dict, th=-1):\n",
        "    \"\"\"\n",
        "    This function uses the mse per class in both MNIST and EMNIST to compute the ratio of anomalies per class (anomalies/examples per class)\n",
        "    We plot anomalies and mse in a bar histogram, ordered by ratio of anomalies\n",
        "    Do not worry too much about how it works.\n",
        "    \"\"\"\n",
        "    if th == -1:\n",
        "        th = mse_mnist.mean() + 2 * mse_mnist.std()\n",
        "\n",
        "    mnist_labels = range(10)\n",
        "    save_array = []\n",
        "\n",
        "    # MNIST\n",
        "    for label in mnist_labels:\n",
        "        indices_class = y_test == int(label)\n",
        "        if indices_class.sum() == 0:\n",
        "            continue\n",
        "        mse_class = mse_mnist[indices_class]\n",
        "        ratio_anom = (mse_class > th).sum() / indices_class.sum()\n",
        "        save_array.append([mse_class.mean(), ratio_anom, str(label), 'salmon'])\n",
        "\n",
        "    # EMNIST\n",
        "    for label in emnist_labels_dict:\n",
        "        indices_class = emnist_y_test == int(label)\n",
        "        if indices_class.sum() == 0:\n",
        "            continue\n",
        "        mse_class = mse_emnist[indices_class]\n",
        "        ratio_anom = (mse_class > th).sum() / indices_class.sum()\n",
        "        char_label = label_to_char(label)\n",
        "        save_array.append([mse_class.mean(), ratio_anom, char_label, 'steelblue'])\n",
        "\n",
        "    # Sort by anomaly ratio\n",
        "    save_array = sorted(save_array, key=lambda x: x[1])\n",
        "\n",
        "    mse_class = [x[0] for x in save_array]\n",
        "    anomalies_class = [x[1] for x in save_array]\n",
        "    labels_names = [x[2] for x in save_array]\n",
        "    colors = [x[3] for x in save_array]\n",
        "\n",
        "    # Plotting\n",
        "    plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    bars = plt.bar(range(len(save_array)), anomalies_class, color=colors)\n",
        "    plt.xticks(range(len(save_array)), labels_names, rotation=90)\n",
        "    plt.legend([bars[0], bars[-1]], ['MNIST', 'EMNIST'], loc='upper center')\n",
        "    plt.title('Anomalies per class')\n",
        "    plt.ylabel('Ratio anomalies')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.bar(range(len(save_array)), mse_class, color=colors)\n",
        "    plt.xticks(range(len(save_array)), labels_names, rotation=90)\n",
        "    plt.legend([bars[0], bars[-1]], ['MNIST', 'EMNIST'], loc='upper center')\n",
        "    plt.title('Average MSE per class')\n",
        "    plt.ylabel('MSE')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "print_mse_anomalies(mse_mnist, mse_emnist, emnist_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clp0yBwTUB9s"
      },
      "source": [
        "The top figure shows the ratio of anomalies per class, which is the number of anomalies detected for the data points of that class divided by the number of examples of that specific class. The bottom figure shows the average error per category, where we can see that the classes from EMNIST have a higher reconstruction error than those from MNIST. The shared classes in MNIST and EMNIST, the digits 0-9, seem to have a different distribution in those two datasets as the reconstruction error for those classes in EMNIST is higher than in MNIST. Additionally, some letters that look like the number 1 (`l, I, i, t, j`) or like 0 (`O` and `o`) have a low reconstruction error as they are similar to some of the images in the MNIST dataset. Some other classes, such as `W`, `w`, or `Q` have a high reconstruction error due to being quite dissimilar to any of the images in MNIST.\n",
        "\n",
        "We have seen that data from other datasets will have a high reconstruction error. However, even for MNIST, we can look for samples that have a high reconstruction error compared to the average. We expect to see that most images in a given class will follow a similar distribution, however some of them will deviate from this distribution.\n",
        "\n",
        "In that direction, let's take one of the classes from MNIST and plot some images with high reconstruction error and some others with low reconstruction error. We expect to see those with low reconstruction error to be quite similar between them and represent what the average sample of that class looks like. The high reconstruction error samples will, in turn, contain some elements (pose, shape for example).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrAegIpQUxK5"
      },
      "source": [
        "# Set same threshold we defined before \\mu + 2\\sigma\n",
        "th = mse_mnist.std() + mse_mnist.mean()\n",
        "\n",
        "# We will use the label 8 for the example\n",
        "indices_class = y_test == 8\n",
        "\n",
        "# Compute the number of anomalies (mse > th)\n",
        "anomalies_im = x_test[indices_class][mse_mnist[indices_class] > th]\n",
        "\n",
        "# For the nonanomalies, we will sort the images by reconstruction error\n",
        "# So we will plot the images with low reconstruction error\n",
        "indices_sort = np.argsort(mse_mnist[indices_class])\n",
        "nonanomalies_im = x_test[indices_class][indices_sort]\n",
        "\n",
        "def plot_grid(images, N=5, title=''):\n",
        "  ## Plots data in variable images in a grid of N*N\n",
        "  # Create figure\n",
        "  fig, axes = plt.subplots(N,N, figsize=(8,8))\n",
        "  # Loop to generate grid\n",
        "  for row in range(N):\n",
        "    for col in range(N):\n",
        "      idx = row+N*col\n",
        "      axes[row,col].imshow(images[idx].reshape(28,28), cmap='gray')\n",
        "      axes[row,col].set_xticks([])\n",
        "      axes[row,col].set_yticks([])\n",
        "\n",
        "  # Adjust white space\n",
        "  fig.subplots_adjust(hspace=0.0)\n",
        "  fig.subplots_adjust(wspace=0.0)\n",
        "  fig.subplots_adjust(right=1.0)\n",
        "  fig.subplots_adjust(left=0.245)\n",
        "  # Set title\n",
        "  fig.suptitle(title, x=0.62, y=0.93, fontsize=24)\n",
        "\n",
        "# Plot anomalies\n",
        "plot_grid(anomalies_im, title='Anomalies')\n",
        "# Plot non-anomalies\n",
        "plot_grid(nonanomalies_im, title='Non-anomalies')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkdCjg0Vhud1"
      },
      "source": [
        "The images classified as anomalies for the given threshold $\\tau$ show a much larger variation in shape and pose. In turn, the images with low reconstruction error for the selected label (in this case `8`) show similar proportions and less variation between any two images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hziHexYzsVAx"
      },
      "source": [
        "# Image-to-Image Translation\n",
        "\n",
        "As seen previously, autoencoders aim at creating low-dimensionality representations that only preserve the most discriminative information of the input signals. We can train them by looking into the reconstruction error between the input and the generated data. This training process is a form of *self-supervision* learning. Thus, the encoding part is trained to create a strong feature representation, from which the decoder can reconstruct the original data.\n",
        "\n",
        "The dimensionality reduction/compression is widely used in many signal processing tasks. However, autoencoders are also applied in many image-to-image tasks. Here we will show how to train an autoencoder as a denoising method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtajNZ02xrqs"
      },
      "source": [
        "## Denoising with Autoencoders\n",
        "\n",
        "The idea behind this task is that autoencoders will preserve only the characteristic information and avoid noise on the reconstruction.\n",
        "\n",
        "Noise is random, and it can not be predicted. Therefore, a robust encoding/decoding learns to identify the stable patterns within the image, and avoid the non-smooth perturbations produced by the noise.\n",
        "\n",
        "This idea is in line with the **Efficient Coding Hypothesis** from the vision neuroscience field, which says \". . . the Efficient Coding Hypothesis holds that the purpose of early visual processing is to produce an efficient representation of the incoming visual signal.\"\n",
        "\n",
        "Noise can be presented in many different ways in images. In this tutorial, we will generate synthetically additive Gaussian noise to train our denoising autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMz1KDyksVA1"
      },
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "# Transform for loading original (clean) data\n",
        "transform_clean = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to [0,1] range\n",
        "])\n",
        "\n",
        "# Download and load data\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_clean)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_clean)\n",
        "\n",
        "# Convert to numpy arrays for easier noise addition\n",
        "x_train = torch.stack([img for img, _ in train_dataset]).numpy().transpose(0, 2, 3, 1)  # (N, H, W, C)\n",
        "x_test = torch.stack([img for img, _ in test_dataset]).numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "# Additive Gaussian noise\n",
        "np.random.seed(42)\n",
        "x_train_noise = x_train + np.random.normal(scale=0.3, size=x_train.shape)\n",
        "x_test_noise = x_test + np.random.normal(scale=0.3, size=x_test.shape)\n",
        "\n",
        "# Clip values to [0, 1] after adding noise\n",
        "x_train_noise = np.clip(x_train_noise, 0., 1.)\n",
        "x_test_noise = np.clip(x_test_noise, 0., 1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAJH51FJsVA2"
      },
      "source": [
        "We can visualise some examples to have an idea of how the noise looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKxiJ1D3sVA3"
      },
      "source": [
        "from torchvision.transforms import Resize, ToTensor, Compose\n",
        "\n",
        "resize_transform = Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def resize_batch(imgs):\n",
        "    resized = []\n",
        "    for i in range(imgs.shape[0]):\n",
        "        resized_img = resize_transform((imgs[i] * 255).astype(np.uint8))\n",
        "        resized.append(resized_img.permute(1, 2, 0).numpy())  # (H, W, C)\n",
        "    return np.stack(resized)\n",
        "\n",
        "# Resize first 4 images from clean and noisy sets\n",
        "x_train_resized = resize_batch(x_train[:4])\n",
        "x_train_noise_resized = resize_batch(x_train_noise[:4])\n",
        "\n",
        "# Visualize side-by-side comparison\n",
        "N = 2\n",
        "start_val = 0\n",
        "fig, axes = plt.subplots(N, N, figsize=(8, 8))\n",
        "plt.suptitle('Clean vs Noisy', fontsize=18)\n",
        "\n",
        "for row in range(N):\n",
        "    for col in range(N):\n",
        "        idx = start_val + row + N * col\n",
        "        im = np.concatenate((x_train_resized[idx], x_train_noise_resized[idx]), axis=1)\n",
        "        axes[row, col].imshow(np.clip(im, 0, 1))\n",
        "        axes[row, col].set_xticks([])\n",
        "        axes[row, col].set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to torch tensors (transpose to NCHW)\n",
        "x_train_tensor = torch.tensor(x_train_noise.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
        "x_test_tensor = torch.tensor(x_test_noise.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
        "\n",
        "# Self-supervised: use noisy inputs as targets\n",
        "train_dataset = TensorDataset(x_train_tensor, x_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# Supervised: use clean inputs as targets\n",
        "x_train_clean_tensor = torch.tensor(x_train.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
        "x_test_clean_tensor = torch.tensor(x_test.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
        "\n",
        "train_dataset_supervised = TensorDataset(x_train_tensor, x_train_clean_tensor)\n",
        "train_loader_supervised = DataLoader(train_dataset_supervised, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "EM3j4YoOyf9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsZNEQwVsVA3"
      },
      "source": [
        "### Linear Autoencoders\n",
        "\n",
        "As in regular architecture design, when defining the structure of an autoencoder, we can decide the number of convolutional layers, dense layers, normalisation mechanisms, activation functions, and so on.\n",
        "\n",
        "Similar to the Representation Learning section, in this denoising task, we start with a linear architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnnbe8wYsVA3"
      },
      "source": [
        "# Define linear autoencoder\n",
        "class LinearDenoisingAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 32 * 3, 128),\n",
        "            nn.Linear(128, 30)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(30, 128),\n",
        "            nn.Linear(128, 32 * 32 * 3),\n",
        "            nn.Unflatten(1, (3, 32, 32))  # (N, C, H, W)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC1UhNyfsVA3"
      },
      "source": [
        "We train our model with the standard Mean Square Error (MSE) loss function. We compute MSE between reconstructed images and noisy ones. Ideally, as mentioned above, the noise is random, and therefore, it will not be modelled.\n",
        "\n",
        "Sometimes we have a dataset that contains noisy and clean images. If that is the case, we could compute the MSE between the reconstructed and the clean ones. Using clean ground-truth images (*supervised learning*) improves results compared to only using noisy ones (*self-supervised learning*). However, we want to start from the perspective of image compression and feature representation, and train our denoising model using exclusively noisy images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf82ejmKsVA4"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = LinearDenoisingAutoencoder()\n",
        "print(torchinfo.summary(model, input_size=(1, 3, 32, 32)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    nn.MSELoss(),\n",
        "    optim.Adam(model.parameters(), lr=1e-3),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrKQAd4vsVA4"
      },
      "source": [
        "Let's check how denoised images look like after ten epochs of training. Run the following code multiple times to see different examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L_UWHymsVA4"
      },
      "source": [
        "# Random example from test set\n",
        "idx_example = np.random.randint(0, len(x_test_tensor) - 2)\n",
        "x_input = x_test_tensor[idx_example:idx_example+2].to(DEVICE)\n",
        "\n",
        "# Run model on test example\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x_denoised = model(x_input).cpu().numpy()\n",
        "\n",
        "# Get clean version\n",
        "x_clean = x_test[idx_example:idx_example+2]\n",
        "x_noisy = x_test_noise[idx_example:idx_example+2]\n",
        "\n",
        "# Visualize Noisy vs Reconstructed vs Clean\n",
        "N = 2\n",
        "fig, axes = plt.subplots(N, 1, figsize=(8, 6))\n",
        "plt.suptitle('Noisy Input VS Reconstructed VS Clean Image', fontsize=18)\n",
        "\n",
        "for row in range(N):\n",
        "    im = np.concatenate((\n",
        "        x_noisy[row],                         # Noisy\n",
        "        np.transpose(x_denoised[row], (1, 2, 0)),  # Reconstructed\n",
        "        x_clean[row]                          # Clean\n",
        "    ), axis=1)\n",
        "    axes[row].imshow(np.clip(im, 0, 1))\n",
        "    axes[row].set_xticks([])\n",
        "    axes[row].set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXMZSB-TsVA4"
      },
      "source": [
        "Reconstructed images are a much-blurred version of the clean ones. The proposed autoencoder can learn to remove noise, but results are far from being perfect when comparing them with the ground-truth images. Note though that with a few neurons, the network can generate images that have roughly the same colour and shape than the original ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb4YZKd1sVA5"
      },
      "source": [
        "### Non-linear Autoencoders\n",
        "\n",
        "In non-linear autoencoders, we introduce non-linearities within the network by using non-linear activation functions. This is a big difference with classical technique PCA, which performs a linear transformation of the data. Thus, using non-linear autoencoders allows us to create flexible algorithms that can learn relationships in the data beyond linear transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJJ_YOCSsVA5"
      },
      "source": [
        "class NonlinearDenoisingAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 32 * 3, 128),\n",
        "            nn.Linear(128, 30),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(30, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 32 * 32 * 3),\n",
        "            nn.Sigmoid(),  # restrict outputs to [0, 1]\n",
        "            nn.Unflatten(1, (3, 32, 32))  # Reshape to (C, H, W)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "nonlinear_model = NonlinearDenoisingAutoencoder()\n",
        "print(torchinfo.summary(nonlinear_model, input_size=(1, 3, 32, 32)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    nonlinear_model,\n",
        "    train_loader,\n",
        "    nn.MSELoss(),\n",
        "    optim.Adam(nonlinear_model.parameters(), lr=1e-3),\n",
        ")"
      ],
      "metadata": {
        "id": "-RKbkC7jG0wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZIbZW6xsVA5"
      },
      "source": [
        "We can now visualise the reconstructed images by the proposed non-linear model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejJLE4B7sVA5"
      },
      "source": [
        "idx_example = np.random.randint(0, len(x_test_tensor) - 2)\n",
        "x_input = x_test_tensor[idx_example:idx_example+2].to(DEVICE)\n",
        "\n",
        "# Predict\n",
        "nonlinear_model.eval()\n",
        "with torch.no_grad():\n",
        "    x_denoised = nonlinear_model(x_input).cpu().numpy()\n",
        "\n",
        "x_noisy = x_test_noise[idx_example:idx_example+2]\n",
        "x_clean = x_test[idx_example:idx_example+2]\n",
        "\n",
        "N = 2\n",
        "fig, axes = plt.subplots(N, 1, figsize=(8, 6))\n",
        "plt.suptitle('Noisy Input VS Reconstructed VS Clean Image', fontsize=18)\n",
        "\n",
        "for row in range(N):\n",
        "    im = np.concatenate((\n",
        "        x_noisy[row],\n",
        "        np.transpose(x_denoised[row], (1, 2, 0)),\n",
        "        x_clean[row]\n",
        "    ), axis=1)\n",
        "    axes[row].imshow(np.clip(im, 0, 1))\n",
        "    axes[row].set_xticks([])\n",
        "    axes[row].set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA-M4sHwsVA5"
      },
      "source": [
        "Qualitatively results at this stage are difficult to interpret. Hence, we compute the MSE metric in the test set for both architectures and compare them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROHQ5emIsVA5"
      },
      "source": [
        "def evaluate_mse(model, noisy_input, clean_target):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for i in range(0, len(noisy_input), 128):\n",
        "            batch_input = noisy_input[i:i+128].to(DEVICE)\n",
        "            batch_target = clean_target[i:i+128].to(DEVICE)\n",
        "\n",
        "            pred = model(batch_input)\n",
        "            predictions.append(pred.cpu())\n",
        "            targets.append(batch_target.cpu())\n",
        "\n",
        "        predictions = torch.cat(predictions)\n",
        "        targets = torch.cat(targets)\n",
        "        return nn.MSELoss()(predictions, targets).item()\n",
        "\n",
        "mse_linear = evaluate_mse(model, x_test_tensor, x_test_clean_tensor)\n",
        "mse_nonlinear = evaluate_mse(nonlinear_model, x_test_tensor, x_test_clean_tensor)\n",
        "\n",
        "print(f\"Linear Model MSE: {mse_linear:.4f}\")\n",
        "print(f\"Non-linear Model MSE: {mse_nonlinear:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4nGG4uWsVA6"
      },
      "source": [
        "Although results are close, we can observe an improvement in MSE just by applying a non-linear activation function to the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Fsb5MMsVA6"
      },
      "source": [
        "### Convolutional Autoencoders\n",
        "\n",
        "As seen in previous tutorials, 2D convolutions are more suitable when the input is an image than 1D convolutions. In practical settings, convolutional autoencoders are always applied to images, they simply perform much better.\n",
        "\n",
        "CNNs are used exactly as before, however, now we have to take into account elements such as strides, pooling, upsampling, and deconvolutions. Those elements are needed to control the size (width and length) of the generated feature maps. A regular setting in image denoising is that the input and output dimension is constant.\n",
        "\n",
        "Besides, we need to define the number of levels of compression within the architecture, which can be controlled by limiting the number of convolutional layers and/or downsampling steps:\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/vB2jq4C/autoencoders.png)\n",
        "\n",
        "Let's define and train an architecture with 3 compression levels:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HSv9ATUsVA6"
      },
      "source": [
        "class CNNAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNAutoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(3, 3, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid() # Added Sigmoid activation to constrain output to [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f-kMMTQsVA7"
      },
      "source": [
        "In our example, our encoder uses strides of (2, 2) in the convolutions. (2, 2) stride downsamples the dimension of the feature map by two at every step. To recover the original size, in the decoder, we use UpSampling2D layers. (2, 2) UpSampling2D layers increase by two the dimensionality of the input feature map. These two mechanisms allow us to control the dimensions of the feature maps throughout the autoencoder.\n",
        "\n",
        "Let's check how the reconstructed images look:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "conv_model = CNNAutoencoder()\n",
        "print(torchinfo.summary(conv_model, input_size=(1, 3, 32, 32)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    conv_model,\n",
        "    train_loader,\n",
        "    nn.MSELoss(),\n",
        "    optim.Adam(conv_model.parameters(), lr=1e-3),\n",
        ")"
      ],
      "metadata": {
        "id": "3tEl5vpcQIvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW7DYjnWsVA7"
      },
      "source": [
        "# Predict\n",
        "idx_example = np.random.randint(0, len(x_test_tensor) - 2)\n",
        "x_input = x_test_tensor[idx_example:idx_example+2].to(DEVICE)\n",
        "\n",
        "conv_model.eval()\n",
        "with torch.no_grad():\n",
        "    x_denoised = conv_model(x_input).cpu().numpy()\n",
        "\n",
        "x_noisy = x_test_noise[idx_example:idx_example+2]\n",
        "x_clean = x_test[idx_example:idx_example+2]\n",
        "\n",
        "N = 2\n",
        "fig, axes = plt.subplots(N, 1, figsize=(8, 6))\n",
        "plt.suptitle('Noisy Input VS Reconstructed VS Clean Image', fontsize=18)\n",
        "\n",
        "for row in range(N):\n",
        "    im = np.concatenate((\n",
        "        x_noisy[row],\n",
        "        np.transpose(x_denoised[row], (1, 2, 0)),\n",
        "        x_clean[row]\n",
        "    ), axis=1)\n",
        "    axes[row].imshow(np.clip(im, 0, 1))\n",
        "    axes[row].set_xticks([])\n",
        "    axes[row].set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zo2X57pRjct"
      },
      "source": [
        "And check their MSE error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzrU8SBoRl7E"
      },
      "source": [
        "mse_cnn = evaluate_mse(conv_model, x_test_tensor, x_test_clean_tensor)\n",
        "\n",
        "print('Linear Model MSE: {:0.4f}'.format(mse_linear))\n",
        "print('Non-linear Model MSE: {:0.4f}'.format(mse_nonlinear))\n",
        "print('CNN Model MSE: {:0.4f}'.format(mse_cnn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCmZz1UWRxlW"
      },
      "source": [
        "We are getting better at it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--3PpK_msVA8"
      },
      "source": [
        "### Denoising with Clean Images\n",
        "\n",
        "We showed that autoencoders can encode the image's structure and recover a clean but blurry image. They ignore the non-smooth perturbations produced by the noise, allowing us to train them in a self-supervised setting.\n",
        "\n",
        "Although having a ground-truth of clean images is not always possible, it is desirable when training denoising models. As we have generated synthetically the noise, we can train the network using the original image as the clean one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGxPmU2DsVA8"
      },
      "source": [
        "class CNN_GT_Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_GT_Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),  # -> (B, 32, H/2, W/2)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # -> (B, 64, H/4, W/4)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),# -> (B, 128, H/8, W/8)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(3, 3, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOFmZNvPsVA9"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "cnn_gt_model = CNN_GT_Autoencoder()\n",
        "print(torchinfo.summary(cnn_gt_model, input_size=(1, 3, 32, 32)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    cnn_gt_model,\n",
        "    train_loader_supervised,\n",
        "    nn.MSELoss(),\n",
        "    optim.Adam(cnn_gt_model.parameters(), lr=1e-3),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yURZowAhsVA8"
      },
      "source": [
        "Let's now check how the reconstructed images look:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_gt_model.eval()\n",
        "idx_example = np.random.randint(0, len(x_test_noise))\n",
        "with torch.no_grad():\n",
        "    noisy_input = torch.tensor(x_test_noise[idx_example:idx_example+2].transpose(0, 3, 1, 2), dtype=torch.float32).to(DEVICE)\n",
        "    denoised_output = cnn_gt_model(noisy_input).cpu().numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "N = 2\n",
        "start_val = 0\n",
        "fig, axes = plt.subplots(N, 1)\n",
        "plt.suptitle('Noisy Input VS Denoised VS Clean Image', fontsize=18)\n",
        "for row in range(N):\n",
        "    idx = start_val + row\n",
        "    im = np.concatenate((\n",
        "        x_test_noise[idx_example+idx],\n",
        "        np.clip(denoised_output[idx], 0, 1),\n",
        "        x_test[idx_example+idx]\n",
        "    ), axis=1)\n",
        "    axes[row].imshow(np.clip(im, 0, 1))\n",
        "    axes[row].set_xticks([])\n",
        "    axes[row].set_yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ht2IAb1oh3WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qcnZ2ijsVA9"
      },
      "source": [
        "In addition, we can compare the MSE when having clean images for training or not:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sZNJSVjsVA9"
      },
      "source": [
        "mse_supervised = evaluate_mse(cnn_gt_model, x_test_tensor, x_test_clean_tensor)\n",
        "print('\"Self\"-supervised Model (Trained only with Noisy Images) MSE: {:0.4f}'.format(mse_cnn))\n",
        "print('Supervised Model (Trained also with Clean Images) MSE: {:0.4f}'.format(mse_supervised))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECCMBd_gsVA9"
      },
      "source": [
        "Even though there is a clear improvement, at this point, the results do not differ that much. Thus, having clean images is not enough for denoising images. We could try to increase the model's complexity, however, we propose to use now skip connections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCVxbqpCsVA9"
      },
      "source": [
        "### Skip Connections\n",
        "\n",
        "Skip connections in Autoencoders were introduced in the *Common CNN Architectures* tutorial, together with [UNet](https://arxiv.org/pdf/1505.04597.pdf) network. Skip connections, as the name suggests, creates connections between layers within the neural network by jumping some of them. This connection allows the network to feed layers not only with the previous one but, in addition, with other layers that were not directly connected. For example, in the next figure we can see an architecture, where  the encoder and decoder share features through skip connections:\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/d7sX5bh/Skip-Connections.png)\n",
        "\n",
        "The idea behind this is that there is information in the first layers that is hard to recover by posterior ones. This information is important when upsampling the feature map in the decoder, where instead of learning how to recover features from the first layers, it can learn how to directly used them to generate better outputs.\n",
        "\n",
        "Besides, skip connections help to pass information through the network, helping to mitigate the vanishing gradient problem when the architecture is deep. As seen in the *Common CNN Architectures* tutorial, gradient information can be lost during the backpropagation when it goes through many layers. Having a direct path between encoder and decoder layers helps the convergence and training of the network.\n",
        "\n",
        "There are many ways to implement those skip connections, we show in the next example how to generate them by concatenating features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MXUb-m2sVA-"
      },
      "source": [
        "class SkipConnectionAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SkipConnectionAutoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.enc1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)  # conv1\n",
        "        self.enc2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1) # conv2\n",
        "        self.enc3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1) # conv3\n",
        "        self.enc4 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1) # conv4\n",
        "\n",
        "        # Decoder\n",
        "        self.dec1_conv = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.dec2_conv = nn.Conv2d(128 + 128, 64, kernel_size=3, padding=1)   # merge with conv3\n",
        "        self.dec3_conv = nn.Conv2d(64 + 64, 32, kernel_size=3, padding=1)     # merge with conv2\n",
        "        self.dec4_conv = nn.Conv2d(32 + 32, 32, kernel_size=3, padding=1)     # merge with conv1\n",
        "        self.final_conv = nn.Conv2d(32, 3, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        c1 = F.relu(self.enc1(x))  # 16x16x32\n",
        "        c2 = F.relu(self.enc2(c1)) # 8x8x64\n",
        "        c3 = F.relu(self.enc3(c2)) # 4x4x128\n",
        "        c4 = F.relu(self.enc4(c3)) # 2x2x128\n",
        "\n",
        "        # Decoder + Skip connections\n",
        "        u1 = F.interpolate(c4, scale_factor=2, mode='nearest')  # 4x4x128\n",
        "        u1 = F.relu(self.dec1_conv(u1))\n",
        "        m1 = torch.cat([c3, u1], dim=1)  # merge1\n",
        "\n",
        "        u2 = F.interpolate(m1, scale_factor=2, mode='nearest')  # 8x8\n",
        "        u2 = F.relu(self.dec2_conv(u2))\n",
        "        m2 = torch.cat([c2, u2], dim=1)  # merge2\n",
        "\n",
        "        u3 = F.interpolate(m2, scale_factor=2, mode='nearest')  # 16x16\n",
        "        u3 = F.relu(self.dec3_conv(u3))\n",
        "        m3 = torch.cat([c1, u3], dim=1)  # merge3\n",
        "\n",
        "        u4 = F.interpolate(m3, scale_factor=2, mode='nearest')  # 32x32\n",
        "        u4 = self.dec4_conv(u4)\n",
        "\n",
        "        out = self.final_conv(u4)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xD_kPTHsVA-"
      },
      "source": [
        "Let's train the model with skip connections:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85ZuB9izsVA-"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "skip_model = SkipConnectionAutoencoder()\n",
        "print(torchinfo.summary(skip_model, input_size=(1, 3, 32, 32)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    nn.MSELoss(),\n",
        "    optim.Adam(skip_model.parameters(), lr=1e-3),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "skip_model.eval()\n",
        "mse = evaluate_mse(skip_model, x_test_tensor, x_test_clean_tensor)\n",
        "\n",
        "print('Model without Skip Connections MSE: {:0.4f}'.format(mse_supervised))\n",
        "print('Model with Skip Connections MSE: {:0.4f}'.format(mse))"
      ],
      "metadata": {
        "id": "PT_96LzNXvEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zPxhiCRsVA_"
      },
      "source": [
        "Let's visualise how the denoised images look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7zivvwIsVA_"
      },
      "source": [
        "idx_example = np.random.randint(0, len(x_test_noise))\n",
        "with torch.no_grad():\n",
        "    noisy_input = torch.tensor(\n",
        "        x_test_noise[idx_example:idx_example+2].transpose(0, 3, 1, 2),\n",
        "        dtype=torch.float32\n",
        "    ).to(DEVICE)\n",
        "    denoised_output = skip_model(noisy_input).cpu().numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "N = 2\n",
        "fig, axes = plt.subplots(N, 1)\n",
        "plt.suptitle('Noisy Input VS Denoised VS Clean Image', fontsize=18)\n",
        "for row in range(N):\n",
        "    im = np.concatenate((\n",
        "        x_test_noise[idx_example+row],\n",
        "        np.clip(denoised_output[row], 0, 1),\n",
        "        x_test[idx_example+row]\n",
        "    ), axis=1)\n",
        "    axes[row].imshow(np.clip(im, 0, 1))\n",
        "    axes[row].set_xticks([])\n",
        "    axes[row].set_yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFY_sTiFsVA_"
      },
      "source": [
        "Images are a bit blurry, but noise is almost all gone!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DPS5v6T8EdL"
      },
      "source": [
        "## Image Segmentation with Autoencoders\n",
        "\n",
        "Autoencoders have been used for multiple tasks, their applications do not limit to representation learning, or image denoising. We are going to show here how to use them to do a more general task, image segmentation.\n",
        "\n",
        "Image segmentation is the task of assigning a class value to each pixel in the input image. To do so, we have prepared the [COCO-Person](https://www.kaggle.com/oishee30/cocopersonsegmentation) dataset. COCO-Person dataset contains a collection of images and their respective masks. Their masks indicate if a pixel is background or person. Let's first of all download it:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Supervise.ly Filtered Segmentation Person Dataset\n",
        "!wget -q https://imperialcollegelondon.box.com/shared/static/kq70816mxj4mnfph5kxt352xnx40rzeq.zip\n",
        "!unzip -qq kq70816mxj4mnfph5kxt352xnx40rzeq.zip"
      ],
      "metadata": {
        "id": "05USgMLSi4so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmAXDRCK9Amp"
      },
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images, masks):\n",
        "        self.images = torch.tensor(images).permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
        "        self.masks = torch.tensor(masks)  # N,H,W\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.masks[idx]\n",
        "\n",
        "def load_split(split):\n",
        "    images = os.listdir(f'is_coco_dataset/{split}/')\n",
        "    data, labels = [], []\n",
        "    for i in images:\n",
        "        img = cv2.imread(f'is_coco_dataset/{split}/{i}')\n",
        "        mask = cv2.cvtColor(cv2.imread(f'is_coco_dataset/masks/{i}'), cv2.COLOR_BGR2GRAY)\n",
        "        mask = np.where(mask > 100, 1, 0)  # binary mask\n",
        "        data.append(img)\n",
        "        labels.append(mask)\n",
        "    return np.asarray(data), np.asarray(labels)\n",
        "\n",
        "def load_data():\n",
        "    print('Start loading data')\n",
        "    train_data, train_labels = load_split('train')\n",
        "    val_data, val_labels = load_split('val')\n",
        "    test_data, test_labels = load_split('test')\n",
        "    return [train_data, train_labels], [val_data, val_labels], [test_data, test_labels]\n",
        "\n",
        "def shuffle_data(train_data, train_labels, val_data, val_labels):\n",
        "    train_idx = np.random.permutation(len(train_data))\n",
        "    val_idx = np.random.permutation(len(val_data))\n",
        "    return train_data[train_idx], train_labels[train_idx], val_data[val_idx], val_labels[val_idx]\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_data()\n",
        "\n",
        "x_train = x_train.astype(np.float32) / 255.\n",
        "x_val = x_val.astype(np.float32) / 255.\n",
        "x_test = x_test.astype(np.float32) / 255.\n",
        "\n",
        "x_train, y_train, x_val, y_val = shuffle_data(x_train, y_train, x_val, y_val)\n",
        "\n",
        "y_train = y_train.astype(np.int64)\n",
        "y_val = y_val.astype(np.int64)\n",
        "y_test = y_test.astype(np.int64)\n",
        "\n",
        "train_dataset = SegmentationDataset(x_train, y_train)\n",
        "val_dataset = SegmentationDataset(x_val, y_val)\n",
        "test_dataset = SegmentationDataset(x_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_nYZXDM9L9E"
      },
      "source": [
        "We now can visualise the dataset. We plot the input images, segmented masks, and their overlapping. Run it multiple times for different examples:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N = 2\n",
        "y_train_vis = np.where(y_train >= 0.5, 1., 0.).astype(np.float32)\n",
        "y_train_mask = np.tile(y_train_vis[:, :, :, np.newaxis], [1, 1, 1, 3])\n",
        "y_train_vis_gt = np.zeros((len(y_train), 128, 176, 3), np.float32)\n",
        "y_train_vis_gt[:, :, :, 1] = y_train_vis\n",
        "blank_space = np.ones((128, 70, 3), np.float32)\n",
        "\n",
        "start_val = np.random.randint(len(y_train) - N**2)\n",
        "fig, axes = plt.subplots(N, 1)\n",
        "plt.suptitle('Input Image VS GT Mask VS Masked Image', fontsize=18)\n",
        "for row in range(N):\n",
        "    idx = start_val + row\n",
        "    overlap_gt = cv2.addWeighted(x_train[idx], 1, y_train_vis_gt[idx], 0.5, 0)\n",
        "    im = np.concatenate((x_train[idx], blank_space, y_train_mask[idx], blank_space, overlap_gt), 1)\n",
        "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "    axes[row].imshow(np.clip(im, 0, 1))\n",
        "    axes[row].axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lZZYjvkoZIOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgpqoD7V9T59"
      },
      "source": [
        "Segmenting the background of a portrait photo is a really popular task in daily applications. For instance, if you do not have a DSLR camera, you still can get those blurry backgrounds thanks to Autoencoders. Applications do not end there, check some examples of what you can do in the [Google AI Blog](https://ai.googleblog.com/2018/03/mobile-real-time-video-segmentation.html). Here there is one example from them:\n",
        "\n",
        "\n",
        "<a href=\"https://ibb.co/0hhQ3Zt\"><img src=\"https://3.bp.blogspot.com/-jp16CE_SLZk/WpW1sKWU9PI/AAAAAAAACY0/sjHghHiuarEC2aEy5txhmdT6INK9C_OxACLcBGAs/s640/image3.gif\" alt=\"Screenshot-from-2019-02-14-14-49-24\" border=\"0\"></a>\n",
        "\n",
        "\n",
        "Now that we have the data and understand the task, we can define a model and train it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0NnQptfVjuH"
      },
      "source": [
        "class AutoencoderSeg(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.enc1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
        "        self.enc2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
        "        self.enc3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
        "        self.enc4 = nn.Conv2d(128, 128, 3, stride=2, padding=1)\n",
        "        self.up1 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.up2 = nn.Conv2d(256, 64, 3, padding=1)\n",
        "        self.up3 = nn.Conv2d(128, 32, 3, padding=1)\n",
        "        self.up4 = nn.Conv2d(64, 32, 3, padding=1)\n",
        "        self.final = nn.Conv2d(32, num_classes, 3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        c1 = self.relu(self.enc1(x))\n",
        "        c2 = self.relu(self.enc2(c1))\n",
        "        c3 = self.relu(self.enc3(c2))\n",
        "        c4 = self.relu(self.enc4(c3))\n",
        "        u1 = F.interpolate(c4, scale_factor=2, mode='nearest')\n",
        "        u1 = self.relu(self.up1(u1))\n",
        "        u1 = torch.cat([c3, u1], dim=1)\n",
        "        u2 = F.interpolate(u1, scale_factor=2, mode='nearest')\n",
        "        u2 = self.relu(self.up2(u2))\n",
        "        u2 = torch.cat([c2, u2], dim=1)\n",
        "        u3 = F.interpolate(u2, scale_factor=2, mode='nearest')\n",
        "        u3 = self.relu(self.up3(u3))\n",
        "        u3 = torch.cat([c1, u3], dim=1)\n",
        "        u4 = F.interpolate(u3, scale_factor=2, mode='nearest')\n",
        "        u4 = self.up4(u4)\n",
        "        return self.final(u4)  # logits\n",
        "\n",
        "\n",
        "# Training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoencoderSeg(num_classes=2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "best_val_acc = 0\n",
        "patience, patience_counter = 4, 0\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    train_correct, train_total, train_loss = 0, 0, 0\n",
        "    for imgs, masks in train_loader:\n",
        "        imgs, masks = imgs.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * imgs.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        train_correct += (preds == masks).sum().item()\n",
        "        train_total += masks.numel()\n",
        "\n",
        "    val_correct, val_total, val_loss = 0, 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in val_loader:\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            val_correct += (preds == masks).sum().item()\n",
        "            val_total += masks.numel()\n",
        "\n",
        "    train_acc = train_correct / train_total\n",
        "    val_acc = val_correct / val_total\n",
        "    print(f\"Epoch [{epoch+1}/30] Train Loss: {train_loss/len(train_dataset):.4f}, \"\n",
        "          f\"Train Acc: {train_acc*100:.2f}%, Val Loss: {val_loss/len(val_dataset):.4f}, \"\n",
        "          f\"Val Acc: {val_acc*100:.2f}%\")\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter > patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "# Evaluate on test set\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n",
        "test_correct, test_total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for imgs, masks in test_loader:\n",
        "        imgs, masks = imgs.to(device), masks.to(device)\n",
        "        outputs = model(imgs)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        test_correct += (preds == masks).sum().item()\n",
        "        test_total += masks.numel()\n",
        "print()\n",
        "print(f'Test Accuracy: {test_correct/test_total*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46WW9ZSGV6Xh"
      },
      "source": [
        "In addition to the previous code, we also provide code for visualising the predicted masks on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMUHtz6pI9vJ"
      },
      "source": [
        "x_test_batch, y_test_batch = next(iter(test_loader))\n",
        "x_test_batch, y_test_batch = x_test_batch.to(device), y_test_batch.to(device)\n",
        "with torch.no_grad():\n",
        "    preds = model(x_test_batch[:2]).argmax(dim=1).cpu().numpy()\n",
        "x_test_vis = np.zeros((2, 128, 176, 3), np.float32)\n",
        "x_test_vis[:, :, :, 1] = preds\n",
        "y_test_vis = np.zeros((2, 128, 176, 3), np.float32)\n",
        "y_test_vis[:, :, :, 1] = y_test_batch[:2].cpu().numpy()\n",
        "blank_space = np.ones((128, 70, 3), np.float32)\n",
        "\n",
        "N = 2\n",
        "start_val = 0\n",
        "fig, axes = plt.subplots(N, 1)\n",
        "plt.suptitle('Input Image VS Predicted Mask VS GT Mask', fontsize=18)\n",
        "for row in range(N):\n",
        "    idx = start_val + row\n",
        "    img = x_test_batch[idx].cpu().permute(1, 2, 0).numpy()\n",
        "    overlap_pred = cv2.addWeighted(img, 1, x_test_vis[idx], 0.5, 0)\n",
        "    overlap_gt = cv2.addWeighted(img, 1, y_test_vis[idx], 0.5, 0)\n",
        "    im = np.concatenate((img, blank_space, overlap_pred, blank_space, overlap_gt), 1)\n",
        "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "    axes[row].imshow(np.clip(im, 0, 1))\n",
        "    axes[row].axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F88uhMdjD0AZ"
      },
      "source": [
        "You could run your segmentation network on internet images and see how it performs there. You could even try to do the background blurring effect we introduced above!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w3Zw02rsdMK"
      },
      "source": [
        "# Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrJfQmqvASYC"
      },
      "source": [
        "## Task 1: Non-linear Transformations for Representation Learning\n",
        "\n",
        "PCA is a standard dimensionality reduction technique that uses a linear transformation. In this task we are going to define two autoencoders, one convolutional and one without using any convolutional layer, that are capable of learning a non-linear transformation to reduce the dimensionality of the input MNIST image, and we will compare those autoencoders to PCA. A way to evaluate the quality of the representations produced by both PCA and the autoencoder is to learn a classifier on top of those representations with reduced dimensionality. If the classifier has high accuracy, then the representations can be considered meaningful. In our case, we will use representations with dimensionality 10 and we will use those representations to train a linear classifier, which is defined in the code below.\n",
        "\n",
        "The given example architectures for both the non-convolutional and the convolutional autoencoder already produce, after training them, representations of similar quality as PCA. Modify the given architectures and try to increase the accuracy when training a linear classifier on top of the autoencoder representations. The code given below may help you understand the pipeline.\n",
        "\n",
        "As in past notebooks, treat the MNIST test set as your validation set. You can use any of the layers and techniques presented in past notebooks, the only constraints are that the non-convolutional autoencoder should not have any Conv2d layer, that the convolutional autoencoder should include Conv2d layers, and that the representation vector should have dimensionality 10.\n",
        "\n",
        "**Report**:\n",
        "* Table with the accuracy of `classifier` (defined below) obtained with the representations from your two proposed  autoencoder architectures (non-convolutional and convolutional autoencoder) and also with PCA with 10 components in the training set and the validation set. Additionally, include in the table the MSE error in both training and validation set for your non-convolutional autoencoder, for your convolutional autoencoder and for the PCA method. State clearly your two final autoencoder architectures and discuss the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUSZnVLp1kq8"
      },
      "source": [
        "We will use MNIST for this task. First, we resize all the images to have a resolution of 32x32, which will make the definition of the convolutional autoencoder easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVL-ur3qr74_"
      },
      "source": [
        "# Fix seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Load MNIST and preprocess: normalize and resize to 32x32 with 1 channel\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "def resize_dataset(dataset):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    for img, label in dataset:\n",
        "        # img shape (1,28,28), convert to numpy\n",
        "        img_np = img.numpy().transpose(1,2,0)  # (28,28,1)\n",
        "        img_resized = resize(img_np, (32,32,1), anti_aliasing=True).astype(np.float32)\n",
        "        imgs.append(img_resized)\n",
        "        labels.append(label)\n",
        "    imgs = np.array(imgs)\n",
        "    labels = np.array(labels)\n",
        "    return imgs, labels\n",
        "\n",
        "x_train_32, y_train = resize_dataset(train_dataset)\n",
        "x_test_32, y_test = resize_dataset(test_dataset)\n",
        "\n",
        "# Convert numpy arrays to torch tensors\n",
        "x_train_32 = torch.tensor(x_train_32).permute(0,3,1,2)  # (N,1,32,32)\n",
        "x_test_32 = torch.tensor(x_test_32).permute(0,3,1,2)\n",
        "y_train = torch.tensor(y_train).long()\n",
        "y_test = torch.tensor(y_test).long()\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(x_train_32, x_train_32), batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(x_test_32, x_test_32), batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7WLHdZ79Mzk"
      },
      "source": [
        "You can modify the code below to define your non-convolutional autoencoder. You can use any layer you want apart from `Conv2D` layers for this autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Non-convolutional Autoencoder ###\n",
        "class NonConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(32*32, 10),  # Representation with dimensionality 10\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(10, 32*32),\n",
        "            nn.Unflatten(1, (1, 32, 32)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        z = self.encoder(x)\n",
        "        x_recon = self.decoder(z)\n",
        "        return x_recon\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.flatten(x)\n",
        "        z = self.encoder(x)\n",
        "        return z"
      ],
      "metadata": {
        "id": "vjyu1YvmlAQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5XsNpKrt2nS"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "autoencoder = NonConvAutoencoder()\n",
        "print(torchinfo.summary(autoencoder, input_size=(1, 1, 32, 32)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    autoencoder,\n",
        "    train_loader,\n",
        "    nn.MSELoss(),\n",
        "    optim.Adam(autoencoder.parameters()),\n",
        "    num_epochs = 20,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58upUwtD9S7x"
      },
      "source": [
        "You can modify the code below to define your convolutional autoencoder. For this autoencoder you need to include `Conv2D` layers in your design, but you can use any other layer too. We show an example of a simple convolutional architecture below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_s90fEyL8TB"
      },
      "source": [
        "### Convolutional Autoencoder ###\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # output: 32x16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32*16*16, 10)  # representation 10 dim\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(10, 1*16*16),\n",
        "            nn.Unflatten(1, (1,16,16)),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),  # upsample to 32x32\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_recon = self.decoder(z)\n",
        "        return x_recon\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can modify the number of epochs or other hyperparameters\n",
        "set_seed(42)\n",
        "\n",
        "conv_autoencoder = ConvAutoencoder()\n",
        "print(torchinfo.summary(conv_autoencoder, input_size=(1, 1, 32, 32)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    conv_autoencoder,\n",
        "    train_loader,\n",
        "    nn.MSELoss(),\n",
        "    optim.Adam(conv_autoencoder.parameters()),\n",
        "    num_epochs = 20,\n",
        ")"
      ],
      "metadata": {
        "id": "pyJ_kM0vmYCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-bJ9xiNZzeA"
      },
      "source": [
        "Below you have the code you will use to train the classifier. We first extract the representations using any of the two autoencoders we just trained or PCA and then we train the classifier on top, which is just a simple Dense layer. Better representations should make it easier for the given simple classifier to separate the classes and, therefore, have larger accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### PCA ###\n",
        "pca = PCA(n_components=10)\n",
        "pca.fit(x_train_32.numpy().reshape(len(x_train_32), -1))\n",
        "\n",
        "## We compute the representations for the different methods\n",
        "representation_pca_train = pca.transform(x_train_32.numpy().reshape(len(x_train_32), -1))\n",
        "representation_pca_test = pca.transform(x_test_32.numpy().reshape(len(x_test_32), -1))\n",
        "\n",
        "# predict_representation is defined at the beginning of this notebook\n",
        "representation_auto_train = predict_representation(autoencoder, x_train_32)\n",
        "representation_auto_test = predict_representation(autoencoder, x_test_32)\n",
        "representation_conv_auto_train = predict_representation(conv_autoencoder, x_train_32)\n",
        "representation_conv_auto_test = predict_representation(conv_autoencoder, x_test_32)\n",
        "\n",
        "# Compute PCA reconstruction MSE\n",
        "reconst_train = pca.inverse_transform(representation_pca_train).reshape(-1,1,32,32)\n",
        "train_mse_pca = ((reconst_train - x_train_32.numpy())**2).mean()\n",
        "\n",
        "reconst_test = pca.inverse_transform(representation_pca_test).reshape(-1,1,32,32)\n",
        "test_mse_pca = ((reconst_test - x_test_32.numpy())**2).mean()\n",
        "\n",
        "# We print the MSE for PCA, which you need to include on the table\n",
        "print(f'PCA Train MSE: {train_mse_pca:.4f}')\n",
        "print(f'PCA Test MSE: {test_mse_pca:.4f}')"
      ],
      "metadata": {
        "id": "dASdxeY9noR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Linear classifier to test representation quality, DO NOT MODIFY IT!\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=10, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "LAvclywongkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb3RzYrZ2hTZ"
      },
      "source": [
        "def train_classifier(X_train, y_train, X_val, y_val, epochs=30):\n",
        "    set_seed(42)\n",
        "\n",
        "    X_train = torch.tensor(X_train).float()\n",
        "    X_val = torch.tensor(X_val).float()\n",
        "\n",
        "    train_ds = TensorDataset(X_train, y_train)\n",
        "    val_ds = TensorDataset(X_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=128)\n",
        "\n",
        "    model = LinearClassifier(input_dim=X_train.shape[1], num_classes=10).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation accuracy\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                correct += (preds == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "        acc = correct / total\n",
        "\n",
        "        if (epoch+1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] Validation Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "    return acc\n",
        "\n",
        "print(\"Training classifier on PCA representations\")\n",
        "acc_pca = train_classifier(representation_pca_train, y_train, representation_pca_test, y_test)\n",
        "print()\n",
        "\n",
        "print(\"Training classifier on Non-Convolutional Autoencoder representations\")\n",
        "acc_auto = train_classifier(representation_auto_train, y_train, representation_auto_test, y_test)\n",
        "print()\n",
        "\n",
        "print(\"Training classifier on Convolutional Autoencoder representations\")\n",
        "acc_conv_auto = train_classifier(representation_conv_auto_train, y_train, representation_conv_auto_test, y_test)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CogmYPqKAoqt"
      },
      "source": [
        "The code below can help you visualize the quality of the learnt representations. tSNE is a dimensionality reduction technique that leads to nice plots, so we reduce the representations of dimensionality 10 we just learnt to dimensionality 2 via tSNE and plot it. You do not have to include the figures in the report, it is just a qualitative way for you to see the quality of your representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUka6avw5Oh3"
      },
      "source": [
        "## We use tSNE for our dimensionality reduction technique so we can\n",
        "## plot the features using a 2D plot as it leads to nice plots.\n",
        "## However, tSNE is tricky to use as a general dimensionality reduction method\n",
        "## for clustering due to issues mentioned here: https://distill.pub/2016/misread-tsne/\n",
        "## TSNE: https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n",
        "## Nice article explaining shortcomings: https://distill.pub/2016/misread-tsne/\n",
        "\n",
        "## Use these parameters, the plots are highly dependent on perplexity value\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, max_iter=500, n_jobs=-1)\n",
        "representation_tsne = tsne.fit_transform(representation_auto_test)\n",
        "plot_representation_label(representation_tsne, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqqx1Q1WMCRY"
      },
      "source": [
        "You can also check how the reconstructed images look with the autoencoders you just trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEcnL2fMMBtP"
      },
      "source": [
        "ind = np.random.randint(x_test.shape[0] -  1)\n",
        "## The function below is defined in the tutorial\n",
        "plot_recons_original(np.expand_dims(x_test_32[ind],0), y_test[ind], conv_autoencoder, size_image=(32,32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJeMc4vKo7He"
      },
      "source": [
        "## Task 2: Custom Loss Functions\n",
        "\n",
        "In Image-to-Image tasks, researchers have found that an approach to improve the robustness of autoencoders is to replace the quadratic error with a loss function that is more robust to outliers.\n",
        "\n",
        "There is a lot of interest in defining which loss function helps the most specific tasks. For instance, super-image resolution and image denoising may have different optimum loss functions, even though, they are trained in a very similar manner. Therefore, in this task, we will focus on using multiple loss functions to train an image denoise model. Sometimes, you may need to use a loss function that is not defined in Keras. If that happens, you can define it yourself and use it in the model.compile() module. We explain now how to do that.\n",
        "\n",
        "You must define some variables:\n",
        "\n",
        "* **True values** are those that we are aiming to generate, e.g. GT images.\n",
        "* **Predicted values** are those that the network has generated,  e.g. denoised images.\n",
        "* **Loss value** is the computed loss between true and predicted values,  e.g. MSE value.\n",
        "\n",
        "The common structure for the custom loss method is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4XZmYuhAQE9"
      },
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        return []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3XEZlhqAQFf"
      },
      "source": [
        "We are going to use TinyImagenet, and add synthetic noise as before. This task requires a lot of RAM, thus, before starting it, please clean your RAM memory by restarting the Colab session."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download TinyImageNet\n",
        "! git clone https://github.com/seshuad/IMagenet"
      ],
      "metadata": {
        "id": "OWEjAN7Kp_TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzxuuXj_AQFf"
      },
      "source": [
        "class TinyImageNetNoisyDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None, noise_std=0.2):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.noise_std = noise_std\n",
        "        self.image_files = []\n",
        "\n",
        "        if self.split == 'train':\n",
        "            # For training data, images are in subfolders per class\n",
        "            wnids_path = os.path.join(root_dir, 'tiny-imagenet-200', 'wnids.txt')\n",
        "            with open(wnids_path, 'r') as f:\n",
        "                classes = [line.strip() for line in f]\n",
        "            for cls in classes:\n",
        "                image_dir = os.path.join(root_dir, 'tiny-imagenet-200', split, cls, 'images')\n",
        "                for img_file in os.listdir(image_dir):\n",
        "                    if img_file.endswith('.JPEG'):\n",
        "                        self.image_files.append(os.path.join(image_dir, img_file))\n",
        "\n",
        "        elif self.split == 'val':\n",
        "            # For validation data, images are in a single folder and annotations are in a file\n",
        "            annotations_path = os.path.join(root_dir, 'tiny-imagenet-200', split, 'val_annotations.txt')\n",
        "            with open(annotations_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    img_name, _, _, _, _, _ = line.strip().split('\\t')\n",
        "                    self.image_files.append(os.path.join(root_dir, 'tiny-imagenet-200', split, 'images', img_name))\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid split: {split}. Use 'train' or 'val'.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        noisy_img = img + torch.randn_like(img) * self.noise_std\n",
        "        noisy_img = torch.clamp(noisy_img, 0, 1)\n",
        "\n",
        "        return noisy_img, img  # return noisy input and clean target\n",
        "\n",
        "# Image transformations (resizing and normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Corrected root_dir to point to the base directory of TinyImageNet\n",
        "train_dataset = TinyImageNetNoisyDataset('./IMagenet', transform=transform)\n",
        "test_dataset = TinyImageNetNoisyDataset('./IMagenet', split='val', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GKRQn41JpXh"
      },
      "source": [
        "We are going to use the UNet architecture for this task, however, you could use any autoencoder of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdvzRDqvJeqO"
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, 1024, 3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Decoder\n",
        "        self.up6 = nn.Conv2d(1024, 512, 3, padding=1)\n",
        "        self.up7 = nn.Conv2d(512, 256, 3, padding=1)\n",
        "        self.up8 = nn.Conv2d(256, 128, 3, padding=1)\n",
        "        self.up9 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "\n",
        "        self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.conv_final = nn.Conv2d(64, 3, 3, padding=1)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        c1 = self.relu(self.conv1(x))\n",
        "        p1 = self.pool(c1)\n",
        "\n",
        "        c2 = self.relu(self.conv2(p1))\n",
        "        p2 = self.pool(c2)\n",
        "\n",
        "        c3 = self.relu(self.conv3(p2))\n",
        "        p3 = self.pool(c3)\n",
        "\n",
        "        c4 = self.relu(self.conv4(p3))\n",
        "        d4 = self.dropout(c4)\n",
        "        p4 = self.pool(d4)\n",
        "\n",
        "        c5 = self.relu(self.conv5(p4))\n",
        "        d5 = self.dropout(c5)\n",
        "\n",
        "        # Decoder\n",
        "        up6 = self.up_sample(d5)\n",
        "        up6 = self.relu(self.up6(up6))\n",
        "        merge6 = torch.cat([d4, up6], dim=1)\n",
        "        c6 = self.relu(self.up6(merge6))\n",
        "\n",
        "        up7 = self.up_sample(c6)\n",
        "        up7 = self.relu(self.up7(up7))\n",
        "        merge7 = torch.cat([c3, up7], dim=1)\n",
        "        c7 = self.relu(self.up7(merge7))\n",
        "\n",
        "        up8 = self.up_sample(c7)\n",
        "        up8 = self.relu(self.up8(up8))\n",
        "        merge8 = torch.cat([c2, up8], dim=1)\n",
        "        c8 = self.relu(self.up8(merge8))\n",
        "\n",
        "        up9 = self.up_sample(c8)\n",
        "        up9 = self.relu(self.up9(up9))\n",
        "        merge9 = torch.cat([c1, up9], dim=1)\n",
        "        c9 = self.relu(self.up9(merge9))\n",
        "\n",
        "        output = self.conv_final(c9)\n",
        "        output = torch.sigmoid(output)  # constrain output to [0,1]\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKwr7T1nJG2S"
      },
      "source": [
        "Now, an example of how to train UNet architecture with a custom MSE loss function:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        return torch.mean((predicted - target) ** 2)"
      ],
      "metadata": {
        "id": "9Pgxz_-weyy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = UNet()\n",
        "print(torchinfo.summary(model, input_size=(1, 3, 64, 64)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    CustomMSELoss(),\n",
        "    optim.Adam(model.parameters(), lr=1e-4),\n",
        "    num_epochs = 10,\n",
        ")\n",
        "print()\n",
        "\n",
        "evaluate(model, test_loader, CustomMSELoss())"
      ],
      "metadata": {
        "id": "MtP8WmAWe74_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCptRFISDnrL"
      },
      "source": [
        "Use the following code for visualising examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVe3YrKtDmn1"
      },
      "source": [
        "def show_noisy_denoised_clean(idx=0):\n",
        "    model.eval()\n",
        "    noisy_img, clean_img = test_dataset[idx]\n",
        "    noisy_img_t = noisy_img.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        denoised_img = model(noisy_img_t).cpu().squeeze(0)\n",
        "\n",
        "    # Convert tensors to numpy arrays for plotting\n",
        "    noisy_np = noisy_img.permute(1, 2, 0).numpy()\n",
        "    denoised_np = denoised_img.permute(1, 2, 0).numpy()\n",
        "    clean_np = clean_img.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Plot side by side\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(12,4))\n",
        "    axs[0].imshow(np.clip(noisy_np, 0, 1))\n",
        "    axs[0].set_title('Noisy Input')\n",
        "    axs[0].axis('off')\n",
        "    axs[1].imshow(np.clip(denoised_np, 0, 1))\n",
        "    axs[1].set_title('Denoised Output')\n",
        "    axs[1].axis('off')\n",
        "    axs[2].imshow(np.clip(clean_np, 0, 1))\n",
        "    axs[2].set_title('Clean Target')\n",
        "    axs[2].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_noisy_denoised_clean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChyCg8V9AQFi"
      },
      "source": [
        "Try to use a different loss function and see which one gives you the best result. Some well-known loss functions for image denoising are:\n",
        "\n",
        "*  Structural Similarity Index ([SSIM](https://en.wikipedia.org/wiki/Structural_similarity))\n",
        "*  Multiscale Structural Similarity Index ([MS-SSIM](https://ieeexplore.ieee.org/document/1292216?arnumber=1292216&tag=1))\n",
        "* 1 / [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)\n",
        "* MAE\n",
        "* [L0](https://arxiv.org/pdf/1803.04189.pdf)\n",
        "\n",
        "Check the [Noise2Noise](https://arxiv.org/pdf/1803.04189.pdf) paper to learn more about alternative losses.\n",
        "\n",
        "**Report:**\n",
        "*  In this task, you are asked to build a table containing the MSE results on the test split of models trained with different loss functions. Use two or three different loss functions from the previous list and discuss the differences you observe. Report denoised images to support your arguments. You may need to modify the UNet model definition to use some of the previous losses.\n",
        "\n",
        "Use the following code and write your custom loss function within the provided `CustomLoss` class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6v_-PShE-Jn"
      },
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        '''\n",
        "        Define your loss here\n",
        "        '''\n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}