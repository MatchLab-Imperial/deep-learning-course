{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "generative_ai_disabled": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/03_Network_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91E7ozNTWvS9"
      },
      "source": [
        "# Network Training\n",
        "\n",
        "In this tutorial, we introduce different topics related to the training of deep neural networks, including data augmentation, optimization/regularization techniques, weight initializations and hyperparameters tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "\n",
        "import copy\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "import torchinfo\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "siAxFKDYH7rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to control randomness for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "HQ2g8s-zZeVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVa-ZTYsXhNT"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "We load the MNIST dataset and define a model for digit classification using our function `get_data_model`, which returns a fresh untrained model along with PyTorch data loaders for training and testing. The `train` function performs the training loop with a given loss function and optimizer, recording the training and validation losses across epochs. Finally, `plot_history` visualizes these losses to track convergence and compare different experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aGBJBVmXd8b"
      },
      "source": [
        "def get_data_model(train_transform=None, test_transform=None):\n",
        "    # Transform: flatten images to 784 vector and normalize to [0,1]\n",
        "    transform_default = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Lambda(lambda x: x.view(-1))\n",
        "    ])\n",
        "    train_transform = train_transform if train_transform is not None else transform_default\n",
        "    test_transform = test_transform if test_transform is not None else transform_default\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "    # Model definition\n",
        "    class SimpleMLP(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(SimpleMLP, self).__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(784, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 10),\n",
        "            )\n",
        "        def forward(self, x):\n",
        "            return self.net(x)\n",
        "\n",
        "    model = SimpleMLP()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    return model, device, train_loader, test_loader\n",
        "\n",
        "model, device, train_loader, test_loader = get_data_model()\n",
        "print(torchinfo.summary(model, input_size=(1, 784)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, criterion, optimizer, num_epochs=10, val_loader=None):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Compute predictions and accuracy\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(epoch_acc)\n",
        "\n",
        "        if val_loader:\n",
        "            model.eval()\n",
        "            val_running_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, targets)\n",
        "                    val_running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                    preds = torch.argmax(outputs, dim=1)\n",
        "                    val_correct += (preds == targets).sum().item()\n",
        "                    val_total += targets.size(0)\n",
        "\n",
        "            val_loss = val_running_loss / val_total\n",
        "            val_acc = val_correct / val_total\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
        "              + f\"Train loss: {epoch_loss:.4f}, Train acc: {epoch_acc*100:.2f}%\"\n",
        "              + (f\", Val loss: {val_loss:.4f}, Val acc: {val_acc*100:.2f}%\" if val_loader else \"\"))\n",
        "\n",
        "    history = {\n",
        "        'loss': train_losses,\n",
        "        'accuracy': train_accs,\n",
        "    }\n",
        "    if val_loader:\n",
        "        history['val_loss'] = val_losses\n",
        "        history['val_accuracy'] = val_accs\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "awDrxEId2mM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history, metric=None):\n",
        "    if metric is not None and metric in history:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        fig, axes = plt.subplots(2, 1)\n",
        "\n",
        "        # Top plot: Accuracy\n",
        "        axes[0].plot(history[metric])\n",
        "        if 'val_' + metric in history:\n",
        "            axes[0].plot(history['val_' + metric])\n",
        "            axes[0].legend(['Train', 'Val'])\n",
        "        axes[0].set_title('Model ' + metric.capitalize())\n",
        "        axes[0].set_ylabel(metric.capitalize())\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].grid(True)\n",
        "\n",
        "        fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "        # Bottom plot: Loss\n",
        "        axes[1].plot(history['loss'])\n",
        "        if 'val_loss' in history:\n",
        "            axes[1].plot(history['val_loss'])\n",
        "            axes[1].legend(['Train', 'Val'])\n",
        "        axes[1].set_title('Model Loss')\n",
        "        axes[1].set_ylabel('Loss')\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].grid(True)\n",
        "    else:\n",
        "        plt.figure(figsize=(8, 2))\n",
        "        plt.plot(history['loss'])\n",
        "        if 'val_loss' in history:\n",
        "            plt.plot(history['val_loss'])\n",
        "            plt.legend(['Train', 'Val'])\n",
        "        plt.title('Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "JJlTAr1Mz52w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRx68ymWLCT7"
      },
      "source": [
        "## Optimizers\n",
        "\n",
        "In a deep learning problem, the weights of a network are updated to minimize the loss in each optimization step. Optimizers define the specific method used to update in each timestep the weights of the network. For example, the vanilla Stochastic Gradient Descent (SGD) only uses the gradient of the weight with respect to the loss in the current timestep $t$, along with the learning rate, to update the weights. However, other optimizers use more sophisticate methods (e.g., keeping track of past gradient updates to use as momentum) to compute the weights' updates.\n",
        "\n",
        "Since there are optimizers more prone to get stuck in local minima, the choice of the optimizer can potentially affect both the final performance and the speed of convergence.  A nice visualization of such behavior is the following animation, where you can notice some methods, i.e., Adadelta (yellow), and Rmsprop (black) converge significantly faster than SGD (red), which gets stuck in a local minimum.\n",
        "\n",
        "![](http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif)\n",
        "\n",
        "The animation above is from [Sebastian Ruder's blog](http://ruder.io/optimizing-gradient-descent/), who wrote an interesting article showing the formulation and properties of the different optimizers. We are not going to analyze the properties of the different optimizers, but you can read the blog post if you are interested in how optimizers work. As a rule of thumb, Adam is usually easier to tune due to the adaptive learning rate, whereas SGD with momentum [has been shown](https://arxiv.org/pdf/1712.07628.pdf) to reach better results when tuned correctly.\n",
        "\n",
        "In this tutorial, we will follow a hands-on approach and focus mainly on how to use [optimizers in PyTorch](https://pytorch.org/docs/stable/optim.html). PyTorch requires you to create an optimizer object explicitly by passing it the model parameters and any desired hyperparameters, such as the learning rate. For example, the following code creates an Adam optimizer with default parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9od5lUWMWu5X"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "model, device, train_loader, test_loader = get_data_model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "history = train(model, device, train_loader, criterion, optimizer, num_epochs=10, val_loader=test_loader)\n",
        "\n",
        "plot_history(history, \"accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGIIvNFPRZI3"
      },
      "source": [
        "However, it is common to adjust optimizer parameters such as the learning rate, which controls how quickly the weights are updated during training. In PyTorch, you do this when you create the optimizer by specifying the desired settings. For example, the following code uses Adam with a custom learning rate of 0.0003:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCMtC0nERXhi"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "model, device, train_loader, test_loader = get_data_model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "history = train(model, device, train_loader, criterion, optimizer, num_epochs=10, val_loader=test_loader)\n",
        "\n",
        "plot_history(history, \"accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yDSecajTyc6"
      },
      "source": [
        "## Initializers\n",
        "\n",
        "\n",
        "Neural Networks have a large number of weights that need to be initialized properly. Weight initialization is a crucial step in tuning neural networks as different initial values can lead the model to reach different local minima. If your initial weights are too small, the network may have problems learning due to small/vanishing gradients. However, if they are too large, you may run into exploding gradients or you could saturate an activation in the network (e.g. a sigmoid). Hence, we need a proper initialization with not too large nor not too small weights. In [this link](https://www.deeplearning.ai/ai-notes/initialization/) you can find an explanation of the importance of weight initialization.  \n",
        "\n",
        "The weights are usually initialized randomly by different algorithms, such as Xavier, He, or LeCun initialization, each designed to have specific properties under certain assumptions.\n",
        "\n",
        "In PyTorch, you set the initialization explicitly by calling functions from `torch.nn.init` on the model's parameters after you create the layers. The available initialization methods in PyTorch are listed in the documentation. For example, to initialize a `Linear` layer so that the weights (`A` in the equation $y=Ax+b$) are drawn from a normal distribution (with a given standard deviation) and the biases (`b`) are set to zero, you can use the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6lzySByXcMB"
      },
      "source": [
        "layer = nn.Linear(in_features=784, out_features=512)\n",
        "\n",
        "# Initialize weights with a normal distribution (mean=0, std=0.05)\n",
        "nn.init.normal_(layer.weight, mean=0.0, std=0.05)\n",
        "\n",
        "# Initialize biases to zero\n",
        "nn.init.zeros_(layer.bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF_EVmyooqYL"
      },
      "source": [
        "Now let's check the weights of the layers and see if they follow the distributions we set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgHC0CSmowIK"
      },
      "source": [
        "# Access weights and biases\n",
        "weight_tensor = layer.weight.detach().numpy()\n",
        "bias_tensor = layer.bias.detach().numpy()\n",
        "\n",
        "print(weight_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLDzNuqUp2M8"
      },
      "source": [
        "# Now let's check that the mean is 0 and stddev is 0.05\n",
        "print(\"Mean: {:.4f}\\nStandard Deviation: {:.4f}\".format(weight_tensor.mean(), weight_tensor.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uohj6D4rqTfT"
      },
      "source": [
        "# Let's print the bias now\n",
        "print(bias_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69NkDCipqbmg"
      },
      "source": [
        "By default, the kernel weights in PyTorch `nn.Linear` layers are initialized from a uniform distribution with bounds determined by the number of input units. Specifically, the weights are sampled uniformly from the range [-1‚ÄØ/‚ÄØ‚àöf_in, +1‚ÄØ/‚ÄØ‚àöf_in], where *f_in* is the number of input features. The bias is initialized uniformly in the same range. Another commonly used initializer is `kaiming_normal`, which draws weights from a normal distribution scaled for ReLU activations. In most cases, the default initialization in PyTorch works well and allows models to train properly without any changes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-8ittW2L1DI"
      },
      "source": [
        "## Loss\n",
        "\n",
        "Another important step in training deep neural networks is the choice of the loss function, which depends on your specific problem. The full list of standard losses in PyTorch is available [here](https://pytorch.org/docs/stable/nn.html#loss-functions). Although in past tutorials we already used some losses, we now introduce two typical ones: cross-entropy and mean squared error, which are commonly used for classification and regression tasks, respectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T8aSdjCtSSO"
      },
      "source": [
        "### Classification\n",
        "\n",
        "For classification problems, the standard loss used is the cross-entropy loss. For the binary case, the formula is $L = y\\log(p) + (1-y)\\log(1-p)$, where $p$ is a probability value between $[0, 1]$.\n",
        "Typically, a [Sigmoid activation](https://en.wikipedia.org/wiki/Sigmoid_function) is applied in the binary case to constrain the output to be between 0 and 1.  \n",
        "\n",
        "In PyTorch, this binary classification loss is implemented as `nn.BCELoss` if you apply the sigmoid activation yourself, or as `nn.BCEWithLogitsLoss`, which combines a sigmoid layer and the binary cross-entropy loss in a single function for numerical stability. This loss expects targets as a vector of values in $[0, 1]$ (usually either 0 or 1) for each input element.  \n",
        "\n",
        "Below, we show an example of binary classification in PyTorch. To do so, we transform MNIST into a binary classification problem by classifying whether a digit is between 0‚Äì4 or 5‚Äì9.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUQhLNxr5VA7"
      },
      "source": [
        "# Change to single output neuron\n",
        "model, device, train_loader, test_loader = get_data_model()\n",
        "model.net[-1] = nn.Linear(512, 1)\n",
        "print(torchinfo.summary(model, input_size=(1, 784)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload MNIST datasets with transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Convert targets to binary: 0 if digit < 5, 1 if >= 5\n",
        "train_dataset.targets = (train_dataset.targets >= 5).long()\n",
        "test_dataset.targets = (test_dataset.targets >= 5).long()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train_binary(model, device, train_loader, criterion, optimizer, num_epochs=3, val_loader=None):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze(1)\n",
        "            loss = criterion(outputs, targets.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct += (preds.int() == targets).sum().item()\n",
        "            total += inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(epoch_acc)\n",
        "\n",
        "        if val_loader:\n",
        "            model.eval()\n",
        "            val_running_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    outputs = model(inputs).squeeze(1)\n",
        "                    loss = criterion(outputs, targets.float())\n",
        "                    val_running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                    preds = torch.sigmoid(outputs) > 0.5\n",
        "                    val_correct += (preds.int() == targets).sum().item()\n",
        "                    val_total += inputs.size(0)\n",
        "\n",
        "            val_loss = val_running_loss / val_total\n",
        "            val_acc = val_correct / val_total\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train loss: {epoch_loss:.4f}, Train acc: {epoch_acc*100:.2f}%\"\n",
        "              + (f\", Val loss: {val_loss:.4f}, Val acc: {val_acc*100:.2f}%\" if val_loader else \"\"))\n",
        "\n",
        "    history = {\n",
        "        'loss': train_losses,\n",
        "        'accuracy': train_accs\n",
        "    }\n",
        "    if val_loader:\n",
        "        history['val_loss'] = val_losses\n",
        "        history['val_accuracy'] = val_accs\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "TA1-onmkCscw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "history = train_binary(model, device, train_loader, criterion, optimizer, num_epochs=3,\n",
        "                       val_loader=test_loader)\n",
        "plot_history(history, 'accuracy')"
      ],
      "metadata": {
        "id": "-X1gDDXBC2NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jvNHqg--XYy"
      },
      "source": [
        "When the number of classes is higher than 2, we use the cross-entropy loss, which is $L = -\\sum_i y_i\\log(p_i)$.\n",
        "\n",
        "n PyTorch, this loss is implemented as `nn.CrossEntropyLoss`. It expects the model outputs to be raw logits (unnormalized scores) and the targets to be integer class indices in the range\n",
        "[0,ùê∂), where ùê∂ is the number of classes. Unlike some other frameworks, you do not need to one-hot encode the labels manually, as the function handles this internally.\n",
        "\n",
        "Let‚Äôs see an example using the MNIST data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_YBOLN77S1I"
      },
      "source": [
        "# Load MNIST with flattening transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Show first 5 labels\n",
        "for i in range(5):\n",
        "    label = train_dataset.targets[i].item()\n",
        "    print(f\"Label Value: {label}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR_lJgB6CfFd"
      },
      "source": [
        "The input for the cross-entropy needs to be a probability distribution, meaning that the sum of the elements should be $\\sum_i p_i = 1$. However, in PyTorch you do not need to apply the [Softmax activation](https://en.wikipedia.org/wiki/Softmax_function) manually, because `nn.CrossEntropyLoss` automatically applies the `LogSoftmax` function internally. This means you can pass the raw, unnormalized outputs (logits) of your model directly to the loss function and use the predefined training utilities without any extra steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVTAPlc_B99R"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Load model and data (MNIST)\n",
        "model, device, train_loader, test_loader = get_data_model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "history = train(\n",
        "    model,\n",
        "    device,\n",
        "    train_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    num_epochs=3,\n",
        "    val_loader=test_loader\n",
        ")\n",
        "\n",
        "plot_history(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFuOCwNftQgH"
      },
      "source": [
        "### Regression\n",
        "In regression problems, it is common to use as losses the Mean Squared Error (MSE) or Mean Absolute Error (MAE). We now show a quick example using the California Housing dataset, which contains a set of input features and we aim to predict the price of the house."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li20u1NjrwS5"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split train/test\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale inputs\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "# Convert to tensors\n",
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "x_test_tensor = torch.from_numpy(x_test).float()\n",
        "y_test_tensor = torch.from_numpy(y_test).float()\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
        "test_dataset = torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JO5_oiCyPzR"
      },
      "source": [
        "To predict float numbers as the price of the houses, we need a network that has only one output, so we define the `Regressor`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSJH6_Mfsjp-"
      },
      "source": [
        "class Regressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(8, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_regression(model, device, train_loader, criterion, optimizer, num_epochs=100, val_loader=None):\n",
        "    train_mae = []\n",
        "    val_mae = []\n",
        "    train_mse = []\n",
        "    val_mse = []\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    mae_criterion = nn.L1Loss()\n",
        "    mse_criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_mae = 0.0\n",
        "        running_mse = 0.0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            mae_loss = mae_criterion(outputs, targets)\n",
        "            mse_loss = mse_criterion(outputs, targets)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_mae += mae_loss.item() * inputs.size(0)\n",
        "            running_mse += mse_loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_mae = running_mae / len(train_loader.dataset)\n",
        "        epoch_mse = running_mse / len(train_loader.dataset)\n",
        "        train_mae.append(epoch_mae)\n",
        "        train_mse.append(epoch_mse)\n",
        "\n",
        "        if val_loader:\n",
        "            model.eval()\n",
        "            val_running_mae = 0.0\n",
        "            val_running_mse = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    mae_loss = mae_criterion(outputs, targets)\n",
        "                    mse_loss = mse_criterion(outputs, targets)\n",
        "                    val_running_mae += mae_loss.item() * inputs.size(0)\n",
        "                    val_running_mse += mse_loss.item() * inputs.size(0)\n",
        "            val_mae_epoch = val_running_mae / len(val_loader.dataset)\n",
        "            val_mse_epoch = val_running_mse / len(val_loader.dataset)\n",
        "            val_mae.append(val_mae_epoch)\n",
        "            val_mse.append(val_mse_epoch)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
        "            f\"Train MAE: {epoch_mae:.4f}, Train MSE: {epoch_mse:.4f}\"\n",
        "            + (\n",
        "                f\", Val MAE: {val_mae_epoch:.4f}, Val MSE: {val_mse_epoch:.4f}\"\n",
        "                if val_loader\n",
        "                else \"\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    history = {\n",
        "        'mae': train_mae,\n",
        "        'mse': train_mse,\n",
        "    }\n",
        "    if val_loader:\n",
        "        history['val_mae'] = val_mae\n",
        "        history['val_mse'] = val_mse\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "NaufnS0QLYiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2xZRys8zNsD"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "model_mae = Regressor()\n",
        "criterion_mae = nn.L1Loss()\n",
        "optimizer_mae = optim.Adam(model_mae.parameters(), lr=3e-4)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "history_mae = train_regression(\n",
        "    model_mae,\n",
        "    device,\n",
        "    train_loader,\n",
        "    criterion_mae,\n",
        "    optimizer_mae,\n",
        "    num_epochs=100,\n",
        "    val_loader=test_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcJ-jplbVA4U"
      },
      "source": [
        "Now we use the MSE loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XYKtV1GzMQg"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "model_mse = Regressor()\n",
        "criterion_mse = nn.MSELoss()\n",
        "optimizer_mse = optim.Adam(model_mse.parameters(), lr=3e-4)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "history_mse = train_regression(\n",
        "    model_mse,\n",
        "    device,\n",
        "    train_loader,\n",
        "    criterion_mse,\n",
        "    optimizer_mse,\n",
        "    num_epochs=100,\n",
        "    val_loader=test_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBZB_MDCw1Os"
      },
      "source": [
        "def plot_regression_comparison(history_mae, history_mse):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Left: MAE metric comparison\n",
        "    axes[0].plot(history_mae['val_mae'], label='MAE-trained')\n",
        "    axes[0].plot(history_mse['val_mae'], label='MSE-trained')\n",
        "    axes[0].set_title('Validation MAE')\n",
        "    axes[0].set_ylabel('MAE')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Right: MSE metric comparison\n",
        "    axes[1].plot(history_mae['val_mse'], label='MAE-trained')\n",
        "    axes[1].plot(history_mse['val_mse'], label='MSE-trained')\n",
        "    axes[1].set_title('Validation MSE')\n",
        "    axes[1].set_ylabel('MSE')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_regression_comparison(history_mae, history_mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMHRntQU0mrM"
      },
      "source": [
        "def evaluate_model(model, device, loader):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    mae = nn.L1Loss()\n",
        "    mse = nn.MSELoss()\n",
        "    running_mae = 0.0\n",
        "    running_mse = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            running_mae += mae(outputs, targets).item() * inputs.size(0)\n",
        "            running_mse += mse(outputs, targets).item() * inputs.size(0)\n",
        "    mae_final = running_mae / len(loader.dataset)\n",
        "    mse_final = running_mse / len(loader.dataset)\n",
        "    return mae_final, mse_final\n",
        "\n",
        "mae_mae, mse_mae = evaluate_model(model_mae, device, test_loader)\n",
        "print(f\"MAE-trained model achieves MAE: {mae_mae:.4f} and MSE: {mse_mae:.4f}\")\n",
        "\n",
        "mae_mse, mse_mse = evaluate_model(model_mse, device, test_loader)\n",
        "print(f\"MSE-trained model achieves MAE: {mae_mse:.4f} and MSE: {mse_mse:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWr2tc7l0IdG"
      },
      "source": [
        "The resulting plots show that models trained with MAE loss often achieve lower MAE on the validation set compared to those trained with MSE loss. Similarly, models optimized with MSE loss typically yield better validation MSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_VeO_-ORHnn"
      },
      "source": [
        "A problem usually treated as a regression problem with deep learning approaches is depth estimation, where the depth of an image is estimated using the RGB information. Some other regression losses are also used in some settings (e.g. Huber loss or Reverse Huber loss, or the already introduced Mean Percentage Absolute Error), but MAE and MSE are widely used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miKutjvgtXuS"
      },
      "source": [
        "## Regularization\n",
        "As mentioned in the lecture, regularization is an effective tool to fight some of the problems we may have during training, such as vanishing/exploding gradients or overfitting to the training set. We now mention some of the usual ways to regularize our training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-VrqhSbWJ9t"
      },
      "source": [
        "### Loss Regularizers\n",
        "\n",
        "[Regularization](https://pytorch.org/docs/stable/nn.html#weight-norm) introduces penalties during optimization to discourage overly complex models. By penalizing large weight values, regularization helps constrain the model‚Äôs capacity and can reduce overfitting.\n",
        "\n",
        "In PyTorch, regularization is usually applied in two main ways:\n",
        "\n",
        "- **Weight decay**: adds an L2 penalty on the weights directly in the optimizer. This is the most common approach and is equivalent to L2 regularization.\n",
        "- **Manual penalties**: you can compute additional penalties (e.g., L1 or custom norms) on weights or activations and add them to your loss function manually.\n",
        "\n",
        "If we define the output of a layer as $y = Wx + b$, then:\n",
        "\n",
        "- **Weight decay (L2 penalty)**: encourages smaller $W$ and $b$.\n",
        "- **Custom penalties**: you can compute L1 or L2 norms of the parameters or activations and combine them with your loss.\n",
        "\n",
        "Standard penalties commonly used are L1, L2, and combinations of both.\n",
        "\n",
        "In the example below, we‚Äôll see how to compare training and validation metrics by varying the regularization strategy (e.g., different values of weight decay)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv4c80XD_Yzr"
      },
      "source": [
        "# Function to train and return final training and test accuracy\n",
        "def train_with_weight_decay(weight_decay, num_epochs=10):\n",
        "    model, device, train_loader, test_loader = get_data_model()\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate training accuracy\n",
        "    model.eval()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            correct_train += (predicted == targets).sum().item()\n",
        "            total_train += targets.size(0)\n",
        "    train_acc = correct_train / total_train\n",
        "\n",
        "    # Evaluate test accuracy\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs, 1)\n",
        "            correct_test += (predicted == targets).sum().item()\n",
        "            total_test += targets.size(0)\n",
        "    test_acc = correct_test / total_test\n",
        "\n",
        "    return train_acc, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "reg_values = [0.001, 0.0]\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "\n",
        "for reg_val in reg_values:\n",
        "    print(f\"Training with weight decay = {reg_val}\")\n",
        "    train_acc, test_acc = train_with_weight_decay(weight_decay=reg_val)\n",
        "    train_accuracy.append(train_acc)\n",
        "    test_accuracy.append(test_acc)"
      ],
      "metadata": {
        "id": "QD7FRUuhSVM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(reg_values, train_accuracy, marker='o')\n",
        "plt.plot(reg_values, test_accuracy, marker='o')\n",
        "plt.legend(['Training acc.', 'Test acc.'])\n",
        "plt.title('Accuracy vs. Weight Decay (L2 regularization)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Weight Decay')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z5U93ObvSX0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liYOHIYT8YlT"
      },
      "source": [
        "In this example, we show how a large regularization value dropped the final test accuracy, and also reduced the gap between training and test accuracy. The reason is that large regularization values put a constraint on the possible weights learnt by the network, which reduces the model capacity. Hence, you need to be careful when using regularization techniques as a large value can actually hurt the performance of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ-J7tWRTQU7"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "Dropout is another regularization technique commonly applied to neural networks. A dropout layer randomly deactivates some neurons during training, which prevents the model from relying too heavily on specific patterns and improves generalization.\n",
        "\n",
        "The dropout probability (i.e., the chance of disabling each unit) is a parameter you set when defining the layer. In evaluation mode, dropout is automatically disabled and has no effect on predictions.\n",
        "\n",
        "In PyTorch, `nn.Dropout` scales the active units during training by $1/(1-p_{drop})$ so that the expected activation magnitude remains consistent between training and evaluation.\n",
        "\n",
        "Below, we show an example where the dropout probability is set to 0.3.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqcb3TWCWe4e"
      },
      "source": [
        "prob_drop = 0.3\n",
        "drop_layer = nn.Dropout(p=prob_drop)\n",
        "\n",
        "# Random input\n",
        "x = np.random.random((1, 512)).astype(np.float32)\n",
        "input_x = torch.tensor(x)\n",
        "\n",
        "# Apply dropout in \"training\" mode\n",
        "drop_layer.train()\n",
        "y = drop_layer(input_x)\n",
        "\n",
        "print(\"Input (first 10 elements):\")\n",
        "print(x[0, :10])\n",
        "\n",
        "print(\"Output (first 10 elements):\")\n",
        "print(y[0, :10].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXhWs8c1Xw3h"
      },
      "source": [
        "We now check what percentage of elements have been set to 0, and what is the scaling value the other elements have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8njS0-XRXVsc"
      },
      "source": [
        "# Calculate drop percentage\n",
        "drop_percentage = (y == 0).sum().item() / y.numel()\n",
        "print(f\"Drop percentage (should be close to {prob_drop:.4f}): {drop_percentage:.4f}\")\n",
        "\n",
        "# Scaling factor\n",
        "nonzero_mask = y != 0\n",
        "scaling = y[nonzero_mask].sum().item() / input_x[nonzero_mask].sum().item()\n",
        "print(f\"Scaling factor (should be ~{1/(1-prob_drop):.4f}): {scaling:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BucSkwhQ_sJ-"
      },
      "source": [
        "Now we show the training curves of a model without dropout and a model with dropout. First, without dropout:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZRBM9YL9R69"
      },
      "source": [
        "model, device, train_loader, test_loader = get_data_model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "history = train(model, device, train_loader, criterion, optimizer, num_epochs=10, val_loader=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbpQAl82_2u9"
      },
      "source": [
        "Now we add Dropout and define `get_data_model_dropout`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnJDBvYe899Y"
      },
      "source": [
        "def get_data_model_dropout():\n",
        "    # Same transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.view(-1))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "    class MLPWithDropout(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(MLPWithDropout, self).__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(784, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(512, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(512, 10)\n",
        "            )\n",
        "        def forward(self, x):\n",
        "            return self.net(x)\n",
        "\n",
        "    model = MLPWithDropout()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    return model, device, train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dropout, device, train_loader, test_loader = get_data_model_dropout()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_dropout.parameters())\n",
        "history_dropout = train(\n",
        "    model_dropout,\n",
        "    device,\n",
        "    train_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    num_epochs=10,\n",
        "    val_loader=test_loader\n",
        ")"
      ],
      "metadata": {
        "id": "1UChleU8wJDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Default model:\\n\")\n",
        "plot_history(history)\n",
        "print(\"\\nModel with dropout:\\n\")\n",
        "plot_history(history_dropout)"
      ],
      "metadata": {
        "id": "F17ycnsMwUHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8wXhZPs-N-v"
      },
      "source": [
        "Notice how the validation loss stays closer to the training loss in the model with dropout compared to the original model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "gp6sUTo2wNFv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C98HL1rgVACR"
      },
      "source": [
        "### Batch Normalization\n",
        "\n",
        "Batch Normalization normalizes the features by centering them around zero and rescaling them to have variance 1, which leads to faster and more stable training. To do so, batch normalization uses the statistics of the batch. Specifically, a batch normalization layer computes the mean and the standard deviation per channel, i.e. given a feature map of dimensionality $B\\times H\\times W\\times C$ ($B$ is batch size, $H,W$ spatial dimensions and $C$ number of feature channels) the layer computes the mean $\\mu$ and standard deviation $\\sigma$ of the channels, where $\\mu$ and $\\sigma$ have dimensionality $1\\times 1\\times 1\\times C$. Then, the mean $\\mu$ and the standard deviation $\\sigma$ of the batch are expanded again to $B\\times H\\times W\\times C$ and used to standardize the input features to follow a distribution with mean 0 and variance 1. The layer output is then $y = \\gamma\\frac{x-\\mu}{\\sigma} + \\beta$, where $\\gamma$ and $\\beta$ are learnable parameters that give the network more capacity to express different distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax8OE0cbZA7X"
      },
      "source": [
        "Now we will generate a batch of 512x1 (a batch of 512 vectors of only 1 channel) from a uniform distribution under the $[0, 1)$ interval, resulting in mean 0.5 and variance 1/12. We will see how the batch normalization layer will scale the distribution to have mean 0 and variance 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IS_u-HKY1WR"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Generate random input tensor (B, C, H, W) using PyTorch\n",
        "input = torch.rand((512, 1, 10, 10), dtype=torch.float32)\n",
        "\n",
        "# Create and apply BatchNorm2d\n",
        "batch_norm = nn.BatchNorm2d(num_features=1)\n",
        "batch_norm.train()\n",
        "output = batch_norm(input)\n",
        "\n",
        "# Print first 10 samples at channel 0, position (0, 0)\n",
        "print(f\"Input: {input[:10, 0, 0, 0]}\")\n",
        "print(f\"Output: {output[:10, 0, 0, 0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILdlLzAtZ3sp"
      },
      "source": [
        "# Input mean should be ~0.5 and var ~1/12=0.0833\n",
        "print(f\"Input mean: {input.mean():.4f}, var: {input.var():.4f}\")\n",
        "# Output mean should be ~0 and var ~1\n",
        "print(f\"Output mean: {output.mean():.4f}, var: {output.var():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDOne7XbrEhL"
      },
      "source": [
        "It is important to note that batch normalization changes behaviour in evaluation mode. During training, the layer tracks and updates the moving average of $\\mu$ and $\\sigma$ given all possible training batches. $\\mu$ and $\\sigma$ are then used to normalize the testing data without using the statistics from the testing batch.\n",
        "\n",
        "Generally, using Batch Normalization results in faster training and easier convergence. It is quite standard to place Batch Normalization layers just before the activation function, but we can also achieve good performance by placing it after the activation function. So you can see blocks of Conv+BN+Activation or Conv+Activation+BN in different networks.\n",
        "\n",
        "You can add your Batch Normalization layer as any other layer in your sequential model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ck8kC2zXToa"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.BatchNorm2d(num_features=512) # Example with 512 channels\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instance Normalization\n",
        "\n",
        "Instance Normalization normalizes each sample independently by centering its features around zero and rescaling them to have variance 1. Unlike batch normalization, which uses statistics aggregated across the whole mini-batch, instance normalization computes the mean and standard deviation separately for each sample and each channel. This makes it especially useful in tasks like style transfer, where preserving per-instance contrast and feature statistics is important.\n",
        "\n",
        "Given a feature map of shape $B \\times C \\times H \\times W$ ($B$ is batch size, $C$ is the number of channels), the layer computes the mean and standard deviation across the spatial dimensions for each channel within each sample. The input is then standardized using these statistics so that every channel in each sample has approximately zero mean and unit variance.\n",
        "\n",
        "After normalization, learnable parameters $\\gamma$ (scale) and $\\beta$ (shift) are applied to each channel, allowing the network to reintroduce or adapt the normalized activations to any desired distribution."
      ],
      "metadata": {
        "id": "lHTzs1gjYKGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate the effect of instance normalization, we will generate a batch of $2048 \\times 1$ (a batch of 2048 images of size $32 \\times 32$ with only 1 channel) from a uniform distribution in the interval $[0, 1)$. This produces inputs with a mean close to 0.5 and variance around $1/12$.\n",
        "\n",
        "We will see how the instance normalization layer transforms each sample independently to have mean 0 and variance 1."
      ],
      "metadata": {
        "id": "srjM5JE7f1YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Generate random input tensor (B, C, H, W)\n",
        "input = torch.rand((2048, 1, 32, 32), dtype=torch.float32)\n",
        "\n",
        "# Create and apply InstanceNorm2d without affine transformation\n",
        "instance_norm = nn.InstanceNorm2d(num_features=1, affine=False)\n",
        "instance_norm.train()\n",
        "\n",
        "output = instance_norm(input)\n",
        "\n",
        "# Print first 10 values at channel 0, height 0, width 0\n",
        "print(f\"Input: {input[:10, 0, 0, 0]}\")\n",
        "print(f\"Output: {output[:10, 0, 0, 0]}\")"
      ],
      "metadata": {
        "id": "9p4UQPgxcCov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute per-sample mean and var before and after\n",
        "input_means = input.mean(dim=(2, 3)).squeeze(1).detach().cpu().numpy()\n",
        "input_vars = input.var(dim=(2, 3), unbiased=False).squeeze(1).detach().cpu().numpy()\n",
        "\n",
        "output_means = output.mean(dim=(2, 3)).squeeze(1).detach().cpu().numpy()\n",
        "output_vars = output.var(dim=(2, 3), unbiased=False).squeeze(1).detach().cpu().numpy()\n",
        "\n",
        "bin_edges_means = np.arange(-0.01, 0.55 + 0.01, 0.01)\n",
        "bin_edges_stds = np.arange(0.05, 1.05 + 0.015, 0.015)\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Means\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(input_means, bins=bin_edges_means, label=\"Input Means\")\n",
        "plt.hist(output_means, bins=bin_edges_means, label=\"Output Means\")\n",
        "plt.axvline(0.5, color='black', linestyle='--', label='Ref: 0.5')\n",
        "plt.axvline(0, color='red', linestyle='--', label='Ref: 0')\n",
        "plt.title(\"Per-Sample Means\")\n",
        "plt.xlabel(\"Mean\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Variances\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(input_vars, bins=bin_edges_stds, label=\"Input Variances\")\n",
        "plt.hist(output_vars, bins=bin_edges_stds, label=\"Output Variances\")\n",
        "plt.axvline(1/12, color='black', linestyle='--', label='Ref: 1/12')\n",
        "plt.axvline(1, color='red', linestyle='--', label='Ref: 1')\n",
        "plt.title(\"Per-Sample Variances\")\n",
        "plt.xlabel(\"Variance\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VawNMO6CcPjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the histograms above, we can see that the instance normalization layer has produced samples with per-sample means and variances tightly clustered around 0 and 1, as expected."
      ],
      "metadata": {
        "id": "zMTPRveQG9kl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfcOrzhaPpq0"
      },
      "source": [
        "### Normalization Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3aBywmT9__v"
      },
      "source": [
        "# Data loader function\n",
        "def get_mnist_loaders(batch_size=128):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.view(-1))\n",
        "    ])\n",
        "    train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_loader, test_loader = get_mnist_loaders()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChwJ6vtL-B6S"
      },
      "source": [
        "def build_model(norm_type=None):\n",
        "    layers = []\n",
        "    layers.append(nn.Linear(784, 100))\n",
        "    if norm_type == \"batch\":\n",
        "        layers.append(nn.BatchNorm1d(100))\n",
        "    elif norm_type == \"instance\":\n",
        "        layers.append(nn.LayerNorm(100))\n",
        "    layers.append(nn.Sigmoid())\n",
        "\n",
        "    layers.append(nn.Linear(100, 100))\n",
        "    if norm_type == \"batch\":\n",
        "        layers.append(nn.BatchNorm1d(100))\n",
        "    elif norm_type == \"instance\":\n",
        "        layers.append(nn.LayerNorm(100))\n",
        "    layers.append(nn.Sigmoid())\n",
        "\n",
        "    layers.append(nn.Linear(100, 100))\n",
        "    layers.append(nn.Sigmoid())\n",
        "\n",
        "    layers.append(nn.Linear(100, 10))\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "histories = {}\n",
        "for norm in [\"no\", \"batch\", \"instance\"]:\n",
        "    print(f\"Training model with {norm} normalization:\")\n",
        "    model = build_model(norm_type=(None if norm==\"no\" else norm))\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    histories[norm] = train(\n",
        "        model, device, train_loader, criterion, optimizer, num_epochs=10, val_loader=test_loader\n",
        "    )\n",
        "    print()"
      ],
      "metadata": {
        "id": "f7zvGf8TKgcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
        "\n",
        "# Accuracy\n",
        "for norm, history in histories.items():\n",
        "    axes[0].plot(history[\"val_accuracy\"], label=f\"{norm.capitalize()} Norm\")\n",
        "axes[0].set_title(\"Validation Accuracy\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Accuracy\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Loss\n",
        "for norm, history in histories.items():\n",
        "    axes[1].plot(history[\"val_loss\"], label=f\"{norm.capitalize()} Norm\")\n",
        "axes[1].set_title(\"Validation Loss\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Loss\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MyFlW24MKomi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gd-zJFYQrSs"
      },
      "source": [
        "In this example, we see that Batch Normalization speeds up convergence and achieves the best validation accuracy overall. Instance Normalization also accelerates training compared to no normalization but reaches a similar final performance after 10 epochs. Batch Normalization remains a common choice in feedforward networks, while Instance Normalization is often preferred in tasks like style transfer where per-sample normalization can be beneficial (see [*Instance Normalization: The Missing Ingredient for Fast Stylization*](https://arxiv.org/pdf/1607.08022) for details).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuF3Y5DoKylT"
      },
      "source": [
        "## HyperParameters Tuning\n",
        "\n",
        "### Creating a Validation Set\n",
        "\n",
        "When training our model, we need to decide the value of several hyperparameters, what regularization techniques we employ, or the loss used to train the model, among others. To decide these values we should not use as guidance the performance in the test set, as it may lead to overfitting to that set, and in turn to an erroneous estimate of the performance of the model in non-seen data. Hence, we use what is called a validation set, which we use to tweak the hyperparameters.\n",
        "\n",
        "In PyTorch, there is no built-in *validation split* parameter in `DataLoader`. Instead, you usually manually split your training set into a training and validation set using tools like `torch.utils.data.random_split`. For example:   \n",
        "```python\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Suppose 'dataset' contains all training data\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_subset, val_subset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "```\n",
        "This splits 80% of the data for training and 20% for validation.\n",
        "\n",
        "In these tutorials, we often reuse the official test sets of standard datasets (e.g., MNIST test set) as validation data, to ensure everyone works with the same split and can easily compare results. However, in a real project, you should keep the test set completely untouched and reserved for final evaluation only, after all hyperparameter tuning is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DFkIWei6hse"
      },
      "source": [
        "### Early Stopping\n",
        "\n",
        "We now know how to define a validation set and how to use it during the training process. An important step now is how to retrieve the model with the highest validation performance during training. Usually, you keep the model with the highest validation accuracy as your final model, and then, ideally, you would test this final model using an independent test set.\n",
        "However, validation accuracy fluctuates between epochs, and may actually decrease after a few epochs due to overfitting. Hence, we would like to save the model with the best validation performance.\n",
        "\n",
        "We already mentioned how to save the best-performing model in the introductory PyTorch tutorial, but let‚Äôs explain it again now that we know how to define a validation split. By default, PyTorch training loops keep updating the model for all $N$ epochs. To instead retrieve the model with the best validation performance, we can implement early stopping. Early stopping monitors a metric (for example, validation accuracy) and stops training after a specified number of epochs (patience) without improvement. Finally, we can restore the weights of the best model seen during training to avoid overfitting to later epochs.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFfNridt72qT"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Get data and model\n",
        "model, device, train_loader, test_loader = get_data_model()\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 20\n",
        "patience = 4  # Stop after 4 epochs without improvement\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_val_acc = 0\n",
        "epochs_no_improve = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "    val_loss = val_running_loss / len(test_loader.dataset)\n",
        "    val_acc = correct / total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
        "        f\"Train loss: {epoch_loss:.4f}, \"\n",
        "        f\"Val loss: {val_loss:.4f}, \"\n",
        "        f\"Val acc: {val_acc*100:.2f}%\"\n",
        "    )\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(best_model_wts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtujLcUPAc7R"
      },
      "source": [
        "We see how the training process stopped after failing to improve `val_acc` during the number of epochs set by `patience`. We now check that the performance of the saved model is the same as the obtained in the best epoch in terms of validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPiaL47KAY-e"
      },
      "source": [
        "def evaluate_model(model, device, data_loader, criterion):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "val_loss, val_acc = evaluate_model(model, device, test_loader, criterion)\n",
        "print(f\"Test loss: {val_loss:.4f}, Test accuracy: {val_acc*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMd2DVJCGmE7"
      },
      "source": [
        "### Learning Rate\n",
        "\n",
        "One of the most important parameters to tweak is the learning rate, which controls the update step performed during the backpropagation. In PyTorch, this can be adjusted dynamically during training using learning rate schedulers. Two common approaches are:\n",
        "\n",
        "- **Step Learning Rate Scheduler**, which reduces the learning rate by a factor every fixed number of epochs.\n",
        "- **Reduce on Plateau Scheduler**, which monitors a validation metric (like loss) and reduces the learning rate when the metric stops improving.\n",
        "\n",
        "We will demonstrate both schedulers in the examples below.\n",
        "\n",
        "For details, see the [PyTorch LR scheduler documentation](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we define a *step* learning rate scheduler (or `StepLR`) that decreases the learning rate by a factor of 0.1 every 3 epochs. This allows the optimizer to take larger steps in the beginning and finer adjustments later in training.\n"
      ],
      "metadata": {
        "id": "-pIrw1YKh05k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC9in_Zhfnly"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "model, device, train_loader, test_loader = get_data_model()\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Define StepLR scheduler: decay LR by 0.1 every 3 epochs\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "step_lr_history = []\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_val_loss = val_running_loss / len(test_loader.dataset)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    step_lr_history.append(current_lr)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, LR: {current_lr:.2e}\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nInitial Learning Rate: {step_lr_history[0]:.2e}\")\n",
        "print(f\"Final Learning Rate: {step_lr_history[-1]:.2e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAPrOzm0iDTs"
      },
      "source": [
        "Another learning rate scheduler is `ReduceLROnPlateau`, which reduces the learning rate whenever a given metric has stopped improving. There are 5 important arguments:\n",
        "\n",
        " * `monitor`: we specify the metric we want to track\n",
        " * `patience`: number of epochs without improvement before reducing lr\n",
        " * `factor`: the new learning rate will be `new_lr = lr * factor`\n",
        " * `min_lr`: sets the minimum lr\n",
        " * `min_delta`: margin to define when the metric has stopped improving\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os7b7Oh0TO7m"
      },
      "source": [
        "model, device, train_loader, test_loader = get_data_model()\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Reduce LR on Plateau: monitor validation loss\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.2,\n",
        "    patience=1,\n",
        "    threshold=0.01,\n",
        "    min_lr=1e-5\n",
        ")\n",
        "\n",
        "plateau_lr_history = []\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_val_loss = val_running_loss / len(test_loader.dataset)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step(epoch_val_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    plateau_lr_history.append(current_lr)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, LR: {current_lr:.2e}\"\n",
        "    )\n",
        "\n",
        "print()\n",
        "print(f\"Initial Learning Rate: {plateau_lr_history[0]:.2e}\")\n",
        "print(f\"Final Learning Rate: {plateau_lr_history[-1]:.2e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX9W-7yNoyvL"
      },
      "source": [
        "Again, we check how the learning rate has changed. You can check that the learning has indeed decreased when the `val_loss` has not improved by more than 0.01 until it reached the `min_lr` value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVNFkE-qnKVk"
      },
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, len(plateau_lr_history)+1), plateau_lr_history, marker='o', label=\"ReduceLROnPlateau\")\n",
        "plt.plot(range(1, len(step_lr_history)+1), step_lr_history, marker='o', label=\"StepLR\")\n",
        "plt.title(\"Learning Rate Schedules\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm0oVnMQWLZt"
      },
      "source": [
        "## Data augmentation\n",
        "Deep learning models are data-hungry, tend to overfit with small training sets and its performance benefit from large training sets. A way to synthetically create more data is using data augmentation. Data augmentation aims to modify the training examples by applying transformations to the input data. Now, we will show some examples of data augmentation for images.\n",
        "\n",
        "### Images\n",
        "Data augmentation techniques such as rotation, scaling or cropping are usually applied in deep learning pipelines for vision applications. The idea is to take as input an image, apply a transformation to it, and then use it for training.\n",
        "\n",
        "In PyTorch, data augmentation is typically performed using torchvision.transforms, which provides a wide range of transformations you can compose and apply to your datasets. These transformations can be combined in a Compose object and are applied on the fly during data loading, often leveraging multiple CPU workers to keep the GPU fed efficiently. The available transformations are listed [here](https://pytorch.org/vision/stable/transforms.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vftBzsolQcL"
      },
      "source": [
        "We first define a function `plot_data_augmentation` that we will use to show some augmented examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTLpcR43IHpX"
      },
      "source": [
        "def plot_data_augmentation(augmentation=None):\n",
        "    # Basic transform: convert to tensor only\n",
        "    base_transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # If augmentation is provided, chain it after ToTensor\n",
        "    if augmentation is not None:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            augmentation\n",
        "        ])\n",
        "    else:\n",
        "        transform = base_transform\n",
        "\n",
        "    # Load MNIST dataset with the transform\n",
        "    dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=False)\n",
        "\n",
        "    # Fetch a single batch\n",
        "    images, labels = next(iter(loader))\n",
        "    images_np = images.squeeze().numpy()\n",
        "\n",
        "    plt.figure(figsize=(15,3))\n",
        "    for i in range(5):\n",
        "        plt.subplot(1,5,i+1)\n",
        "        plt.imshow(images_np[i], cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoJYjjapiISI"
      },
      "source": [
        "We will now visualize some of the transformations available in this preprocessing module. First, we plot some images without any transformations applied for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyoZHyOTheyU"
      },
      "source": [
        "plot_data_augmentation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFXEC5EejxWG"
      },
      "source": [
        "### Rotation\n",
        "A standard transformation is to rotate the image. In PyTorch, you can use [`transforms.RandomRotation`](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomRotation), which rotates the image by a random angle within a specified range.\n",
        "\n",
        "For example, `transforms.RandomRotation(degrees=45)` rotates the image randomly between -45 and +45 degrees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjgfZ1z3j46X"
      },
      "source": [
        "plot_data_augmentation(transforms.RandomRotation(degrees=45))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Schj43v4ioTY"
      },
      "source": [
        "### Shift\n",
        "\n",
        "We can define a maximum range of both horizontal and vertical shifts using [`transforms.RandomAffine`](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomAffine).\n",
        "\n",
        "In PyTorch, shifting is handled by the `translate argument`, which specifies the maximum fraction for horizontal and vertical translation.\n",
        "\n",
        "For example, `transforms.RandomAffine(degrees=0, translate=(0.2, 0.2))` randomly shifts the image horizontally and vertically by up to 20% of its size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbzq69FfjLd9"
      },
      "source": [
        "plot_data_augmentation(transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uhYKLIPI1LA"
      },
      "source": [
        "### Zooming\n",
        "Zooming into the image can also be achieved using [`transforms.RandomAffine`](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomAffine).\n",
        "\n",
        "Instead of a separate zoom transformation, `RandomAffine` uses the `scale` argument to specify the range of scaling factors applied randomly.\n",
        "\n",
        "For example, `transforms.RandomAffine(degrees=0, scale=(0.8, 1.2))` will randomly zoom in or out by scaling the image between 80% and 120% of its original size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGvZzyL0HQjF"
      },
      "source": [
        "plot_data_augmentation(transforms.RandomAffine(degrees=0, scale=(0.8, 1.2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "845GvrICI9IX"
      },
      "source": [
        "### Flip\n",
        "\n",
        "We can apply random horizontal and/or vertical flips to augment the dataset. In PyTorch, the most common option is `transforms.RandomHorizontalFlip`, which flips images horizontally with a specified probability (default is 0.5). Vertical flips can be applied using `transforms.RandomVerticalFlip`. To randomly apply both flips independently, we can combine them in a `Compose` transform.\n",
        "\n",
        "Below is an example where each flip has a 50% chance of being applied:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6emZPbsHXOV"
      },
      "source": [
        "flip_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5)\n",
        "])\n",
        "\n",
        "plot_data_augmentation(flip_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb3fcBi8Inwy"
      },
      "source": [
        "### Data Normalization\n",
        "Data normalization is usually applied in deep learning pipelines. A typical data normalization applied is data standardization, where we transform the input data to have `mean=0` and `std=1`. When you have multiple features coming from different distributions and different scales, normalization makes all of the features have a similar distribution, which avoids biasing the model towards using the features with larger values.\n",
        "\n",
        "In images, the range of values is already in the same scale of [0-255], but normalizing the range of values to [0-1] can help us avoid starting training in the saturation regions of some activations, such as a sigmoid, where values of large magnitude will fall into the flat region of the activation. Data centering (mean=0) can also useful in some settings because the weight initializers used (e.g. Xavier) are derived following a ``mean=0`` assumption about the data distribution, but probably a model trained with [0-1] will usually converge too.\n",
        "\n",
        "To perform normalization in PyTorch, you typically compute the mean and standard deviation of the training set manually, and then define a `transforms.Normalize()` transformation with these values. Once defined, this normalization transform can be applied consistently to both the training and test data, ensuring the inputs are standardized with `mean=0` and `std=1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGOQ0F-_Lrgr"
      },
      "source": [
        "# Load MNIST training data\n",
        "train_dataset_raw = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "train_loader_raw = torch.utils.data.DataLoader(train_dataset_raw, batch_size=len(train_dataset_raw))\n",
        "train_data_tensor, _ = next(iter(train_loader_raw))\n",
        "train_data_numpy = train_data_tensor.numpy()\n",
        "\n",
        "# Compute mean and std\n",
        "mean = train_data_numpy.mean()\n",
        "std = train_data_numpy.std()\n",
        "\n",
        "print(f\"Train set original mean/std: {mean:.4f}/{std:.4f}\")\n",
        "\n",
        "# Define and apply normalization transform\n",
        "normalize_transform = transforms.Normalize(mean=[mean], std=[std])\n",
        "normalized_data = []\n",
        "\n",
        "for img, _ in train_dataset_raw:\n",
        "    img_norm = normalize_transform(img)\n",
        "    normalized_data.append(img_norm)\n",
        "\n",
        "normalized_tensor = torch.stack(normalized_data)\n",
        "normalized_numpy = normalized_tensor.numpy()\n",
        "\n",
        "print(f\"Train set normalized mean/std: {normalized_numpy.mean():.4f}/{normalized_numpy.std():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5bCiXoiiQFI"
      },
      "source": [
        "When using any kind of data normalization, it is important to note that we need to apply the same transformation to the test data for our model to work properly. In the given example we can use the statistics computed from the training data to standardize the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRBKYvgXid3_"
      },
      "source": [
        "# Load MNIST test data\n",
        "test_dataset_raw = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
        "test_loader_raw = torch.utils.data.DataLoader(train_dataset_raw, batch_size=len(test_dataset_raw))\n",
        "test_data_tensor, _ = next(iter(test_loader_raw))\n",
        "test_data_numpy = test_data_tensor.numpy()\n",
        "\n",
        "print(f\"Test set original mean/std: {test_data_numpy.mean():.4f}/{test_data_numpy.std():.4f}\")\n",
        "\n",
        "normalized_data = []\n",
        "for img, _ in test_dataset_raw:\n",
        "    img_norm = normalize_transform(img)\n",
        "    normalized_data.append(img_norm)\n",
        "\n",
        "normalized_tensor = torch.stack(normalized_data)\n",
        "normalized_numpy = normalized_tensor.numpy()\n",
        "\n",
        "print(f\"Train set normalized mean/std: {normalized_numpy.mean():.4f}/{normalized_numpy.std():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOE33AOLJdPK"
      },
      "source": [
        "### Combining transformations\n",
        "We have seen examples of individual data augmentation transformations. Next, we will show how to combine multiple transformations together and integrate them into a full training pipeline. In PyTorch, this is done by composing all the transformations into a single `transforms.Compose` sequence, which is applied to each training sample before it is passed to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqb15Ga0EsC9"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Dataset statistics (calculated before)\n",
        "mean = 0.13066\n",
        "std = 0.30811\n",
        "\n",
        "# Defining a composed transform with normalization and augmentations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[mean], std=[std]),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "# For the test set, we only normalize\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[mean], std=[std]),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "model, device, train_loader, test_loader = get_data_model(train_transform=train_transform, test_transform=test_transform)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "history = train(model, device, train_loader, criterion, optimizer, num_epochs=10, val_loader=test_loader)\n",
        "\n",
        "plot_history(history, \"accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipjcr_8Gi_0S"
      },
      "source": [
        "With this setup, data augmentation is applied on-the-fly during training each time a batch is loaded. At evaluation time, only normalization is performed.\n",
        "\n",
        "> **Note**: In PyTorch, augmentations are applied dynamically when sampling batches, rather than as part of a model layer. This design avoids storing augmented copies and ensures fresh randomized transformations at every epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Ynz4OOee5_"
      },
      "source": [
        "In these examples, we showed practical examples of data augmentation for images. However, other modalities, such as text or audio can also benefit from data augmentation.\n",
        "\n",
        "Keep in mind that applying a data augmentation strategy that is too aggressive may actually harm the performance in the test set, e.g., applying extreme zooms not likely to be present in the dataset or strong contrast shifts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUpwc1fHxF4N"
      },
      "source": [
        "# Coursework\n",
        "\n",
        "### Task 1: Tuning a Classification Model\n",
        "In a machine learning problem, and especially when using a deep learning approach, finding the right set of hyperparameters, the right data augmentation strategy, or a good regularization method can make the difference between a model that performs poorly and a model with great accuracy.\n",
        "\n",
        "For this exercise, you will be training a CNN to perform classification in CIFAR-10 (we use the official test set, which is why the variables are called `x_test` and `y_test`, as our validation set) and will analyze the impact of some of the most important elements presented in this tutorial.\n",
        "\n",
        "Use the CNN we give in the code below, along with the given optimizer and number of training epochs as the default setting. Only modify the given CNN architecture to add Dropout or Batch Normalization layers when explicitly stated. Use 40 epochs to plot all of your curves. However, you can train for more epochs to find your best validation performance if your network has not finished training in those 40 epochs.\n",
        "\n",
        "**Report:**\n",
        "*  First, train the given default model without any data augmentation. Then define two data augmentation strategies (one more aggressive than the other) and train the model with data augmentation. Clearly state the two augmentation strategies you apply (i.e., the specific transformations). Discuss the training and validation loss curves for the two data augmentation strategies along with the original run without data augmentation. Attach in the appendix those training and validation curves. Report in a table the best validation accuracy obtained for the three runs (no data augmentation, data augmentation 1, data augmentation 2).\n",
        "\n",
        "*  Without using any data augmentation, analyze the effect of using Dropout in the model. Carry out the same analysis for Batch Normalization. Finally, combine both. Report in the same table as in the data augmentation task the best validation accuracy for each of the three settings (baseline + Dropout, baseline + Batch Normalization, baseline + Batch Normalization + Dropout). The performance will vary depending on where the Dropout layers and Batch Normalization layers are, so state clearly where you added the layers, and what rate you used for the Dropout layers. Discuss the results.\n",
        "\n",
        "* Using the default model/hyperparameters and no data augmentation, report the best validation accuracy when using `zeros` for the kernel initialization. Report the performance in the same table as in the dropout/batch normalization/data augmentation tasks. Discuss the results that you obtained.\n",
        "\n",
        "*  Using the default model and no data augmentation, change the optimizer to SGD and train it with learning rates of `3e-3`, `1e-3` and `3e-4`. Report in a figure the training and validation loss for the three learning rate values and discuss the figure."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading CIFAR-10 train set to calculate mean and std for normalizations\n",
        "x = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "loader = torch.utils.data.DataLoader(x, batch_size=len(x), shuffle=False)\n",
        "data_tensor, _ = next(iter(loader))\n",
        "\n",
        "# Compute mean and std per channel\n",
        "mean = data_tensor.mean(dim=[0,2,3])\n",
        "std = data_tensor.std(dim=[0,2,3])\n",
        "\n",
        "mean = mean.tolist()\n",
        "std = std.tolist()\n",
        "\n",
        "print(\"Per-channel mean:\", mean)\n",
        "print(\"Per-channel std:\", std)"
      ],
      "metadata": {
        "id": "Pwx-hX-X1HzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default: No augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "    # You can later add augmentations like:\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)), etc.\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform)\n",
        "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Image shape:\", np.array(train_dataset[0][0]).shape)\n",
        "print(\"Total number of training samples:\", len(train_dataset))\n",
        "print(\"Total number of validation samples:\", len(test_dataset))"
      ],
      "metadata": {
        "id": "ULFyOsS42DeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, use_dropout=False, use_batchnorm=False, dropout_rate=0.5, init_mode=\"xavier_uniform\"):\n",
        "        super().__init__()\n",
        "\n",
        "        def conv_block(in_channels, out_channels, use_bn, init, use_dropout, dropout_rate):\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "            if init == \"zeros\":\n",
        "                nn.init.constant_(conv.weight, 0.0)\n",
        "                nn.init.constant_(conv.bias, 0.0)\n",
        "            else:\n",
        "                nn.init.xavier_uniform_(conv.weight)\n",
        "                nn.init.constant_(conv.bias, 0.0)\n",
        "            layers = [conv]\n",
        "            if use_bn:\n",
        "                layers.append(nn.BatchNorm2d(out_channels))\n",
        "            layers.append(nn.ReLU())\n",
        "            if use_dropout:\n",
        "                layers.append(nn.Dropout2d(dropout_rate))\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "        # 4 conv blocks with ReLU and MaxPool2d(padding=1)\n",
        "        self.features = nn.Sequential(\n",
        "            conv_block(3, 32, use_batchnorm, init_mode, use_dropout, dropout_rate),\n",
        "            nn.MaxPool2d(2,2, padding=0),\n",
        "\n",
        "            conv_block(32, 64, use_batchnorm, init_mode, use_dropout, dropout_rate),\n",
        "            nn.MaxPool2d(2,2, padding=0),\n",
        "\n",
        "            conv_block(64, 128, use_batchnorm, init_mode, use_dropout, dropout_rate),\n",
        "            nn.MaxPool2d(2,2, padding=0),\n",
        "\n",
        "            conv_block(128, 256, use_batchnorm, init_mode, use_dropout, dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout_rate if use_dropout else 0.0),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RqBpuNY42sl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=40):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct_train += (preds == targets).sum().item()\n",
        "            total_train += targets.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_acc = correct_train / total_train\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct_val = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                correct_val += (preds == targets).sum().item()\n",
        "\n",
        "        val_loss /= len(test_loader.dataset)\n",
        "        val_acc = correct_val / len(test_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{epochs}] \"\n",
        "            f\"- Train Loss: {train_loss:.4f},  Train Acc: {train_acc*100:.2f}%, \"\n",
        "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\"\n",
        "        )\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"train_acc\": train_accuracies,\n",
        "        \"val_loss\": val_losses,\n",
        "        \"val_acc\": val_accuracies\n",
        "    }\n",
        "    return history"
      ],
      "metadata": {
        "id": "LkZejJ3x6lcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Use \"model = CNNModel()\" to get the default model\n",
        "model = CNNModel(use_dropout=False, use_batchnorm=False, init_mode=\"xavier_uniform\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4) # Edit optimizer and learning rate here.\n",
        "\n",
        "history = train_model(model, train_loader, test_loader, criterion, optimizer, epochs=40)"
      ],
      "metadata": {
        "id": "g8wS242y6oBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}