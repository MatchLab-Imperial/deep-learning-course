{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/04_Common_CNN_architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhXUenVgBmjf"
      },
      "source": [
        "# Introduction to common Convolutional Neural Networks Architectures\n",
        "In this tutorial, we will learn about well-known CNN architectures in the field of computer vision and how to implement them in Pytorch. Moreover, we will show that some of the standard architectures are already available into the Pytorch framework. Finally, we will see how we can import and use them in our code.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "6krwl11PW-6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchinfo\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# Global variables\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Utility functions\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def train_classifier(num_epochs, data_loader, model, criterion, optimizer, device=DEVICE):\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            # Forward + Backward + Optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and Accuracy\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predicted = outputs.argmax(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss/len(train_loader.dataset):.4f}, Accuracy: {100.*correct/total:.2f}%\")\n",
        "\n",
        "def train_denoiser(num_epochs, data_loader, model, criterion, optimizer, device=DEVICE):\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "def evaluate_classifier(data_loader, model, criterion, device=DEVICE):\n",
        "    model = model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = outputs.argmax(1)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    print(f\"Test Loss: {running_loss/len(data_loader.dataset):.4f}\")\n",
        "    print(f\"Test Accuracy: {100.*correct/total:.2f}%\")\n",
        "\n",
        "def evaluate_denoiser(data_loader, model, criterion, device=DEVICE):\n",
        "    model = model.to(device)\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "\n",
        "    print(f\"Test Loss: {running_loss/len(data_loader.dataset):.4f}\")"
      ],
      "metadata": {
        "id": "8tXffP6Ie1mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auwuthntar8h"
      },
      "source": [
        "# CNNs in Image Classification\n",
        "We start this tutorial by presenting the networks that have been widely used for image classification.  One of the most famous problems among the computer vision community was the annual software contest run by the ImageNet project, **ImageNet Large Scale Visual Recognition Challenge (ILSVRC)**. This contest evaluates how well an algorithm does in the tasks of object category classification and detection. The challenge provides hundreds of object categories and millions of images. The resolution of those images is also bigger than those on MNIST or CIFAR datasets.\n",
        "\n",
        "As we already have seen, CIFAR10 dataset contains ten different classes of 32x32 images. We will show how to resize the data to be able to use some of the following architectures, as not all of the CNN architectures accepts arbitrary input sizes. Remember that they were designed for ImageNet, and were trained using 224x224 crops.\n",
        "\n",
        "Let's start by loading CIFAR10:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaQUxiIirq3r"
      },
      "source": [
        "In order to use the original models, we need to resize the CIFAR10 32x32 images into 224x224 images. However, as the whole dataset of CIFAR10 resized ot 224x224 would not fit in our RAM, we need to create a preprocessing step as we load our data. That would take the 32x32 images and, just before going into the architecture, resize them into our desired shape. To do so, we use the `transform` function in Pytorch, which applies any specified transforms (in this case `resize_images`) in a set. We can see how this function looks like for a single batch of 32 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i87in95Fj0zF"
      },
      "source": [
        "# Transform to resize and normalize CIFAR10 images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load CIFAR10 train and test sets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "classes = trainset.classes\n",
        "\n",
        "# Build data loader\n",
        "train_loader = DataLoader(trainset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FihA5egr6qs"
      },
      "source": [
        "# Utility function to display image\n",
        "def imshow(img_tensor, title=None):\n",
        "    img = img_tensor.numpy().transpose((1, 2, 0))  # CHW to HWC\n",
        "    img = np.clip(img, 0, 1)  # Ensure values are in [0, 1]\n",
        "    plt.imshow(img)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get a batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Display first 4 images in batch\n",
        "for i in range(4):\n",
        "    imshow(images[i], title=classes[labels[i]])\n",
        "    print(f\"Image {i} shape: {images[i].shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeChk5z14kUQ"
      },
      "source": [
        "We have the dataset and the transform function ready to use to for any of the standard classification architectures. In the next section, we will show how we can code a common architecture and train it with our preprocessed dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwo-H2oSfQ0s"
      },
      "source": [
        "## AlexNet\n",
        "\n",
        "[AlexNet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) made a huge impact in 2012 in the ImageNet challenge when it reduced the top-5 error (i.e. the correct class is not among the top-5 predictions) from 26% to 15.3%. The second place was close to 26.2%, and it was not a CNN based system. AlexNet shares a lot in common with its predecessor architecture, [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) network by Yann LeCun et al. When LeNet came out, the computational complexity of the networks was an important constraint. Nowadays, with GPUs being every day more and more powerful, the computational complexity is more plausible to deal with that it was years before. Therefore, the authors in AlexNet decided to make the architecture bigger by using more convolutional layers and more filters.\n",
        "\n",
        "The architecture proposed in their paper is as follows:\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/QFG561f/0-x-POQ3bt-Z9r-QO23-LK.png)\n",
        "\n",
        "The network consisted of convolutional layers with kernels of size 11x11, 5x5 and 3x3. The architecture uses layers with strides, max poolings, dropouts, ReLU activation functions, and three dense layers at the very end. In their original work, the network was split into two streams, as seen in the figure above. That was because GPU constraints, and hence, the authors needed to train it on two separate GPUs. We found a great implementation in [Rizwan's blog](https://engmrk.com/alexnet-implementation-using-keras/), where it presented an AlexNet model without the split concept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U4lWS9WyBP0"
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),   # Conv1\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                   # Pool1\n",
        "\n",
        "            nn.Conv2d(96, 256, kernel_size=11, stride=1, padding=0), # Conv2\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                   # Pool2\n",
        "\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=0), # Conv3\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=0), # Conv4\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=0), # Conv5\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)                    # Pool5\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(AlexNet(10), input_size=(1, 3, 224, 224))"
      ],
      "metadata": {
        "id": "ljHpFw_oOffq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSfj3T_T0ZZ9"
      },
      "source": [
        "Keep in mind that we can use the original code because we have added a line of code to resize the CIFAR images from 32x32 to 224x224. We can now train our AlexNet model on CIFAR."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = AlexNet(num_classes=10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "train_classifier(3, train_loader, model, criterion, optimizer)\n",
        "evaluate_classifier(test_loader, model, criterion)"
      ],
      "metadata": {
        "id": "1KuWw_ibPfAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zyJoeM76R6W"
      },
      "source": [
        "As it could be observed, training this kind of architectures in big datasets is time-consuming. We might need to train for several hours to start getting state-of-the-art results. That is why Pytorch saves us time and offers us several pre-trained models on the ImageNet dataset.\n",
        "\n",
        "Those models can be used directly for image prediction, image classification, feature extraction, or fine-tuning, among others, without the need of spending long hours of training. Isn't that great!?\n",
        "\n",
        "Even if the task or the dataset is different, using the pre-trained weights as initialisation for the training process provides usually faster convergence (and sometimes better results) than random initialisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wks8tkdjdtgV"
      },
      "source": [
        "# Loading Pre-trained Models in Pytorch\n",
        "\n",
        "As mentioned, Pytorch contains many models that have been trained in the ImageNet dataset. Those models were originally released to participate in the ILSVRC competition. However, now they are available alongside their pre-trained weights and used in multiple different tasks. Using pre-trained weights has become a common practice to initialise networks. You can learn more about it [here](https://docs.pytorch.org/vision/main/models.html).\n",
        "\n",
        "We will show how to initialise different models and load their pre-trained weights in ImageNet. Afterwards, we will show how to modify the last layer to classify only the ten classes we have in CIFAR10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWbK1gE99o7K"
      },
      "source": [
        "## VGGNet\n",
        "\n",
        "VGGNet is an architecture presented by Simonyan and Zisserman in 2014. VGGNet is similar to previous AlexNet network, however, it only contains 3x3 convolutional kernels and many more filters. It has become really popular since researchers have found that it can be used to extract powerful feature representations from an image. Therefore, extracted features have proven that they are useful for many other computer vision domains and not only image classification. For instance, they can be used in feature representation, style transfer, or image captioning. You can check the [Oxford VGG paper](https://arxiv.org/pdf/1409.1556.pdf) for further details.\n",
        "\n",
        "Next image shows the standard VGG architecture ([source](https://www.cs.toronto.edu/~frossard/post/vgg16/)):\n",
        "\n",
        "![](https://www.cs.toronto.edu/~frossard/post/vgg16/vgg16.png)\n",
        "\n",
        "The training time of the architecture is massive since it has more than 130 million parameters. Thankfully, we can find it already pre-trained in Pytorch. In the original work, authors proposed two versions of VGG; VGG16, and VGG19. Pytorch provides the both proposed versions, where the difference lies in the number of layers within the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPpzPdYAREBb"
      },
      "source": [
        "torchinfo.summary(models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1), input_size=(1, 3, 224, 224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6TMSj9r3Dkr"
      },
      "source": [
        "The argument `weights` indicates to Pytorch whether we need to use the pre-trained weights from ImageNet.\n",
        "\n",
        "If we want to use the original VGG model in CIFAR10 classification, we need to change some things. The first thing to modify is the last fully connected layer. In the original VGG, the last layer is designed to classify among the 1,000 classes provided in ImageNet. In CIFAR10, as we already know, we only have ten classes. Therefore, we switch the last dense layer in the model to be able to perform classification on CIFAR10. Moreover, a common practise when using pre-trained networks is to freeze the model and only train the new dense layer. This strategy is called Transfer Learning. You can read more about it and its difference to Fine-tuning in this great [blog](https://towardsdatascience.com/cnn-transfer-learning-fine-tuning-9f3e7c5806b2). Updating only the weights of the last layer provides several advantages; faster training time, good performance, and sometimes helps the networks to not overfit in small datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1c26De_3ihg"
      },
      "source": [
        "# Load pretrained VGG16 model\n",
        "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in vgg16.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace classifier (fc layers)\n",
        "vgg16.classifier = nn.Sequential(\n",
        "    *list(vgg16.classifier.children())[:-1],  # keep up to fc2\n",
        "    nn.Linear(4096, 10),                      # new output layer\n",
        ")\n",
        "\n",
        "# Check architecture\n",
        "torchinfo.summary(vgg16, input_size=(1, 3, 224, 224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3gLnH3p4Wl6"
      },
      "source": [
        "We can confirm that the original dense layer is not there anymore, and instead, we have a new fully-connected layer with only ten activations. Also, as we have frozen the weights of the model, we see that the trainable parameters of the network belong to the new dense layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdIufD1rDyig"
      },
      "source": [
        "Let's now see how we can modify the VGG architecture to use any input image shape. First of all, if we want to use any other image resolution than 224x224, we need to only use the model `features`. This argument will remove all dense layers at the end of the architecture. Remember that the dense layer comes after a flatten layer (see the model summary above) and, therefore, the size of the flatten vector is fixed. If we modify the input image size, the flatten vector would inevitably change as well. Hence, we need to redefine all dense layers within the model every time we switch the input size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDjgj0Q4Ds1-"
      },
      "source": [
        "# Replace classifier with custom dense layers for smaller input (32x32)\n",
        "# Note: We must recalculate the flattened feature size after pooling\n",
        "# For 32x32 input, VGG16 reduces it to 1x1 feature map after conv/pooling\n",
        "\n",
        "class CustomVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Load pretrained VGG16 model (without top classifier layers)\n",
        "        vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
        "        # Freeze all the convolutional layers\n",
        "        for param in vgg16.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.features = vgg16.features  # only the convolutional part\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096),  # 512 from final feature map size: (512, 1, 1)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(model = CustomVGG(), input_size=(1, 3, 32, 32))"
      ],
      "metadata": {
        "id": "Tia8KTqxTJr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izc03Dfc7lL_"
      },
      "source": [
        "We have added the same number of dense layers as in original VGG. We are ready to train the newly added layers as we did in the previous step. But first, we must prepare a new dataset to match the required input size."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform to normalise CIFAR-10 images (no resizing)\n",
        "norm_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 train and test datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=norm_transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=norm_transform)\n",
        "\n",
        "# Build data loaders\n",
        "train_loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "2UxAgUM5bfhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IRy6UZf70rU"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = CustomVGG()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "train_classifier(3, train_loader, model, criterion, optimizer)\n",
        "evaluate_classifier(test_loader, model, criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSaBmsu7IKD9"
      },
      "source": [
        "## GoogLeNet / Inception v1\n",
        "\n",
        "Google presented [GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf) in 2014, the same year that VGGNet was introduced.  GoogLeNet won the ImageNet competition achieving a top-5 error rate of 6.67%, almost betting the human error rate (5%).\n",
        "\n",
        "GoogLeNet introduced many ideas that helped the development of current state-of-the-art architectures. Instead of stacking more and more CNN layers, GoogLeNet introduced what was called inception modules:\n",
        "\n",
        "![](https://i.ibb.co/JKqptrj/Googlenet-inception.png)\n",
        "\n",
        "Those modules apply convolutions with 3 different sizes of kernels (1x1, 3x3 and 5x5) at the same level. The idea behind those modules is based on the premise that significant information can be presented in images at different scales. Therefore, by using a multi-scale approach, they are more likely to capture meaningful information. Moreover, they claimed that designing a wider architecture, instead of a deeper, helps the gradients to navigate throughout the entire network.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*uW81y16b-ptBDV8SIT1beQ.png)\n",
        "\n",
        "The architecture consisted of 9 inception modules stacked linearly, containing 22 deep CNN layers. However, in this network, the total number of parameters was reduced to 4 million! Authors also included two auxiliary classifiers in the middle part of the architecture to avoid the vanishing gradient problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGCXlBgVIxP6"
      },
      "source": [
        "Even though they won the image classification contest, original Inception v1 was a complex and heavily engineered architecture. Authors introduced many tricks to push its performance, both in terms of speed and accuracy. In the literature, we can find a constant evolution of the architecture which led to the creation of several improved versions. A list and explanation of all inception networks can be found in [this blog](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202).  \n",
        "\n",
        "In Pytorch, the Inception model v3 architecture is available, together with its pre-trained weights on ImageNet. It can be initialised as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw6quKrOpGjT"
      },
      "source": [
        "torchinfo.summary(models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, aux_logits=True), input_size=(1, 3, 299, 299))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxmx-iMnp4Wz"
      },
      "source": [
        "Every network has its requirements. For instance, Inception v3 needs a minimum image input size of 75x75 due to its design. We can use the lambda function defined previously in the tutorial and see how the network performs in CIFAR10."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the complete model with preprocessing and new classification head\n",
        "class CustomInceptionV3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Input preprocessing (resizing)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((299, 299), interpolation=transforms.InterpolationMode.BILINEAR)\n",
        "        ])\n",
        "\n",
        "        # Create InceptionV3 model with pretrained weights\n",
        "        base_model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, aux_logits=True)\n",
        "        # Freeze all the layers\n",
        "        for param in base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Use the feature extraction part of InceptionV3\n",
        "        # We need to find the layer before the fully connected layers\n",
        "        # Examining the structure, the 'Mixed_7c' module seems to be the last feature extractor\n",
        "        self.features = nn.Sequential(\n",
        "            base_model.Conv2d_1a_3x3,\n",
        "            base_model.Conv2d_2a_3x3,\n",
        "            base_model.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            base_model.Conv2d_3b_1x1,\n",
        "            base_model.Conv2d_4a_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            base_model.Mixed_5b,\n",
        "            base_model.Mixed_5c,\n",
        "            base_model.Mixed_5d,\n",
        "            base_model.Mixed_6a,\n",
        "            base_model.Mixed_6b,\n",
        "            base_model.Mixed_6c,\n",
        "            base_model.Mixed_6d,\n",
        "            base_model.Mixed_6e,\n",
        "            base_model.Mixed_7a,\n",
        "            base_model.Mixed_7b,\n",
        "            base_model.Mixed_7c,\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(2048, 10) # 2048 is the output channels of Mixed_7c\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply transformations\n",
        "        x = self.transform(x)\n",
        "        # Extract features\n",
        "        x = self.features(x)\n",
        "        # Apply pooling and classification\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "07-OGaLAl7wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(CustomInceptionV3(), input_size=(1, 3, 224, 224))"
      ],
      "metadata": {
        "id": "cdytneHaUQRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZwcxCiGxRQ-"
      },
      "source": [
        "Let's train Inception v3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_kaiLcGxE8V"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = CustomInceptionV3()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "train_classifier(3, train_loader, model, criterion, optimizer)\n",
        "evaluate_classifier(test_loader, model, criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCV0NEx-F-0D"
      },
      "source": [
        "## ResNet\n",
        "\n",
        "In 2015, the winner of the ImageNet challenge was the Residual Neural Network ([ResNet](https://arxiv.org/pdf/1512.03385.pdf)) architecture. Similar to Inception, ResNet is built by micro-architectures modules, called residual blocks. Those blocks introduced skip connections, which allowed to train huge architectures (152 layers) while still having lower complexity than VGGNet. Thus, residual connections allowed authors to design deeper architectures since the gradient could backpropagate easier through the skip connections. Next image is from [Das' blog](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5) and shows ResNet architecture with its skip connections:\n",
        "\n",
        "![texto alternativo](https://cdn-images-1.medium.com/max/800/0*pkrso8DZa0m6IAcJ.png)\n",
        "\n",
        "ResNet has various verions (e.g., ResNet50, ResNet101, ResNet152) with the main difference lying in the number of layers used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3AJ5oF0dz64"
      },
      "source": [
        "# Custom ResNet50 model for CIFAR-10\n",
        "class CustomResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained ResNet50\n",
        "        base_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Freeze all layers\n",
        "        for param in base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Remove the original classifier (fc)\n",
        "        self.features = nn.Sequential(*list(base_model.children())[:-2])  # Up to last conv layer\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling\n",
        "        self.classifier = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)          # ResNet backbone\n",
        "        x = self.pool(x)              # Global Average Pooling\n",
        "        x = torch.flatten(x, 1)       # Flatten\n",
        "        x = self.classifier(x)        # Final Dense layer\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(CustomResNet50(num_classes=10), input_size=(1, 3, 32, 32))"
      ],
      "metadata": {
        "id": "-KFautOMVJcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC5S2IONeaWd"
      },
      "source": [
        "Let's check the performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRJ80Qi2ef0U"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = CustomResNet50(num_classes=10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)\n",
        "\n",
        "train_classifier(3, train_loader, model, criterion, optimizer)\n",
        "evaluate_classifier(test_loader, model, criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlupkHZ11D6z"
      },
      "source": [
        "Note that all previous networks may need a different number of epochs until they converge. Even though we use pre-trained weights, fully training a model can be quite time-consuming."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DenseNet\n",
        "\n",
        "DenseNet, short for Densely Connected Convolutional Network, was introduced in 2017 in the paper [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993). It brought a novel concept of dense connectivity, where each layer receives the feature maps of all preceding layers as input. This approach mitigates the vanishing gradient problem, improves feature propagation, and allows for fewer parameters than traditional architectures by reusing features.\n",
        "\n",
        "Each dense block is followed by a transition layer that performs downsampling. Compared to ResNet's additive skip connections, DenseNet concatenates the outputs of previous layers, which encourages feature reuse and compact models.\n",
        "\n",
        "The diagram below (from the original paper) illustrates how DenseNet differs in connectivity:\n",
        "\n",
        "![DenseNet Connectivity](https://miro.medium.com/v2/resize:fit:1400/0*7H9mNwLLWFiLYhLb.jpeg)\n",
        "\n",
        "\n",
        "PyTorch offers pre-trained DenseNet models. The most popular variant is DenseNet-121, which has 121 layers, but deeper variants like DenseNet-169, 201, and 264 also exist.\n",
        "\n"
      ],
      "metadata": {
        "id": "lPnPAg11VV5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom DenseNet121 for CIFAR-10\n",
        "class CustomDenseNet121(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained DenseNet121\n",
        "        base_model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Freeze feature extractor layers\n",
        "        for param in base_model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Replace the classifier\n",
        "        self.features = base_model.features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5_bStSTUnvG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(CustomDenseNet121(num_classes=10), input_size=(1, 3, 32, 32))"
      ],
      "metadata": {
        "id": "a-VPTByDWT3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = CustomDenseNet121(num_classes=10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)\n",
        "\n",
        "train_classifier(3, train_loader, model, criterion, optimizer)\n",
        "evaluate_classifier(test_loader, model, criterion)"
      ],
      "metadata": {
        "id": "KFxaCpAZnyRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ILGzCzQlD26"
      },
      "source": [
        "## More Architectures ...\n",
        "\n",
        "Pytorch offers many more network implementations trained on ImageNet. You can check the [documentation](https://docs.pytorch.org/vision/main/models.html) to see all of them. Besides, the documentation reports their accuracies, number of parameters and depth of each architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_mCvUoEZYmd"
      },
      "source": [
        "# CNNs for Other Tasks\n",
        "\n",
        "Besides the success of CNNs in classification problems, CNNs have been used in many other domains. To name a few: image inpainting, facial recognition, semantic segmentation, image captioning, depth prediction, among many others. In this section, we are going to talk about two different problems in computer vision: image denoising and object detection/classification. We are aiming to provide examples of the wide applicability CNNs architectures have.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5AKwf_Hl-Gv"
      },
      "source": [
        "## Image Denoising with UNet\n",
        "\n",
        "[UNet](https://arxiv.org/pdf/1505.04597.pdf) was presented in 2015 for biomedical image segmentation, but since then, it has been employed in countless different tasks, such as semantic segmentation, image colorisation, image stylisation... The UNet architectural idea is similar to a regular encoder-decoder layout but with skip connections. The first part of the network encodes the image and reduces it to a map of smaller size, forcing it to capture more global information. Then, the next part is in charge of decoding the image from that reduced feature map.\n",
        "\n",
        "UNet uses a similar idea of the skip connections used in ResNets. In addition to helping to backpropagate the gradients, the authors used the skip connections to facilitate the flow of information between encoder and decoder. We will talk more about UNet in future tutorials, but for now, let's see how we can code an UNet model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoYzGXVRnP5k"
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = self.double_conv(3, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = self.double_conv(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = self.double_conv(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.conv4 = self.double_conv(256, 512)\n",
        "        self.drop4 = nn.Dropout(0.5)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv5 = self.double_conv(512, 1024)\n",
        "        self.drop5 = nn.Dropout(0.5)\n",
        "\n",
        "        # Decoder\n",
        "        self.up6 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
        "        self.conv6 = self.double_conv(1024, 512)\n",
        "\n",
        "        self.up7 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
        "        self.conv7 = self.double_conv(512, 256)\n",
        "\n",
        "        self.up8 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.conv8 = self.double_conv(256, 128)\n",
        "\n",
        "        self.up9 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.conv9 = self.double_conv(128, 64)\n",
        "\n",
        "        self.conv10 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
        "\n",
        "    def double_conv(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        c1 = self.conv1(x)\n",
        "        p1 = self.pool1(c1)\n",
        "\n",
        "        c2 = self.conv2(p1)\n",
        "        p2 = self.pool2(c2)\n",
        "\n",
        "        c3 = self.conv3(p2)\n",
        "        p3 = self.pool3(c3)\n",
        "\n",
        "        c4 = self.conv4(p3)\n",
        "        d4 = self.drop4(c4)\n",
        "        p4 = self.pool4(d4)\n",
        "\n",
        "        c5 = self.conv5(p4)\n",
        "        d5 = self.drop5(c5)\n",
        "\n",
        "        # Decoder\n",
        "        u6 = F.interpolate(d5, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        u6 = self.up6(u6)\n",
        "        u6 = torch.cat([u6, d4], dim=1)\n",
        "        c6 = self.conv6(u6)\n",
        "\n",
        "        u7 = F.interpolate(c6, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        u7 = self.up7(u7)\n",
        "        u7 = torch.cat([u7, c3], dim=1)\n",
        "        c7 = self.conv7(u7)\n",
        "\n",
        "        u8 = F.interpolate(c7, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        u8 = self.up8(u8)\n",
        "        u8 = torch.cat([u8, c2], dim=1)\n",
        "        c8 = self.conv8(u8)\n",
        "\n",
        "        u9 = F.interpolate(c8, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        u9 = self.up9(u9)\n",
        "        u9 = torch.cat([u9, c1], dim=1)\n",
        "        c9 = self.conv9(u9)\n",
        "\n",
        "        output = self.conv10(c9)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(UNet(), input_size=(1, 3, 32, 32))"
      ],
      "metadata": {
        "id": "nXjx2XpdgKr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfMCDfIJAdYY"
      },
      "source": [
        "Now, we are going to create a toy example of image denoising.\n",
        "\n",
        "We have CIFAR10 images already loaded from previous experiments. We add some Gaussian noise (you can play with the scale of the noise) to those CIFAR10 images and then train the UNet to remove that added noise from the images. To do so, we train with the noisy image as input and the clean image as the target. We use the Mean Absolute Error as the loss function to optimise the architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OarMCDhS7UjV"
      },
      "source": [
        "# Get full dataset tensors from DataLoader (no labels used for autoencoder)\n",
        "def extract_data(loader):\n",
        "    all_images = []\n",
        "    for imgs, _ in loader:\n",
        "        all_images.append(imgs)\n",
        "    return torch.cat(all_images, dim=0)\n",
        "\n",
        "# Step 1: Extract CIFAR-10 images from original loaders\n",
        "x_train_clean = extract_data(train_loader)\n",
        "x_test_clean = extract_data(test_loader)\n",
        "\n",
        "# Step 2: Add Gaussian noise\n",
        "noise_std = 0.1\n",
        "x_train_noisy = x_train_clean + noise_std * torch.randn_like(x_train_clean)\n",
        "x_test_noisy = x_test_clean + noise_std * torch.randn_like(x_test_clean)\n",
        "\n",
        "# Clip to valid range [0,1] if needed (optional, depending on transforms)\n",
        "x_train_noisy = torch.clamp(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = torch.clamp(x_test_noisy, 0., 1.)\n",
        "\n",
        "# Step 3: Wrap as TensorDatasets\n",
        "train_dataset = TensorDataset(x_train_noisy, x_train_clean)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(x_test_noisy, x_test_clean)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = UNet()\n",
        "criterion = nn.L1Loss()  # Mean Absolute Error\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "train_denoiser(5, train_loader, model, criterion, optimizer)\n",
        "evaluate_denoiser(test_loader, model, criterion)"
      ],
      "metadata": {
        "id": "P6jRy_z2yjW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CULZTfRdBk3C"
      },
      "source": [
        "We can visualise the noisy image next to the output of the UNet. The output images are a little bit blurred, but the noise we added has definitely been reduced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0H2dGQ0p98I"
      },
      "source": [
        "def to_rgb(tensor):\n",
        "    return torch.clamp(tensor, 0, 1).permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "preds = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x_noisy, _ in test_loader:\n",
        "        x_noisy = x_noisy.to(DEVICE)\n",
        "        output = model(x_noisy)\n",
        "        preds.append(output.cpu())\n",
        "pred = torch.cat(preds, dim=0)\n",
        "\n",
        "N = 5\n",
        "start_val = 0\n",
        "fig, axes = plt.subplots(N, N, figsize=(10, 10))\n",
        "for row in range(N):\n",
        "    for col in range(N):\n",
        "        idx = start_val + row + N*col\n",
        "        noisy_img = to_rgb(x_test_noisy[idx])\n",
        "        pred_img = to_rgb(pred[idx])\n",
        "        im = np.concatenate((noisy_img, pred_img), axis=1)\n",
        "        axes[row, col].imshow(im)\n",
        "        axes[row, col].set_xticks([])\n",
        "        axes[row, col].set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT_TfhCsfXsH"
      },
      "source": [
        "## Object Detection with RetinaNet\n",
        "\n",
        "We now will show an object detection example. Object detection differs from image classification in that in an image classification setting we give a label to the whole image, whereas in an object detection setting we give a label and bounding box per each of the classes that are present.\n",
        "\n",
        "Top detector models include Faster R-CNN, YOLO (yes, it is called YOLO, standing for You Only Look Once; this is the unusual [resume](https://pjreddie.com/static/Redmon%20Resume.pdf) of the YOLO developer) or SSD (Single Shot Detector). Another detector is RetinaNet, which we use to give an example of object detection using the implementation based on [torchvision](https://github.com/pytorch/vision/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
        "from torchvision.transforms import functional as F\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import time\n",
        "\n",
        "\n",
        "SCORE_THRESHOLD = 0.45\n",
        "\n",
        "# Load the model\n",
        "weights = RetinaNet_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "model = retinanet_resnet50_fpn_v2(weights=weights, box_score_thresh=SCORE_THRESHOLD)\n",
        "model.eval()\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Get the COCO category names\n",
        "COCO_INSTANCE_CATEGORY_NAMES = weights.meta[\"categories\"]\n",
        "\n",
        "def draw_boxes(image, boxes, labels, scores):\n",
        "    \"\"\"Draw bounding boxes on the image\"\"\"\n",
        "    import cv2\n",
        "    image = np.array(image)\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        # Convert box coordinates to integers\n",
        "        box = box.cpu().numpy().astype(int)\n",
        "\n",
        "        # Generate random color for this class\n",
        "        color = tuple(np.random.randint(0, 255, 3).tolist())\n",
        "\n",
        "        # Draw box\n",
        "        cv2.rectangle(image,\n",
        "                     (box[0], box[1]),\n",
        "                     (box[2], box[3]),\n",
        "                     color,\n",
        "                     2)\n",
        "\n",
        "        # Draw label\n",
        "        caption = f\"{COCO_INSTANCE_CATEGORY_NAMES[label]} {score:.3f}\"\n",
        "        cv2.putText(image,\n",
        "                   caption,\n",
        "                   (box[0], box[1] - 10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                   0.5,\n",
        "                   color,\n",
        "                   2)\n",
        "\n",
        "    return image\n",
        "\n",
        "def detection_url(url):\n",
        "    # Download and load the image\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "    # Prepare image for the model\n",
        "    transform = weights.transforms()\n",
        "    img_tensor = transform(image).to(device)\n",
        "\n",
        "    # Perform detection\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        prediction = model([img_tensor])\n",
        "    print(\"processing time: \", time.time() - start)\n",
        "\n",
        "    # Get predictions\n",
        "    boxes = prediction[0]['boxes']\n",
        "    scores = prediction[0]['scores']\n",
        "    labels = prediction[0]['labels']\n",
        "\n",
        "    # Filter predictions based on score\n",
        "    mask = scores > SCORE_THRESHOLD\n",
        "    boxes = boxes[mask]\n",
        "    labels = labels[mask]\n",
        "    scores = scores[mask]\n",
        "\n",
        "    # Draw boxes on image\n",
        "    result_image = draw_boxes(image, boxes, labels, scores)\n",
        "\n",
        "    # Display result\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(result_image)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iJJ3BDxxmubK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di9tA9Zqij06"
      },
      "source": [
        "Now, we can use any *URL* pointing to an image (some *URLs* may fail) as input, and we will get an output with the detections out of the 80 classes in Microsoft COCO. If you want to check the classes available, you can print the variable labels_to_names.\n",
        "\n",
        "As an example, we, of course, input an image full of cats. The labels are a bit blurry, but you can see all of the cats have the correct bounding box around them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBIL89k7ix_S"
      },
      "source": [
        "url = 'https://st3.depositphotos.com/1032808/12984/i/950/depositphotos_129842732-stock-photo-group-of-various-breeds-cats.jpg'\n",
        "detection_url(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVYyfx_VDrPh"
      },
      "source": [
        "You can even try with images that contain different classes, and see how RetinaNet deals with multi-class images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kw9YL1GDn8d"
      },
      "source": [
        "url = 'https://cdn.mos.cms.futurecdn.net/hbKifQWBTcdhTEw8zsJWnF-1200-80.jpg'\n",
        "detection_url(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Coursework**"
      ],
      "metadata": {
        "id": "FsKc0btBtDyy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urw5Sugu1F7r"
      },
      "source": [
        "## Task 1: Classification on Tiny-ImageNet\n",
        "\n",
        "In this task, we are going to explore different models to do classification on 64x64 Tiny-ImageNet. Tiny-ImageNet is a smaller version of ImageNet (as the name indicates), containing \"only\" 200 classes. Each class has 500 images. The test set contains 10,000 images. All images are 64x64 RGB images.\n",
        "\n",
        "In the Network Training notebook, we explained how to define a validation set, and now we will put that into practice. Hence, as we now have a bigger dataset, we are going to use the standard split of training, validation, and test data. Therefore, you will check the performance of the network in the validation set while training your network. Hence, your decisions need to be based on validation performance. Once you have obtained your best model using the training and validation data, you need to report the performance on the test set. Please try to no overfit to the test data, as in other problems it may not be available to you.\n",
        "\n",
        "In this exercise, you are asked to train VGG models with different strategies. Optionally, you are asked to use any other architecture of your choice to do classification in Tiny-ImageNet.\n",
        "\n",
        "Run the following script to get the data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download TinyImageNet\n",
        "! git clone https://github.com/seshuad/IMagenet"
      ],
      "metadata": {
        "id": "KOgLmEaJmCD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmPJ2vKG5BB_"
      },
      "source": [
        "def get_id_dictionary():\n",
        "    id_dict = {}\n",
        "    for i, line in enumerate(open('IMagenet/tiny-imagenet-200/wnids.txt', 'r')):\n",
        "        id_dict[line.replace('\\n', '')] = i\n",
        "    return id_dict\n",
        "\n",
        "def get_class_to_id_dict():\n",
        "    id_dict = get_id_dictionary()\n",
        "    all_classes = {}\n",
        "    result = {}\n",
        "    for i, line in enumerate(open('IMagenet/tiny-imagenet-200/words.txt', 'r')):\n",
        "        n_id, word = line.split('\\t')[:2]\n",
        "        all_classes[n_id] = word\n",
        "    for key, value in id_dict.items():\n",
        "        result[value] = (key, all_classes[key])\n",
        "\n",
        "    return result\n",
        "\n",
        "def get_data(id_dict):\n",
        "    train_data, val_data, test_data = [], [], []\n",
        "    train_labels, val_labels, test_labels = [], [], []\n",
        "    for key, value in id_dict.items():\n",
        "        train_data += [cv2.imread('IMagenet/tiny-imagenet-200/train/{}/images/{}_{}.JPEG'.format(key, key, str(i))) for i in range(450)]\n",
        "        train_labels += [value] * 450\n",
        "\n",
        "        val_data += [cv2.imread('IMagenet/tiny-imagenet-200/train/{}/images/{}_{}.JPEG'.format(key, key, str(i))) for i in range(450, 500)]\n",
        "        val_labels += [value] * 50\n",
        "\n",
        "    for line in open('IMagenet/tiny-imagenet-200/val/val_annotations.txt'):\n",
        "        img_name, class_id = line.split('\\t')[:2]\n",
        "        test_data.append(cv2.imread(f'IMagenet/tiny-imagenet-200/val/images/{img_name}'))\n",
        "        test_labels.append(id_dict[class_id])\n",
        "\n",
        "    return np.array(train_data), np.array(train_labels), np.array(val_data), np.array(val_labels), np.array(test_data), np.array(test_labels)\n",
        "\n",
        "def shuffle_data(train_data, train_labels, val_data, val_labels):\n",
        "    # This function shuffles separately the train set and the\n",
        "    # validation set\n",
        "    size = len(train_data)\n",
        "    train_idx = np.arange(size)\n",
        "    np.random.shuffle(train_idx)\n",
        "\n",
        "    size = len(val_data)\n",
        "    val_idx = np.arange(size)\n",
        "    np.random.shuffle(val_idx)\n",
        "\n",
        "    return train_data[train_idx], train_labels[train_idx], val_data[val_idx], val_labels[val_idx]\n",
        "\n",
        "train_data, train_labels, val_data, val_labels, test_data, test_labels = get_data(get_id_dictionary())\n",
        "train_data, train_labels, val_data, val_labels = shuffle_data(train_data, train_labels, val_data, val_labels)\n",
        "\n",
        "# Let's visualize some examples\n",
        "N=3\n",
        "start_val = 0 # pick an element for the code to plot the following N**2 values\n",
        "fig, axes = plt.subplots(N,N)\n",
        "for row in range(N):\n",
        "  for col in range(N):\n",
        "    idx = start_val+row+N*col\n",
        "    tmp = cv2.cvtColor(train_data[idx],cv2.COLOR_BGR2RGB)\n",
        "    axes[row,col].imshow(tmp, cmap='gray')\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    axes[row,col].set_xticks([])\n",
        "    axes[row,col].set_yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels, val_data, val_labels, test_data, test_labels = get_data(get_id_dictionary())\n",
        "train_data, train_labels, val_data, val_labels = shuffle_data(train_data, train_labels, val_data, val_labels)\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "train_data = torch.from_numpy(train_data).permute(0, 3, 1, 2).float() / 255.0\n",
        "val_data   = torch.from_numpy(val_data).permute(0, 3, 1, 2).float() / 255.0\n",
        "test_data  = torch.from_numpy(test_data).permute(0, 3, 1, 2).float() / 255.0\n",
        "\n",
        "# Normalize to [-1, 1], channel independent\n",
        "mean = train_data.mean(dim=(0, 2, 3))\n",
        "std  = train_data.std(dim=(0, 2, 3))\n",
        "train_data = (train_data - mean[None, :, None, None]) / (std[None, :, None, None] + 1e-7)\n",
        "val_data   = (val_data   - mean[None, :, None, None]) / (std[None, :, None, None] + 1e-7)\n",
        "test_data  = (test_data  - mean[None, :, None, None]) / (std[None, :, None, None] + 1e-7)\n",
        "\n",
        "# Build data loader\n",
        "train_dataset = TensorDataset(train_data, torch.from_numpy(train_labels))\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(val_data, torch.from_numpy(val_labels))\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "test_dataset = TensorDataset(test_data, torch.from_numpy(test_labels))\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "QzcWKswmmRJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXLjtvJEAF_r"
      },
      "source": [
        "**Report**:\n",
        "*   In a plot, please report the training and validation accuracy curves for the following models:\n",
        "\n",
        "> *   VGG16 trained from scratch.\n",
        "\n",
        "> *   Transfer Learning VGG16: load pre-trained ImageNet weights and only train the newly added dense layers.  To do so, freeze all layers and only train the dense layers you have modified in the model.\n",
        "\n",
        "> *   Fine-tuning VGG16: load pre-trained ImageNet weights and train the whole architecture.\n",
        "\n",
        "*   Discuss the previous figure in the main text. And report in a table the test accuracy and the training and inference times of previous VGG16 experiments. Training times are computed per epoch and you can find them displayed in the .fit() method information. Report either the total training time, or the number of epochs and training time per epoch. Inference times are computed per image, and we give you the code below to obtain them.\n",
        "\n",
        "*   Now that we are familiar with loading and using models in Pytorch, you can use any model of your choice to classify Tiny-ImageNet. You can take the model directly from Pytorch, any GitHub repository, or do the code yourself. You need to report your results in the previous table and compare your model of choice with previous VGG16 networks.\n",
        "\n",
        "Note that training/inference time will depend on which GPU you are using. Report the time results in the same instance, or at least when using the same GPU. Report also the GPU you were using to compute those inference times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae4MXFHF6Lsk"
      },
      "source": [
        "import copy\n",
        "import time\n",
        "# You may want to import more modules.\n",
        "\n",
        "# Early stopping utility\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_accuracy = None\n",
        "        self.best_model_weight = None\n",
        "\n",
        "    def __call__(self, model, val_accuracy):\n",
        "        if self.best_accuracy is None:\n",
        "            self.best_accuracy = val_accuracy\n",
        "            self.best_model_weight = copy.deepcopy(model.state_dict())\n",
        "        elif val_accuracy < self.best_accuracy + self.min_delta:\n",
        "            self.counter += 1\n",
        "            return self.counter >= self.patience\n",
        "        else:\n",
        "            self.best_accuracy = val_accuracy\n",
        "            self.best_model_weight = copy.deepcopy(model.state_dict())\n",
        "            self.counter = 0\n",
        "            return False\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Define your model here\n",
        "# model = ...\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=1e-3)\n",
        "\n",
        "for epoch in range(20):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0.0\n",
        "\n",
        "    time_start = time.time()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    time_end = time.time()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_accuracy = train_correct / len(train_loader.dataset) * 100\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy = val_correct / len(val_loader.dataset) * 100\n",
        "\n",
        "    print((\n",
        "        f\"Epoch [{epoch+1}/20] \"\n",
        "        f\"Train Loss: {train_loss:.4f}, \"\n",
        "        f\"Train Accuracy: {train_accuracy:.2f}%, \"\n",
        "        f\"Validation Loss: {val_loss:.4f}, \"\n",
        "        f\"Validation Accuracy: {val_accuracy:.2f}%, \"\n",
        "        f\"Training Time/Epoch: {time_end-time_start:.2f}s\"\n",
        "    ))\n",
        "\n",
        "    # Early stopping check\n",
        "    if early_stopping(model, val_accuracy):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        print()\n",
        "        break\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(early_stopping.best_model_weight)\n",
        "\n",
        "# Evaluate on test set\n",
        "running_time = 0.0\n",
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        time_start = time.time()\n",
        "        outputs = model(inputs)\n",
        "        time_end = time.time()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        test_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        running_time += time_end - time_start\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = test_correct / len(test_loader.dataset) * 100\n",
        "time_per_image = running_time / len(test_loader.dataset)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Inference Time/Image: {time_per_image:.4f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: ConvNeXt Model Scaling on Tiny-ImageNet\n",
        "ConvNeXt is a modern convolutional neural network architecture introduced by Facebook AI Research in the 2022 paper titled \"[A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)\" by Liu et al. It reimagines the classic ResNet architecture by integrating design principles from Vision Transformers (ViTs), such as large kernel sizes, inverted bottlenecks, depthwise convolutions, Layer Normalization, and GELU activations. Despite being fully convolutional, ConvNeXt matches or even surpasses the performance of transformer-based models like the Swin Transformer on benchmarks such as ImageNet. Its success demonstrated that, with the right architectural updates, CNNs can remain competitive in the transformer era, significantly influencing how researchers view the future of convolutional models in computer vision.\n",
        "\n",
        "Pytorch offers pretrained model architecture for ConvNeXt at varying sizes, which can be seen [here](https://docs.pytorch.org/vision/main/models/convnext.html).\n",
        "\n",
        "In this task, we explore how increasing model size affects performance while keeping architecture the same. You will load ConvNeXt-Tiny, Small, Base, and Large pretrained on ImageNet, and evaluate them on the previously loaded TinyImageNet data to investigate the tradeoff in performance for size constraints.\n",
        "\n",
        "*Note: You are NOT training or fine-tuning these models.*\n",
        "\n",
        "Use the code below to load the data in the format required by the models."
      ],
      "metadata": {
        "id": "OKlWOwzps6b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms for ConvNeXt input (resize to 224, normalize like ImageNet)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class TinyImageNetTestDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = np.argmax(labels, axis=1)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "test_dataset = TinyImageNetTestDataset(test_data, test_labels, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "7t1nC0dDwH81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model_head(model, num_classes=200):\n",
        "    # Map model_name string to torchvision convnext constructor\n",
        "    # Replace the classifier head to match Tiny-ImageNet classes\n",
        "    num_ftrs = model.classifier[2].in_features  # ConvNeXt classifier is nn.Sequential(..., nn.Linear)\n",
        "    model.classifier[2] = nn.Linear(num_ftrs, num_classes)\n",
        "    return model\n",
        "\n",
        "# ..."
      ],
      "metadata": {
        "id": "W7jEtQ8hzpZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report**:\n",
        "*   In a plot, report the accuracy and test MSE against number of parameters of the model.\n",
        "\n",
        "*   Discuss the previous figure(s) in the main text, explaining the trend seen between model size and performance and comment on the benefits/drawbacks of using a larger/smaller model.\n"
      ],
      "metadata": {
        "id": "dhEarez0zpuW"
      }
    }
  ]
}